summary,instance"When USPTO unified its distribution methods to supply bulk open data, then third parties were able to create new tools and products (Reed Tech’s Pair Data and PEDS) that have improved the examination process.When USPTO has made data available for AI/ML models, then third party partners were able to create new search tools (the Office Actions dataset and PatentsView project) for more efficient patent processing as well as process data for academic researchers. Submitted by: Anonymous.","(1) Patent dataUS Patent & Trademark Office (USPTO) Director Andrei Ian cu has prioritized advancing USPTO’s “Big Data capabilities, data analytics and artificial intelligence” to “improve overall performance, as well as fuel data-driven decision and policy making.” (https://www.uspto.gov/about-us/news-updates/statement-director-andrei-iancu-committee-judiciary) Governments providing bulk access to patent data enables a whole industry of free and commercial tools that work to make this complicated information accessible and useful to the public.  The USPTO has made available public patent and trademark bulk data (https://www.uspto.gov/learning-and-resources/bulk-data-products).  The availability of this data  has not only enabled more meaningful access for the public, but also has served USPTO with valuable tools from the commercial sector developed using this public data to improve the examination process when searching for prior art. The USPTO supports this ecosystem with timely releases of patent information in various formats through their Bulk Data products (https://www.uspto.gov/learning-and-resources/bulk-data-products). Several important data products are distributed through third parties (PAIR data from Reed Tech) or new systems (PEDS) as a result of historical technical limitations. The USPTO should continue to unify their data distribution methods for providing both efficient incremental updates and the full datasets.The USPTO has also focused on applying Artificial Intelligence/Machine Learning to patent search tools to increase internal process efficiency and make data accessible for building Machine Learning models.  For the public community, the research datasets and projects by the USPTO Office of the Chief Economist have been invaluable in making new data available for the first time (the Office Actions dataset), as well as releasing previously-public data in easy-to-use formats for Data Mining and Machine Learning (the PatentsView project). The timeliness of updates to these datasets is not as important because the data is used for infrequent training of models that can be applied to new inputs, or for academic research that can tolerate delays.When developing new applications, collecting and storing metrics on their usage and performance is important to guide and evaluate future improvements. These metrics from internal applications, such as search queries and results clicked on, can also benefit the agencies' goals of applying Machine Learning to improve their systems by training models on these metrics.(https://www.uspto.gov/learning-and-resources/ip-policy/economic-research/research-datasets)  A key element of enabling this involves linking USPTO data with other international data formats.  Developing uniformity of patent numbering with patent offices internationally would be critical to the USPTOs goals here.   Information not directly produced by the USPTO can still be important to measuring patent quality and the patent system's effects. The Patent Litigation Docket Reports Data (https://www.uspto.gov/learning-and-resources/electronic-data-products/patent-litigation-docket-reports-data) collects information from PACER about patent cases, which can be used to study patents used in litigation. The USPTO should continue to collect external data.""When researchers studied administrative and other data in connection with Oregon's Medicaid expansion, they found enrollment was associated with lower depression rates, higher ER use, and no significant change on employment. Submitted by: J-PAL North America.","Understanding the Impact of Expanding MedicaidUsing administrative data, researchers studied the expansion of Oregon’s Medicaid program to inform one of the most salient political debates of the last decade: the impact of providing health insurance to the uninsured. The state of Oregon’s decision in 2008 to allocate application slots in its expanded Medicaid program by lottery offered researchers an extraordinary opportunity to understand the causal impact of the program by comparing the outcomes of lottery winners to lottery losers. In addition to a survey of lottery winners and losers, researchers measured outcomes for over 20,000 individuals using administrative data sources such as hospital records, employment data, and state and federal benefits records. The administrative data allowed researchers to discover a key finding that would have otherwise been obscured by fading memories: When researchers surveyed lottery winners and lottery losers about emergency room use, they found no statistically significant difference. However, by looking at Portland-area hospital records, researchers found that Medicaid increased emergency room use by 40 percent. Other outcomes measured included health care utilization (use of preventive care and emergency services increased), financial strain (catastrophic expenses and other measures decreased), mental health (depression rates declined), physical health (perceptions of health improved, but measured physical health did not detectably change), and employment and earnings (neither improved). Measuring all of these outcomes for both lottery winners and losers would have likely been impossible through surveys alone. ""When the U.S. Census Bureau collects business data from various sectors and governments, then companies are able to compare their operations to industry norms, find new markets, and inform key decisions. If the Brazilian government creates five-year projections of specific regions and skill sets to select the best provider of a training curriculum, then they will be better able to meet emerging manpower needs. When the Department of Commerce’s Economic and Statistics Administration publishes economic indicators daily, then retailers are able to use neighborhood-level demographics to maintain their inventory across the country. Submitted by: The MITRE Corporation.","Examples of how supporting the production and dissemination of comprehensive, accurate, and objective statistics on the state of the nation helps businesses and markets operate more efficiently.1. Business data collected by the U.S. Census Bureau from various sectors of the economy, such as manufacturing, construction, retail trade, and health care, along with data collected from state and local governments and international trade, helps businesses compare their operations to industry norms, find new markets, and inform key decisions. Examples are available at https://www.census.gov/programssurveys/economic-census/guidance/data-uses.html2. Data from the Securities and Exchange Commission (SEC) has powered investment firms for decades, and it is now possible to combine SEC data with other data sources for faster, more accurate, and more usable analysis.3. Statistics on a rolling five-year projection of the manpower needed in specific geographies and skill areas was developed by the Brazilian government26 in collaboration with private companies, trade associations, and labor unions in order to identify the best training provider to co-develop a curriculum with selected companies to meet emerging manpower needs.4. Economic Indicators published by the Department of Commerce’s Economic & StatisticsAdministration are accessed and used by businesses every day to make decisions. Retailers, such as Target, use the government data on neighborhood-level demographics, such as population density, owner-occupancy, and household size, to determine the optimal mix of goods with which to stock its stores throughout the country.27[26 https://www.mckinsey.com/industries/public-sector/our-insights/government-by-design-four-principles-for-abetter-public-sector27 http://www.esa.gov/sites/default/files/the-value-of-the-acs.pdf]""When the Securities and Exchange Commission (SEC) provides business data, then  investment firms are able to use more accurate and usable analysis.  Submitted by: The MITRE Corporation.","2. Data from the Securities and Exchange Commission (SEC) has powered investment firmsfor decades, and it is now possible to combine SEC data with other data sources forfaster, more accurate, and more usable analysis.""If funding is provided to allow Third Sector to provide technical assistance to government partners in five states for a feasibility study, then Pay for Performance funds can help benefits programs understand the effects of different program design choices. Submitted by: The Third Sector.","Concept Note: Paying for Outcomes Across Workforce, Education, and TrainingTHE OPPORTUNITYSeveral recent legislative changes have created a unique opportunity over the next nine months. States having a combined SNAP E&T and Workforce plan can use existing Dept of Labor authorities as part of their outcome-oriented project design for SNAP E&T eﬀorts. At the end of the nine-month period, the Pay for Success project can use the ﬁrst expenditures from the $100M Social Impact Partnerships to Pay for Results Act (SIPPRA) fund. To access those funds localities need to have completed a feasibility study. We recommend that FNS sponsor Technical Assistance (TA) to 5 states to determine the feasibility of Pay for Success projects for Education and Training in preparation to utilize SIPPRA funding. Even if only three of the cohort sites are funded, the FNS TA money will be ampliﬁed 15 times by the SIPPRA funds.SNAP E&TThe U.S. Department of Agriculture (USDA) currently spends over $300 million on Supplemental Nutrition Assistance Program Employment and Training (SNAP E&T) programming nationwide and up to$1 billion in funding allocated by FY2021. While states are required to administer SNAP E&T programs, they have a large degree of ﬂexibility in deciding whom to serve and which services to provide. Many states are interested in increasing the work requirements for receiving beneﬁts across multiple federal beneﬁts programs. There is a limited understanding, however, of the eﬀects of disparate program models and components—and their long-term eﬀects on participants’ lives.The Workforce Innovation and Opportunity ActIn 2014, Congress and the Department of Labor (DOL) included key provisions in the Workforce Innovation and Opportunity Act (WIOA) to catalyze local innovation and outcomes-oriented approaches to workforce programming. WIOA Pay-for-Performance (P4P) guidelines now allow workforce boards to set aside funds that will not expire during the traditional two-year funding cycle if they are tied to the achievement of long-term outcomes that measurably improve people's lives. These P4P payments could incentivize workforce providers to meet longer-term performance objectives, such as sustained increases in employment, wage growth, and academic achievement, and reductions in the need for utilization of public beneﬁts and time spent in the justice system.This shift away from compliance and towards longer-term outcomes through the authorization of P4P contracts, has created an unprecedented opportunity for government to build on the history of performance-based contracting to develop more comprehensive services, and deploy resources in increasingly outcomes-driven ways. Generous funding from the Social Innovation Fund (SIF) allowed Third Sector to work with ﬁve jurisdictions to ultimately launch the nation’s ﬁrst two WIOA P4P contracts in Northern Virginia (2017) and San Diego County (2018).Combining Funding StreamsJurisdictions shifting to outcomes-oriented programming can utilize the P4P provisions in WIOA to reward achieving long-term workforce and education outcomes while covering service delivery costs with SNAP E&T funds. Using WIOA authorities would allow states to shift to an outcomes-orientation from compliance-focused programming without requiring changes to current USDA authorities. Starting in 2016, states could submit plans to align state workforce strategy across federal programs through DOL, EDOE, HHS, HUD and USDA. Already, nine states have combined WIOA eﬀorts with SNAP E&T through joint plans, including Washington, Minnesota, Missouri, Arkansas, Louisiana, Alabama, Tennessee, Virginia, and Massachusetts.Social Impact Partnerships to Pay for Results Act (SIPPRA)The 2018 Social Impact Partnerships to Pay for Results Act (SIPPRA)1, for the ﬁrst time, establishes a funding stream and structure to support and advance the implementation of Pay for Success contracting impacting Federal outcomes and related costs. The joint SNAP/WIOA P4P initiatives can leverage this$100 million fund available to state and local governments to pay directly for program outcomes, project feasibility studies, evaluation, and program administration. The authority to review and approve proposals sits with the Secretary of the Treasury, in consultation with the newly formed Federal Interagency Council on Social Impact Partnerships. SIPPRA has the potential to transform the way states think about the provision of human and social services and reduce in the need for federal support by ﬁnding real solutions to the most persistent challenges.PROPOSED SCOPE OF WORKThird Sector proposes providing TA on Pay for Success to 5 states that have implemented combined SNAP/WIOA plans. The cohort would include a diverse range of states and agencies that are at diﬀerent points in the process of aligning workforce programming across diﬀerent funding streams. Through the proposed scope of work, states would develop a comprehensive P4P contracting strategy, a prerequisite for utilizing WIOA’s P4P provisions and for accessing the SIPPRA funding.Third Sector has deep experience in the workforce development and job readiness training programs and has been a national pioneer in partnering with government to implement P4P. We are excited by the opportunity to launch another cohort focused on coordinated outcomes-oriented contracting across the SNAP E&T and WIOA programs. The following section outlines the work required to demonstrate how outcomes-oriented SNAP E&T programming would operate. The overall work would be split into two phases over the course of 15 months with the following goals:Phase 1: Assess Phase (6 months, 5 sites plus a learning Community, $1.0 - $1.25M)● Prepare a plan for accessing the SIPPRA funding for Phase 2 and for matching success payments● Create an engaged coalition among government partners to launch a joint WIOA/SNAP P4P pilot project in 5 states that have combined SNAP E&T/WIOA plans or that otherwise demonstrate readiness for a P4P collaboration● Develop a data-informed understanding of the population to be served at each site, the referral pathway, and the outcomes to be delivered by the pilot using existing administrative data such as Unemployment Insurance, National Student Clearinghouse, and/or income tax ﬁlings.● Identify evidence-based or a novel theory of change to achieve desired outcomes and agree on a roadmap for integrating the appropriate service provider, data, and contracting support for launching the pilot1 http://www.americaforward.org/practice-policy-sippra-little-known-legislation-big-implications/ The Assess Phase will end with a deliverable that solidiﬁes a vision of the engaged stakeholders’ goals, the shared understanding of a pilot project, and the roadmap for implementing the pilot.Phase 2: Build Phase (9 months, funded separately)● Transition the Assess Phase vision and deliverables to a realized project● Address any gaps identiﬁed in Assess Phase and build out outcome-oriented contracts/grants with the state agreements oﬃcers● Launch 2-year pilot projects with robust evaluation plan in at least 3 statesAt the end of the Build Phase, processes and contracts will have been ﬁnalized to operationalize an outcomes orientation through the pilot.Phase 1 and Phase 2 require active participation and leadership from staﬀ and government partners (including departmental decision-makers and staﬀ) to collaborate with Third Sector on these work streams and provide access to program information and data. Working with states that are committed to improving outcomes by implementing new approaches that connect reimbursement to improvement in the lives of those receiving services is critical.Assess Phase Scope of WorkThe cohort model has demonstrated the value of starting in several communities at the same time. Each state learns how others addressed their challenges and they provide feedback on how to scale nationally. The move to outcome-orientation requires changes across the contracting, ﬁnance, services, and data systems. Our experience has shown that this is diﬃcult to accomplish and to get multiple sites to Phase 2, a cohort size of three to ﬁve is needed at the start of the Assess Phase. There are several reasons why moving to outcome-orientation is not practicable in all sites immediately, and the cohort model makes it likely that at least one site will be ready at the end of the Assess Phase.Each cohort member develops a plan to meet the local need using the best local providers. A detailed work plan is included in the table below. Success payments are used to scale projects by rewarding the most eﬀective providers. The project design must work within constraints on data access, community need, and contracting regulations and a rigorous evaluation framework. As an example of a full project, the SkillSource Group of Northern Virginia used WIOA P4P to revamp their outreach to youth exiting the foster care or the justice system. If providers meet the same performance metrics with these historically underserved groups, they get a roughly 10% bonus to further expand nontraditional outreach.While the sites develop their plans, the learning community will share best practices within the cohort and also provide USDA with insight on the best path to expand outcome-oriented approaches to more states. This pilot program can help FNS build a nation-wide plan to improve the outcomes of those they serve by rewarding providers who make positive changes in people’s lives.Work Stream DeliverableKickoffThird Sector will review and ﬁnalize the engagement work plan, including timelines and responsibilities for all parties. This eﬀort will ● Project Work Plan● Information and data request identify the needed government decision-makers, programmatic experts, and data integrators to support the execution of the work plan. ● Pilot goals and vision, including rosters for Steering Committee and Working Group(s)Beneficiary PopulationIdentify speciﬁc eligibility criteria for need, referral, and enrollment of beneﬁciaries. Estimate the number of participants that can feasibly be served by the pilot and understand the opportunities and challenges associated with population ﬂow dynamics. ● Intended Beneﬁciary Population and Referral Pathway MapPerformance MeasurementPrioritize outcome(s) and the metrics used to track success over time. These activities enable stakeholders to understand existing utilization of services, impact potential of the pilot, measurability of the pilot’s results, and government stakeholder enthusiasm to pay and measure pilot outcomes once the pilot is completed. ● Outcome Metric and Baseline AssessmentData AccessIdentiﬁes the data required to measure priority outcomes and identify, refer, and enroll the beneﬁciary population into the pilot. Assess the data capacity of government partners and service providers. Third Sector will document high-level data gaps (if any) and oﬀer initial recommendations to alleviate, work-around, or build capacity in the needed data area(s). ● Data Source Assessment (for enrollment and impact measurement)Evaluation DesignEvaluation options include a Randomized Control Trial orquasi-experimental methods to estimate the isolated impact of the intervention on the population being served. Recommendations leverage best practices throughout the country for comparing to a similar population not receiving the intervention. ● Evaluability Assessment with preferred evaluation approachIntervention AssessmentUnderstanding of how the anticipated services delivers better outcomes to the beneﬁciary population, including the existing evidence base.Gauge service provider’s capacity to deliver priority outcomes and meet evaluation needs identiﬁed in the work streams listed above. ● Service Delivery Requirements (programming and resources)● Service Provider Track RecordEconomics and FinancingAll of the programmatic works streams listed above will be integrated and articulated in economic terms. Third Sector will articulate the operating cost of service delivery during the pilot, and the related programmatic and economic beneﬁts to accrue to government stakeholders. ● Service Delivery Budget● Cost - Beneﬁt AnalysisAppropriations and ContractingGovernment stakeholder engagement to ensure service delivery can be maintained, replicated, or expanded after the pilot. Identifying state and federal funding sources to support service delivery. Translate the ● Outcomes Value Mapping (by funding stream and outcome)● Outcomes Orientation Roadmap (for operationalizing referral, pilot design process and outcomes delivery model into ﬁnancial, political, and administrative incentives to be embedded in service policy and procurements. service delivery, data sharing, and evaluation work streams into contracts)At the conclusion of the Assess Phase, Third Sector will draft a “Go Forward” recommendation on how to proceed into the Build Phase of the engagement. This deliverable will synthesize the vision for the pilot, prioritize challenges in structuring the pilot, identify programmatic and administrative conditions for launching the pilot, and conﬁrm that a Q2 CY19 launch of the pilot is feasible. It will also include a recommendation on how best to apply and win Build Phase support from local sources and SIPPRA funding.Build Phase Scope of WorkThe build phase would be funded separately from this initial project design work. Only those sites where the assessment showed that the pilot would be practical would move forward into this phase. The goal of the Build Phase is to create the concrete agreements needed to launch the pilot project. These agreements include the service contract, the evaluation plan, and other data use agreements as needed to execute the evaluation.If desired, support for the build phase could be built into the overall TA agreement with funding as an option on the Assess Phase contract. For those states that are ready to move to the Build Phase, the option can be exercised, and the Build Phase work can start independent of an award from SIPPRA.TIMING, STAFFING AND BUDGETMost engagements commence within 6-12 weeks of contract signing in order to mobilize key stakeholders and align Third Sector resources to engagement needs. During this period Third Sector requests the key information and contact info from key stakeholders. In this way, we are able to build momentum quickly once the engagement formally starts. Once the project has begun, we ask that the TA-recipients continue supporting the Steering Committee and working group teams with their internal teams requiring about 50% FTE.Our team typically works at the state’s site at key points throughout an engagement with most daily work done in the Third Sector oﬃces. Staﬃng for each site includes a Director (10% FTE) who will serve as the Relationship Manager and Senior Project Resource, a Manager (50% FTE) who will serve as the Project Lead and runs daily management of the project, and an Associate (50% FTE) who will provide ongoing engagement support.In addition, the Learning Community has an additional team with a Director at 10% FTE and a Manager and an Associate at 50% FTE each to coordinate with the sponsor and facilitate the cross-site Learning Community and long-term scale-up plan for FNS. In a 6-month engagement, there is typically one in-person cohort meeting and two virtual gatherings using video teleconferencing.Third Sector is on the GSA schedule (NAICS Code 541618) with established rates and volume discounts. A 6-8 month Assess Phase program at our established rates and applying a 5% volume discount has an approximate price for 5 sites and the Learning Community between $1M and $1.25M. ABOUT THIRD SECTORThird Sector Capital Partners (“Third Sector”) is a 501(c)3 nonproﬁt consulting ﬁrm that advises governments, community organizations, and funders on how to unlock public sector innovation to solve pressing challenges such as economic mobility for all and the well-being of our children. Our proven approach is to collaborate with our clients to deﬁne impact, draw actionable insights from data, and implement outcomes-oriented contracting. In the past six years, we have worked with over 40 communities that embrace the challenge of becoming more eﬀective, eﬃcient and transparent with taxpayer dollars.Together with our partners, Third Sector has helped launch ten outcomes-oriented contracts that transition over $360 million in total public funding towards positive outcomes. Third Sector has launched an initiative to create system-wide impact in America’s workforce development ﬁeld by designing nationally scalable, performance-driven contracts. Beginning with partnerships in Austin, Boston, Denver, Northern Virginia and San Diego, Third Sector is taking advantage of the unique and time-sensitive opportunity resulting from enabling guidelines recently passed in the federal Workforce Innovation and Opportunity Act.Using SIF federal grants, Third Sector has created two issue-based cohorts of TA recipients–one focused on two-generation outcomes for children and families and the other on WIOA–to promote peer-to-peer learning and eﬃcient scaling of outcomes-oriented contracting.To select sites for these cohorts, Third Sector held a national competition and selected local governments to receive TA over the course of twelve months to design, implement, and scale P4P initiatives. In addition to site-speciﬁc technical assistance, sites in Third Sector’s cohorts participated in learning communities that featured joint trainings, workshops, and webinars focused on outcomes contracting strategies.Please contact Brian Beachkofski (bbeachkofski@thirdsectorcap.org, 937-626-2650) for more information or any questions.""If public sector executives at agencies working with electronic evidentiary information embrace best practices related to cloud solutions, they will reap benefits in storage, recovery, management, and compliance. Submitted by: Anonymous.","https://1901group.com/wp-content/uploads/2018/07/WhitePaper1.pdfA strategy  to managing evidentiary data in a federal environment.""When NOAA shares weather data, it helps lower economic and human costs of weather-related damage and helps power a multi-billion dollar industry. Submitted by: The MITRE Corporation.","4. Government research, related to leveraging its data, continues to foster economic growth in significant ways. A prime example is NOAA’s providing access to weather data, which has significantly lowered the economic and human costs of weather-related damage through forecasts, enabled the development of a multi-billion-dollar weather derivative financial industry dependent on seasonal data records, and catalyzed a growing million-dollar industry of tools and applications derived from NOAA’s real-time data.""The University of Michigan proposes updating the Bureau of Justice Statistics State Court Processing Statistics program (SCPS), using the Criminal Justice Administrative Records System (CJARS) as a model. Developed in 2016, the CJARS is an integrated data repository policymakers use to improve public safety, save money, and minimize legally- and socially-imposed penalties. SCPS hasn’t been updated in almost 10 years. Submitted by: University of Michigan.","Criminal Justice Administrative Records System (CJARS) Use CaseWe propose a Use Case that updates the dormant Bureau of Justice Statistics State Court Processing Statistics program (SCPS) using CJARS as a model.The Criminal Justice Administrative Records System (CJARS) is a new integrated data repository designed to fundamentally transform research and reporting on criminal policy in the United States. CJARS collects longitudinal electronic records from police, sheriffs, courts, and correctional agencies and harmonizes these records to track a criminal episode across multiple stages of the justice system. Disparate local records are harmonized into a common national format, and probabilistic record linkage identifies all events associated with a specific individual and reconstructs the sequence of criminal justice activities in response to a given offense. In partnership with the US Census Bureau, these records can be merged with extensive social and economic information and form the basis for a new permanent federal CJARS repository accessible to all qualified researchers.CJARS has been in pilot development since Fall 2016, during which we have collected criminal justice data on over seventeen million people in nine states, covering more than forty-nine million criminal justice events. These include data on arrests and bookings, case filings and dispositions, and spells of jail, probation, prison, and parole. We have developed a data schema and governance infrastructure with the ability to scale nationally. In order to build stable relationships with data providers, CJARS will provide regular statistical reports to agencies using linked survey and administrative records.A central goal of CJARS is to give policymakers tools to make data-driven decisions about criminal justice and non-criminal justice programs. Scientifically informed policy can direct more efficient spending to improve public safety, save money, and minimize collateral consequences. In this spirit we would like to propose restarting the dormant Bureau of Justice Statistics State Court Processing Statistics program (SCPS), which has not been updated in almost a decade, using CJARS as a Use Case in the Data Incubator Project.Features of CJARS make it particularly attractive for producing the SCPS.  An existing and growing cache of historical administrative records would enable the production of retrospective statistical series for previous dormant years as well as benchmarking relative to prior methods.  Additionally, linking with Census data would support the integration of socio-demographic characteristics from surveys and other administrative records that might not otherwise be feasible.CJARS is only two-years old, so it does not currently have national level coverage. It is a long-term project, designed from the ground up to be sustainable and generate real value for data providers. But SCPS was never a national data collection. And given the time since it was last completed, we think a pilot-level program across 10-20 states could be useful, and generate momentum for CJARS as a critical part of the federal statistical system. The permanent structure of CJARS will also allow SCPS to continue to be produced in a timely manner.A reincarnated State Court Processing Statistics series with regular updates could transform how these data are used to inform a number of critical policy questions in the United States. Detailed local information on criminal activity over time could help build models on the scope and trajectory of the opioid epidemic. Sentencing information could be used to compare policy and its effectiveness across states. Detailed supervision histories could be used in sampling frames to improve coverage of hard-to-capture subpopulations. Integrated criminal histories could be used to develop population-level profiles of interactions with the criminal justice system. Together, we believe both the direct and secondary applications of a CJARS-supported SCPS will strongly advance the public policy interests of the United States.""If data relating to gun violence or made more available, researchers would be better-placed to help policymakers curb harm in this policy area. Submitted by: American Institutes for Research.","Studying Gun Violence on Social MediaAccording to CDC, every day in the United States, close to 40 people are killed by guns (not including suicides). According to a 2016 study, the gun-related murder rate in the United States is 25 times higher than the rate in 22 other high-income nations.Increasing numbers of researchers are asking questions about the causes and patterns of gun-related violence in the United States. However, the lack of research data is highly limiting. Better availability of data would help researchers develop a descriptive understanding of the issue, thereby improving policymakers’ ability to craft evidence-based policies to respond to and ultimately reduce gun violence.AIR has developed a social media monitoring framework and dashboard that is connected to real-time data streams to filter and collect information that users post about “gun violence” topics online (Figure 3). Beyond data collection, the tool allows the capture of any changes in the social media space; that is, it identifies those changes as events or new trends by extracting relevant descriptors from the text data to continuously update which posts become part of the research dataset. Availability of these data creates opportunities for further research, such as network analysis to identify influencers of grassroots movements, sentiment analysis to detect differences among stakeholder groups, and user identification models to map out the diverse set of institutions, individuals, and other groups active within this policy arena.""When researchers worked with H&R Block, they found that a streamlined aid process improved access to college for individuals with limited means. Submitted by: J-PAL North America.","Simplifying Financial Aid ProcessesThrough access to national-level data on college enrollment and financial aid, researchers found that a streamlined aid process improved access to college. Researchers partnered with H&R Block to conduct a large-scale randomized evaluation of the impact of providing financial aid application assistance to low-income adults and dependent children from low-income families.  For recipients in the treatment group, H&R Block pre-populated some aspects of the free application for federal student aid (FAFSA) and provided personalized assistance and information about financial aid options. Compared to a control group that received a basic brochure on college financial aid, treatment group individuals were more likely to submit the form and receive aid, as measured in U.S. Department of Education data. Records on college registration from the Ohio Board of Regents and the National Student Clearinghouse allowed researchers to track later outcomes for over 25,000 individuals and determine that simply making it easier to apply for financial aid substantially increased the likelihood that individuals with limited means would attend college and stay in college once they enrolled. In recent years, policymakers have increasingly called for simplifying the financial aid process, and a White House report cited this study as the most direct test of the impact of reducing complexity in the financial aid application process.""The Veterans Affairs Center for Innovation (VACI), the Dean Center for Tick Borne Illness at Spaulding Rehabilitation Network/Harvard Medical School Department of Physical Medicine and Rehabilitation, MIT Hacking Medicine, and the Open Medicine Institute are collaborating and sharing data to help solve challenges in the prevention and treatment of Lyme disease. The Patent and Trademark Office’s open data platform is showcasing unique ways data can be combined with other data sets, such as economic and geographic data. A collaboration between NIH, the UK-based Welcome Trust, the Howard Hughes Medical Institute, and The Open Science Prize inspired prototypes of tools, products, and services using open digital content to solve public health and biomedical challenges. By sharing weather data, NOAA helps lower the economic and human costs of weather-related damage. It has also inspired the creation of a multi-billion dollar financial industry dependent on seasonal data and a multi-million dollar industry of tools and apps derived from NOAA’s real-time data. Submitted by: MITRE Corporation.","Examples of how enabling external users to access and use government data for commercial or additional public purposes spurs innovative technological solutions and fills gaps in government capacity and knowledge 1. The U.S. Department of Veterans Affairs Center for Innovation (VACI), the Dean Center for Tick Borne Illness at Spaulding Rehabilitation Network/Harvard Medical School Department of Physical Medicine and Rehabilitation, MIT Hacking Medicine, and the Open Medicine Institute are collaborating and sharing data to help solve challenges in the prevention and treatment of Lyme disease. 2. The U.S. Patent and Trademark Office (USPTO) open data portal has improved the discoverability, accessibility, and usability of public patent and trademark data. The Developer Hub established a shareable and “social” platform to showcase unique ways data can be combined with other data sets, such as economic and geographic data. The portal also includes APIs for innovators to use in further mining this data, helping to inform USPTO customers where to spend their limited research and development resources, and providing a much more detailed view of the competitive landscape than previously available. 3. The collaboration between the National Institutes of Health (NIH), the UK-based Welcome Trust, the Howard Hughes Medical Institute (HHMI), and The Open Science Prize has successfully stimulated international collaborations around open data. Prototypes of new tools, products, and services that use open digital content are being developed to help solve pressing public health and biomedical research challenges. 4. Government research, related to leveraging its data, continues to foster economic growth in significant ways. A prime example is NOAA’s providing access to weather data, which has significantly lowered the economic and human costs of weather-related damage through forecasts, enabled the development of a multi-billion-dollar weather derivative financial industry dependent on seasonal data records, and catalyzed a growing million-dollar industry of tools and applications derived from NOAA’s real-time data.""If the Federal Chief Information Officer Council and National Council of Information Sharing and Analysis Center were held up as ideal models, information exchanges across federal agencies would improve. Submitted by: The MITRE Corporation.","Consider leveraging existing interagency governance bodies (directly or by learning from their approaches) to quickly initiate action and minimize the chances of duplicative efforts.Examples: The Federal Chief Information Officer (CIO) Council from the IT domain or the National Council of Information Sharing and Analysis Center (ISAC) in the cyber domain• Best Practice/Common Solution• The Federal CIO Council has several communities of practice in place focused on security, innovation, services, strategies and infrastructure, and workforce.• ISAC could serve as a model for establishing information exchanges across federal agencies.Investigate if the CIO Council structure and procedures would enable a more rapid development of a data governance forum.ISAC policies and practices can be studied while developing data-centric versions.""If HUD's Homeless Management Information System is used as a model of government data quality practices, it would help other organizations focus on, and strengthen, federal data quality. Submitted by: The MITRE Corporation.","Enhance the focus on the quality of federal data. Example: HUD’s Homeless Management Information System (HMIS) provides a data quality framework, tools, and standards to enable national, state, and local communities to monitor data quality on a continuous basis. 31 • Best Practice • HMIS may serve as a model to leverage for government data quality practices. • Consistent data quality practices would naturally lead to better data to support evidence-based decision making. Identify other government best practices for improving the quality (i.e., condition) of data to ensure that it is fit for use for its intended purpose. Develop new guidance or strengthen existing guidance on data quality for high-value and highly shared datasets.[31 https://www.hudexchange.info/news/homelessness-data-collection-reporting-updates-and-deadlines/]""If the secure data-sharing concept underpinning ASIAS, a trusted public-private platform developed by MITRE to collect and analyze airline and FAA safety data, were extended to other domains, it would enable sharing important, actionable data while protecting it. Submitted by: The MITRE Corporation","ASIAS is a trusted public-private platform in which airline and FAA safety data is gathered, cleansed, integrated, anonymized, and analyzed. The data is then presented back to the airlines for their use. • Game Changer • The approach enables the sharing of important, actionable information while protecting sensitive data and the source of the data. MITRE (which originally developed ASIAS) has already ported the concept to other domains, such as cybersecurity. It could be further extended to other domains where community sharing of sensitive data is required.""If a platform, such as the Sunlight Foundation's Open Law Library, were created on top of machine-readable policies and legislation, it would be easier to share and access legal data and improve decision making. Submitted by: The MITRE Corporation.","Develop methods for policymakers to more easily gather data from traditionally unstructured formats. Example: The non-profit Sunlight Foundation’s “Open Law Library” is creating a platform to make policies and legislation, which are often available only in .PDF format, machine readable and easily accessible. 32 • Game Changer • Most governments publish their laws and policymaking documents in .PDF form, making it difficult to access the information contained within and use it at a later date. “The ability to share and access legal data generated by other jurisdictions trying to answer the same question provides valuable context and can greatly improve decision making.”33 Explore the creation of a platform that makes policies and legislation machine readable. [32 https://sunlightfoundation.com/2017/10/30/lawmaking-is-data-making/ 33 Ibid.]""When USPTO created its open data portal, Developer Hub, and API, it improved access to and use of public patent and trademark data as well as facilitated linkage of these data with other data. Submitted by: The MITRE Corporation.","2. The U.S. Patent and Trademark Office (USPTO) open data portal has improved the discoverability, accessibility, and usability of public patent and trademark data. The Developer Hub established a shareable and “social” platform to showcase unique ways data can be combined with other data sets, such as economic and geographic data. The portal also includes APIs for innovators to use in further mining this data, helping to inform USPTO customers where to spend their limited research and development resources, and providing a much more detailed view of the competitive landscape than previously available. ""When NIH, the UK-based Welcome Trust, the Howard Hughes Medical Institute, and The Open Science Prize formed a collaboration, it successfully stimulated creation of new tools and services to help solve pressing public problems. Submitted by: The MITRE Corporation.","3. The collaboration between the National Institutes of Health (NIH), the UK-based Welcome Trust, the Howard Hughes Medical Institute (HHMI), and The Open Science Prize has successfully stimulated international collaborations around open data. Prototypes of new tools, products, and services that use open digital content are being developed to help solve pressing public health and biomedical research challenges. ""When the DOC's Economic & Statistics Administration publishes economic indicators, businesses access and use them to make important decisions. Submitted by: The MITRE Corporation.","4. Economic Indicators published by the Department of Commerce’s Economic & Statistics Administration are accessed and used by businesses every day to make decisions. Retailers, such as Target, use the government data on neighborhood-level demographics, such as population density, owner-occupancy, and household size, to determine the optimal mix of goods with which to stock its stores throughout the country.27[26 https://www.mckinsey.com/industries/public-sector/our-insights/government-by-design-four-principles-for-abetter-public-sector""When master reference data standards for enterprise are developed, such as DHS and USAID have done, internal efficiencies and alignment with other agencies are realized while information exchanged is accelerated. Submitted by: OPM.","Cross Agency Data StandardsHaving master reference data standards for enterprise data not only enables internal efficiencies, but also facilitates alignment with other agencies that can speed information exchange and save taxpayer dollars.  DHS and USAID have both expressed interest in using Department of State standards, and we are actively working to find the best way to share our data externally with trusted partners.  (DHS is specifically interested in datasets that would aid alignment with Consular Affairs, including Countries and Areas.  USAID is interested in aligning with the Department as well, particularly on Foreign Assistance interests.)""If a national Supplemental Nutritional Assistance Program (SNAP) web service template for accepting applications and data from third parties is developed, it may facilitate research and reduce state systems' costs. Submitted by: mRelief.","Project Name: National SNAP Web Service Template InitiativeProblem Statement - We do not have a common interface for ingesting Supplemental Nutrition Assistance Program (SNAP) application and outcome data from third parties as well as sending data to USDA for additional evaluation of the SNAP Program. The publication of SNAP eligibility requirement data on data.gov has already catalyzed innovative technological solutions to enhance service delivery in the form of mRelief.com the easy-to-use platform that enables Americans to find out if they meet basic SNAP requirements on web, text messaging and voice. Enabling SNAP application and outcome web service access is consistent with the federal data strategy of Commercialization, Innovation, and Public Use as it spurs innovative technological solutions and fills gaps in government capacity and knowledge.Key Goals of a National Web Service:Provides the capability for an accessible end-to-end service that enables third parties to inform governments about effective outreach methods for identifying eligible SNAP applicants to enable capital efficiency.Providing high quality and timely information to inform evidence-based decision-making and learning in support of program integrity facilitating national database querying at the point of application submission before additional time is expended by state administrators and SNAP applicants. Facilitating external research on the effectiveness of SNAP Delivery by measuring the length of application processes in real time, and linking SNAP participation to communication platforms that facilitate accessible surveys to participants on response to policy changesReducing costs on the state in upgrading their systems Similar use cases:The IRS has an end point that takes submissions from Turbo Tax.Ag Gateways Initiatives standardizing fertilizer reporting processExisting Infrastructure:Acre Crop Reporting - https://usda.github.io/data-standards/index.htmlSome Standard Data Elements that overlap with SNAP application and outcome information already exist here: https://usda.github.io/data-standards/data-elements/index.htmlDeliverable: A web service that would enable third parties to feed SNAP application and outcome data and provide national reporting mechanisms to the federal government.""When Project Data Sphere (PDS) provides access to de-identified patient-level data from health sources, it facilitates important research. Submitted by: Anonymous.","Case Study: Data Integration Innovations to Enhance Analytic Utility of Clinical Trial ContentProject Data Sphere (PDS) is a research platform that provides the research community with broad access to both de-identified patient-level data from oncology clinical trials and related analytic tools. While these data are rich in measures that characterize the clinical trials under study, data providers are required to de-identify patient-level data by removing key demographic data. To address these analytic constraints, the data profiles in selected PDS patient-level cancer phase III clinical datasets have been augmented by linking the social, economic, and health-related characteristics of like cancer survivors from nationally representative health and health care-related survey data. Using statistical linkage and model-based techniques, patient-level records in selected PDS datasets have been linked to those of comparable cancer survivors, and are thereby augmented with survey content on social, economic and health-related characteristics. These new analytically enhanced PDS data resources enable more targeted analyses designed to examine questions such as how disparities in cancer patients’ access to health care and income impact patient outcomes in specific phase III clinical trials, and what variations in patient outcomes are associated with specific demographic, socioeconomic, and health-related factors. This case study provides an overview of the methodologies used to connect patient-level clinical trial data with nationally representative health-related data on cancer survivors from the national Medical Expenditure Panel Survey (MEPS). MEPS was designed to provide national population-based health care use, expenditure, and source of payment estimates in addition to measures of health status, demographic characteristics, employment, health insurance coverage, and access to health care. Study findings include probabilistic assessments of the representation of the patients in the respective clinical trials relative to the characteristics of cancer survivors in the general population. The study also demonstrates how the augmented datasets serve to enable researchers to assess the impact of socioeconomic factors added through data integration on cancer survival and related outcomes of interest. Summary: Cancer researchers continue to advance discoveries and treatment protocols, yet every year, millions of lives are lost to cancer. Solutions are not advancing quickly enough. Researchers work independently and must often compete for resources needed to carry out their work. Project Data Sphere, LLC (PDS) was formed in 2012 to catalyze cancer research by bringing together diverse minds and technologies to help unleash the full potential of existing clinical trial data. PDS, an independent initiative of the CEO Roundtable on Cancer’s (CEORT’s) Life Sciences Consortium, operates a first-of-its-kind research platform. PDS provides the research community with broad access to both de-identified patient-level data from oncology clinical trials and freely available analytic tools to assist them in analyzing those data. A primary goal of PDS is to advance new research efforts that will improve the lives of cancer patients and their families around the world (Abdallah et al., 2015; Greene et al, 2015). These data are rich in measures that characterize the clinical trials under study, treatment protocols, and patient outcomes. However, to address the confidentiality provisions inherent to the trials, data providers are required to de-identify patient-level data prior to uploading datasets to ProjectDataSphere.org by masking or removing certain demographic data. Consequently, the influence of health-related and socioeconomic factors, access to and ""When the Census-USDA administrative data platform produces SNAP access estimates of Texas and New York eligible households, it helps administrators better target potential beneficiaries. Submitted by: USDA.","USE CASE PROPOSAL: Census-USDA Joint Project linking Supplemental Nutrition Assistance Program (SNAP) administrative and Census data: Linked Census-SNAP administrative data were used to produce sub-State estimates of access to SNAP among eligible households in Texas and New York. Census’s American Community Survey has a large sample size that can be used with SNAP administrative data to provide SNAP access rates for demographic subgroups and counties within States and subgroups within some large counties. Census is using this method to estimate sub-State SNAP access for States that share their administrative data with Census. The information helps SNAP administrators understand and better target households who may be eligible but not accessing the program.Current and future work using the Census-USDA administrative data platform includes the projects listed below. These projects and other potential analysis scale up the initial linkages by expanding the number of states, adding new administrative data to assess a broader set of programs, and providing analysis for specific groups such as veterans. o Since all SNAP participants are income-eligible for WIC, linking SNAP administrative data to WIC administrative data allows us to accurately identify infants and children who were eligible for WIC program but did not enroll.  Knowing the demographic characteristic characteristics of those who receive WIC benefits and those who, among eligible individuals, do not is important for assessing and improving program performance. Special focus is given to infants and children inasmuch as eligible infants are more likely to participate in WIC than eligible children.o SNAP administrative records provide a valuable tool to examine spells of benefit receipt because they capture the universe of SNAP recipients and contain information on the duration and amounts of benefits.  How long do individuals receive SNAP benefits?  What individual, household, and environmental characteristics distinguish long-term from short-term recipients?  What is the incidence of isolated short spells versus repeated cycling on and off SNAP? The use of administrative records will help overcome the problems of under-reporting and spell censoring found in survey data.o A future project could estimate SNAP’s poverty-reducing effect using a monthly timeframe, which better captures how SNAP benefits reduce poverty. The project links data from the Survey of Income and Program Participation to SNAP administrative data from several States to accurately identify (a) SNAP participants, (b) the particular months in which SNAP benefits were officially issued, and (c) the dollar amount of SNAP benefits. The analysis complements other ERS research that uses the Current Population Survey to measure the effect of SNAP in reducing annual poverty.o Linking administrative records with survey data holds the potential for improving understanding of food security and food insecurity among veterans.  With approval by the Department of Veterans Affairs (VA), administrative data already provided by VA and currently held at the Census Bureau can be linked with USDA’s Food Security Supplement to the Current Population Survey to examine prevalence, severity, and trends of food insecurity among veterans and their households.  The project plans to distinguish the experiences and conditions of working-age veterans from those of retired veterans.    ""If wage data from participants in social programs is compared before and after enrollment and against control groups, program effectiveness can be evaluated. Submitted by: Johnson County, Kansas.","Evaluating social programsLocal governments and nonprofit organizations are in need of quantitative analyses on effectiveness of the social programs that are implemented with the intention of helping people become self-sufficient. Examples of such social programs include educational and job training programs in local jails and state prisons, psychological and medical treatments at community based mental health services, and stabilization assistance provided by human services departments. Comparing short- and long-term individual wage data before vs. after people participating in a program, or between program participants vs. a control group would provide statistical evidence in terms of program effectiveness. For example, a group project at an Applied Data Analytics Workshop (https://coleridgeinitiative.org/training) found that 61% of prisoners did not earn any wage during the 8 quarters after being released from prisons. This finding, although preliminary, alarmed leaders of the state prison system, because the large amount of fund invested in educational and job training programs in prisons did not bring about the intended outcome: helping people obtain employment after they exited prisons. This preliminary study was conducted inside NYU Administrative Data Research Facility (https://coleridgeinitiative.org/computing), a secure cloud computing environment that hosts individual wage data (among many other micro-data sets) and provides software for data analysis. All steps of data manipulation, visualization, and analysis are done inside the secure environment, and permission to export final results is granted only after content passes security inspection.""If city- and metropolitan-level economic trends are scaled, along the lines of Kansas City's work with NYU, it could allow for better forecasting nationwide. Submitted by City of Kansas City, MO.","The City of Kansas City, Missouri like many other cities, is challenged to identify economic development metrics that are meaningful on a local scale. Cities like KCMO spend a great deal of time focused on enhancing economic development by targeting resources and programs, but measuring the outcomes of these results is imperfect without locally-focused economic development metrics.The city of KCMO participated in a focused analytics training project with the State of Missouri as facilitated by NYU that used confidential micro-data on employers and wages to examine economic development trends within the state and city. This type of work could provide a foundation for creating local measures of GDP that could then be used to understand and improve a focus on economic development. Scaling the development of these metrics to cities and metros across the U.S. would allow for more effective benchmarking and sharing of best practice strategies. ""When the American Institutes of Research's platform draws on public data alongside mining and linking techniques, it generates actionable insights. Submitted by: American Institutes of Research.","Health Provider PlatformBy using publicly available data sets and advanced data mining and linking techniques, our provider data platform draws from a plethora of public and private data sources that contain demographics/licensure, service, and quality information on providers that participate in Medicare, Medicaid, and Qualified Health Plans offered on the Affordable Care Act Marketplace and by commercial insurance plans (Figure 4). Our provider platform contains rich information on providers such as:• Hospitals and ambulatory care facilities• Physicians (including individual and medical groups), nurses, dentists, and pharmacists• Behavioral health providers and psychiatric residential treatment facilities""If the government developed a pilot product providing a common, machine-readable format for output with security and confidentiality, it would help researchers and others make more effective decisions. Submitted by: Palantir Technologies.","There are several examples from the Federal Government’s current data product offering that illustrate the value of a central repository that prioritize ease-of-use and compatibility:• Data.gov is an example of a central repository for the metadata of U.S. government data products. This portal provides links to open, machine readable formatted datasets hosted by various federal agencies. Within Data.gov’s data catalog, end users can search for datasets using different criteria.• The Federal Reserve Economic Data (FRED) is a central repository for U.S. and international economic indicators. FRED hosts 509,000 time series, including macroeconomic data like GDP and inflation, from 87 sources, including central banks from around the world. End users can access these data through FRED’s web-based user interface or its API service.• The National Centers for Environmental Information (NCEI) is the world’s largest repository of weather and climate data. It hosts data from land based, marine, satellite, radar, weather balloon, and paleoclimatic sources. The Federal Government should develop a pilot data product that embodies the principles and best practices of Leveraging Data as a Strategic Asset. For example, the government could develop a central portal for data relevant to first responders in natural disasters. This portal could integrate weather data from NCEI with population data from the Census Bureau. This product would aid in disaster response by enabling first responders to better predict the impact of natural events and allocate resources more efficiently. At the same time, this repository could provide the data provenance for each data series that it hosts; a common, machine-readable format for outputs; and ensure the security and confidentiality of the underlying data (and metadata).Leveraging Data as a Strategic Asset will allow the Federal Government to better govern and increase the effectiveness of its data products. The principles and best practices outlined in the strategy and in the comments above will make the government's data products more valuable and more accessible. Without affecting utilization, these structures will also protect the security of the data and the underlying confidentiality of respondents. Finally, researchers, managers, investors, and policymakers will be able to make more effective decisions based on these improved data.""If the Administration publishes federal management guidance in machine-readable data formats instead of documents, using the United States Legislative Markup (USLM) language, it would achieve the Cross Agency Priority goal of shifting from low-value to high-value work. Submitted by: Data Coalition.","Cross Agency Priority Goal 6, “Shifting From Low-Value to High-Value Work,” seeks to establish “regular processes to assess the burden [of OMB’s management guidance] on agencies and to rescind or modify requirements over time.” That is, the Administration should recognize the central role of documents in government data and encourage the application of modern “data-first” technologies for authoring documents and begin the process of defining a data standard and strategy specifically for guidance documents.ACTION:The Administration should publish federal management guidance in integrated, machine-readable data formats instead of documents. The Data Coalition’s work to pursue open data for laws and mandates provides a use case for exactly the same transformation, starting with Congressional laws, bills, and amendments. The United States Legislative Markup (USLM) provides an excellent framework for such a standard, and is already used for bills, laws, and applications within the United States Code and the Code of Federal Regulations.INTERNAL BENEFITS:By treating federal policy documents as data, and establishing standards for the drafting and interoperable dissemination/issuance of such documents, management and the agencies can more readily understand how policies integrate with existing policies and therefore work to comply more readily. Furthermore, executive leadership will have the ability to smartly and efficiently draft and issue policies while being able to measure compliance and successful realization of intended effects.EXTERNAL BENEFITS:The public and Congress will be able to more readily and fully understand the specific directives currently guiding and constraining agency programs and leadership. This will help guide and inform public policy conversations while further building trust in government. ""When Veritas deployed Data Insight at a large federal agency, it helped the agency distinguish between data necessary to run the agency and redundant, obsolete, trivial data. Submitted by: Veritas.","Use Case 1: Veritas deployed Data Insight at a large Federal agency to help them understand their stored data. The findings showed the following.- 58% of their data was dark, meaning that it hadn’t been accessed in over thirty-six months and had no owner in Active Directory.- 31% of their data was ROT (Redundant, Obsolete and Trivial) and had not been accessed for between one and three years.- 11% o4f their data was active information needed to run the agency.The agency took actions based on the intelligence it received from Data Insight. Veritas’ Integrated Classification Engine was used in conjunction with Enterprise Vault to scan the data and then, through policy, move it from primary storage to other cheaper storage areas seamlessly, with no impact on the end users. For example, the dark data was moved to Amazon Glacier. All sensitive data was moved to a long-term storage device with an automated seven year retention policy. The organization’s most important data was moved to high-speed, primary storage. This data strategy resulted in a much lower primary storage footprint, which moved a large amount of data out of their backup environment.The agency also used Data Insight to understand user behavior. At one point, this allowed the agency to detect a user copying a large amount of data that was outside of their normal workday. While the incident was minor, it could have been the prelude to a data breach. Data Insight gave them the tools to detect this behavior deviation.As a tangential benefit, the agency was now in position to start an eDiscovery practice using Veritas’ eDiscovery Platform. The eDiscovery Platform allowed the department to search all of their storage locations, email, SharePoint, desktops and other loose file locations for data that pertains to a particular matter. They used this tool to answer FOIA requests and legal searches. In the past, it took four or five days to respond to a small or medium sized request and redaction was almost impossible. With the eDiscovery Platform in place, they were quickly able to respond and redact in at least a third of the time.Legal hold matters were also an issue. They needed a tool that could notify and track users that were required to be on legal hold. They were tracking this on a spreadsheet. Now they had a tool that could create, monitor, track and escalate legal holds automatically. This freed up the time of one of their supervisors to do other work. Lastly producing data was streamlined for the FOIA and legal departments by placing all their search and redaction results into easy to configure and use files that could be sent to FOIA requestors or the legal team for their needs. ""When Federal Student Aid (FSA) began to develop an enterprise data warehouse more than four years ago, an integrated data dictionary was also in the works. The goal was to have analysts use common definitions in queries and reports, toggling directly to metadata within the warehouse without using separate Microsoft Word or Excel documents. Two years ago, the Department of Education launched the Cognos integrated data dictionary. Submitted by: Department of Education.","• 4½ years ago, as FSA began to develop an enterprise data warehouse (EDW) covering the full student aid lifecycle, an integrated data dictionary was planned from the very beginning. • The goal was to have analysts using common definitions in their queries & reports, with the ability for them to toggle directly to metadata within the warehouse without having to consult separate Microsoft Word or Excel documents.• Originally, the primary user interface to the warehouse was through the Cognos BI tool.o Creation of a Data Dictionaryo Dictionary is a user tool covering the data contained in the warehouse -- not intended as a product to cover all of the data elements in Federal Student Aido To include information such as:- The data source (both go-forward (“regular production”) and back-filled history)- aggregation level of the data element- student aid programs and years covered by the data element usability tips “tricks of the trade” informationo To integrate a guide to data values for commonly-used elements, including where the available values change over timeo To document FSA data standards and data definitions which have been established over time (and used for reporting, analysis, DRT requests, etc.) but that have not typically been documented in the pasto Data dictionary and data standards were intended to be updated as changes occur in student aid programs, policies, reporting, and source systemso Due to the huge scope of data content, the project initially focused on commonly-used warehouse tables, such as “Roll Up” tableso The process requires manual compilation of metadata• In August 2016 we launched the Cognos integrated data dictionary. We usually call the integrated data dictionary “IGC,” an acronym for the IBM software application: Information  Governance Catalog.• Warehouse users can right-click on a data element or table in Cognos and then select “Glossary” to enter the IGC. Once authenticated, IGC users also have the option to navigate via a URL to IGC production, which offers a fully-searchable interface. Content editors have additional access to click into IGC development.• In addition to providing the ability to toggle directly to dictionary content from within the warehouse, the IGC version of the data dictionary offers far more content than the warehouse vendor’s Excel data dictionary deliverable. • For data elements, here are some examples of the metadata:o Data level (such as whether the data element is person-level, loan level, grant level, school-level or servicer level)o The specific student aid Programs applicable to the data element)o Years available (generally the time-frame when regular processing of the warehouse has been in place, although some elements/tables store most-recent-month only)o Prior Data (indicates whether there are data available from before regular processing began, for example data from backfill and history dump)o Issues (includes data quality)o Usability and Availability (examples include tips and tricks for using the data)• At the table level, IGC also includes descriptive information which is applicable to the table as a whole.• There is a one-to-one relationship enforced between the warehouse data model and IGC. In other words, there are no data elements and tables in IGC which are not in the warehouse.• However, in April 2017 I added a Reference Addendum capability which is available to store useful information not specific to an individual data element or table. This allows for content in IGC which (1) applies to multiple tables or elements or (2) serves as reference for those who are running or writing various reports and dashboards. Examples include:o Changes over time in valid values for data elementso Full listings of valid values for selected key data elements• Possible future enhancements includeo An enterprise-wide data dictionary -- including source systemso Direct toggle access for warehouse users who are not using Cognos""If support is provided for Leidos to collect veteran suicide data from the VA Open Data Portal and Census, it will leverage its Global Monitoring and Planning System (GLIMPS) to compare rates of suicide in rural vs. urban areas. Submitted by: Leidos.","Use Case: Veteran Suicide MappingPreventing suicide among America’s veterans is a high priority both at the U.S. Department of Veterans Affairs (VA) and within the current administration. At Leidos, our mission is to make the world safer, healthier, and more efficient through information technology, engineering, and science. We offer this use case for Veteran Suicide Mapping in support of this mission and in support of America’s veterans.Twenty veterans commit suicide every day according to a 2018 analysis [1]. In fact, peacetime veteran suicide, which used to be well below the rate of the general public, rose above the civilian rate for the first time in history in 2008 [2]. It is a high priority at the VA to provide care to all veterans, regardless of their geographic location. In particular, a new presidential executive order directs the VA, Department of Defense, and the Department of Homeland Security “to collaborate to provide, to the extent consistent with law, seamless access to mental health care and suicide prevention resources for Veterans” [3].According to a 2016 research report on veteran suicide, there is a need for “comparison of suicide rates for residents of urban and rural areas” [4]. We propose mapping veteran suicide at the census tract level. This enables the VA to 1) identify areas to deploy additional mental health services, 2) identify veteran suicide groupings and map their location by classification (i.e., urban, rural, and highly rural), and 3) determine the scope of the problem when considering mental health care availability in urban versus rural areas (e.g., it may be necessary to deploy more Community Care resources to certain rural and semi-rural areas). For this geospatial data analytics use case, we will collect veteran suicide data from the VA Open Data Portal [5] and census tract data for urban, rural, and highly rural areas from the U.S. Census Bureau [6]. After aggregating and refining the data, Leidos will leverage our Global Monitoring and Planning System (GLIMPS), which is currently being used by America’s intelligence and special operations communities to predict global hotpots at a 96% accuracy rate, five years into the future.About Leidos - Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world’s toughest challenges in the defense, intelligence, homeland security, civil, and health markets. Recognized as a Top 10 Health IT provider, Leidos draws on decades of success to deliver a range of solutions and services designed to meet the healthcare challenges of today. A company of scientists, engineers, and technologists, we deliver a broad range of customizable, scalable solutions to hospitals and health systems, biomedical organizations, and every U.S. federal agency focused on health, including the Department of Defense (DoD), Veterans Affairs (VA), and Health and Human Services (HHS).References:[1] National Academies of Sciences, Engineering, and Medicine. Evaluation of the Department of Veterans Affairs Mental Health Services. National Academies Press, 2018.[2] Bachynski, Kathleen E., et al. ""Mental health risk factors for suicides in the US Army, 2007–8."" Injury Prevention 18.6 (2012): 405-412.[3] U.S. Department of Veterans Affairs. Office of Public and Intergovernmental Affairs - https://www.va.gov/opa/pressrel/pressrelease.cfm?id=4064[4] Department of Veterans Affairs. ""Suicide among Veterans and other Americans 2001-2014."" Washington, DC, Office of Suicide Prevention (2016). https://www.mentalhealth.va.gov/docs/2016suicidedatareport.pdf[5] U.S. Department of Veterans Affairs Open Data Portal. https://www.data.va.gov/search/type/dataset?query=suicide&sort_by=changed&sort_order=DESC[6] 2010 Census Urban and Rural Classification and Urban Area Criteria Geographic Branch - https://www.census.gov/geo/reference/ua/urban-rural-2010.html""When Socrata worked with Pennsylvania to standardize data sets, it allowed all of them to be accessed by API and facilitated rapid creation of a Opioids dashboard. Submitted by: Census Bureau.","Case Study #1 – Opioids PennsylvaniaSocrata worked with Pennsylvania to standardize the Commonwealths data sets from a variety of sources, allowing all the data, regardless of source, to be accessed via the Socrata platform's API interfaces. All of this is done in a secure FedRamp Moderate environment. This data is updated in near real-time from various data sources from which the Socrata platform draws the data.This allowed Socrata to quickly create and stand up a statewide dashboard for the Pennsylvania Opioid Operational Command Center and Public, which was deployed in just 3 weeks. This data also allows users to access and visualize data concerning Prevention, Rescue and Treatment in a single user experience where they can create filtered views and advanced visualizations that compare data that was previously siloed within separate agencies.Today, all relevant opioid-related data is available within the government of Pennsylvania by means of secure API for authenticated users, with capability to promote some or all to the public, entrepreneurs and civic innovators via our public API's as well. This provides for public accountability and allows private businesses to develop solutions and civic innovators to participate in hack-a-thons trying to find new ways to combat and solve the opioid epidemic.See: https://data.pa.gov/stories/s/Pennsylvania-Opioids/9q45-nckt/""When Socrata worked with DHHS to identify and standardize data sets, it facilitated creation of a predictive model for opioid overdoses to help better-allocate public resources. Submitted by: Census Bureau.","Case Study #2 – Opioids Department of Health and Human Services (DHHS)Socrata worked with DHHS to identify and standardize datasets from around DHHS, States and other Federal departments related to the opioid epidemic. This allowed all the data, regardless of source, to be accessed via the Socrata platform's API interfaces. All of this is done in a secure FedRamp Moderate environment. This data can be updated almost instantaneously from various data sources from which the Socrata platform draws the data. This data was then offered to entrepreneurs, researchers, nonprofits, and data scientists at the HHS Opioid Code-a-Thon the agency hosted in December of 2017.DHHS CTO Bruce Greenstein and DHHS CDO Mona Siddiqui's goal was to do the following. First, the goal was to save lives. Second, to show HHS internally and the public the power of sharing data. They wanted to liberate data from various silos and put it to work. Lastly, they wanted to bring together the community, that is researchers, entrepreneurs, startups and data scientists from around the country.They want to work together on something, on their own time, in a volunteer way, to make a difference in people’s lives. See Attachment3_DHHS Opioid Crisis. The HHS Opioid Code-a-Thon was very successful. It accomplished all three of DHHS' goals.There were 50 teams that executed 47,000+ API calls against the Socrata platform over a 24-hour period. The winning team was comprised of a Yale University medical student, a Google software engineer, a machine learning engineer from Palantir, a Johns Hopkins pain management physician, and a Stanford pain researcher, representing Origami Innovations.The team focused on and followed a human-centered, patient-centered, design-thinking approach that considered the whole journey in opioid addiction, including how pain medications are prescribed, how excess pills are dropped off, education on using them, and more.The team also wanted to consider the role of physicians, patients who need opioid medication, public health specialists, first responders, and others in the medical field in their solution, not just focus on those struggling with addiction. They found a data set that encompassed these attributes on the State of Connecticut's open data portal, which also uses the Socrata platform. They were able to create an application that used these datasets to construct a predictive model around locations of spikes in opioid overdoses and deaths. This will allow the proper allocation of public resources to help save lives. See Attachment4_Winning App for Opioid Crisis.""When states use the Socrata platform to standardize data, it shares data throughout state agencies and to the public. Submitted by: Census Bureau.","Case Study #3 – Transportation: Federal Highway Administration (""FHWA"") Funding Data As part of FHWA compliance, State DOT's must submit reports to the FHWA that answer a series of questions concerning: Safety; Infrastructure Conditions; Congestion Reduction; System Reliability; Freight Movement and Economic Vitality; Environmental Sustainability; and Reduced Project Delivery Delays.A number of States use the Socrata platform to standardize the data, have it accessible in one place via API's, internally and externally. This allows these States to:• Keep data in one place• Give more staff access to the same data• Automate the flow of that data• Visualize all key metrics• Create the right dashboards• Publish the accurate, real-time status of progress against goals• Easily drill down into the data to discover new insights• Broadly share the data throughout DOT and to external stakeholders and the public.A detailed Case Study on State DOT's using Socrata's Platform with FHWA data can be found in Attachment5_Transporttion Performance.""If the Data Incubator Project links data from DoD, DOL, VA, and state unemployment offices in a longitudinal dataset on veterans, it will transform research opportunities. Submitted by: Summit.","Data Incubator Project Use Case SubmissionThere are more data collected on military veterans throughout their lives and careers than perhaps any other segment of the American population. The Department of Defense (DoD), Department of Labor (DOL), Department of Veterans Affairs (VA), and state-level unemployment offices (UI), among others, independently track valuable data. Unfortunately, these data, and the wealth of knowledge they collectively contain, are unlinked. Our Data Incubator Project Use Case proposes to link the data together for the first time in a comprehensive, longitudinal dataset on veterans. The formation of cross-agency data-sharing agreements and the corresponding research possibilities will help veterans’ agencies collaborate at a new level of efficiency and effectiveness.The Data Incubator Project can expect the following returns on investment for our Use Case:• Increased data efficiency and capacity for veterans’ agencies. Cross-agency information sharing is currently unformalized and slowed by bureaucratic gridlock. Our Use Case will streamline data collaborations. At the end of Year 1, we expect to have an operational data warehouse accessible by multiple veterans’ agencies.• More impactful research. Currently, researchers note that veterans’ data are often difficult to access and/or to link to other data. Studies are limited to cross-sectional snapshots of one set of veterans during one time-period. Our Use Case will transform research possibilities through richer historical data and repeated observations on veteran activities and outcomes. We expect many of today’s correlational studies to be tomorrow’s causal impact evaluations. Year 1 will have an initial exploratory analysis. Year 2 and Year 3 will produce rigorous evaluations on the new longitudinal dataset.• Automated, self-sustaining data tools. The ultimate measure of success for our Use Case is its sustainability among federal collaborators. We will provide detailed, hands-on trainings to agency representatives to ensure that collaborators are proficient with the operation of the data warehouse. Throughout the project, we will ensure that that federal agencies can implement the data processes, replicate the tools, and perpetuate the project going forward.In Year 1 of our Use Case, we will develop memoranda of understanding, formalize cross-agency data sharing agreements, and complete an initial descriptive analysis on the data. At the end of Year 1, we expect to have an operating data warehouse and research hub. In Year 2 and Year 3, we will finalize research questions, study designs, and conduct the first evaluations and impact studies using the longitudinal data. They will use the descriptive analysis of Year 1 as a foundation. From Year 1 through Year 5, we will create and automate data dashboards for participating agencies and prepare recurring public use data files. We will conduct frequent trainings with federal agencies to sustain the effort and increase capacity for future data collection and evaluation. By the end of Year 5, we expect the data warehouse, data dashboards, and research projects to be self-sustained by the federal government.Our proposal team consists of agency leadership at DOL’s Veterans’ Employment and Training Services (VETS) and Summit Consulting, along with VA and DoD transition agencies. VETS, VA, and DoD bring unparalleled subject matter expertise on military service records, employment services and outreach to veterans, and benefits after military separation; Summit Consulting brings industry-leading data science capacity and more than 10 years’ experience evaluating federal programs. We anticipate that our Use Case will require roughly $400,000 annually.""When the Bureau of Transportation Statistics launched a curation project for the statistical data it creates, it supports the Federal Data Strategy's four strategic areas in various ways. Submitted by: DOT.","Use CasesBeginning in 2016, the Bureau of Transportation Statistics launched a new project to curate the statistical data it creates. The data curation project supports all four strategy areas in various ways. 1. Enterprise Data Governance:a. BTS requires the creation of data management plans for each datasetb. BTS requires data be made publicly available in open formatsc. BTS requires the creation of a machine-readable metadata file to accompany a datasetd. BTS requires robust documentation of data collection and all variablese. BTS has adopted ISO 8601 for dates and times""If a FEMA grants program pilot were revitalized, it would facilitate creation of an enterprise-wide big data solution. Submitted by: Hitachi Vantara Federal.","FEMA Grants Program DirectoratePilot program began more than a year and a half ago, but is stalled.Change agents exist and are motivated to create innovative solutions to improve how the government issues grants to ensure state and local communities are prepared for natural disasters. Innovators have identified the aspirational goal of using predictive analytics an AI to identify communities in particular risk and allocate funds they will most likely need based on historical trends and unstructured data. However, they are hindered in creating an enterprise-wide big data solution due to structural and cultural impediments within the agency regarding IT management. Would provide an optimal avenue for use given the vast amount of data, need for public to have access to it and amount of bureaucracy involved at the state, local and federal level.  Yet, this use case would be isolated enough to yield measurable results.""If the Federal Data Strategy supports a pilot, along the lines for the partnership between Census LEHD and University of Texas system, policymakers would achieve greater understanding of the impact education and training programs have on individuals' employment outcomes and economic mobility. Submitted by: Western Interstate Commission for Higher Education.","IntroductionUnderstanding the impact that education and training programs have on individuals’ employment outcomes and economic mobility is a crucial need for federal, state, and local policymakers, as well as the private sector, for a variety of reasons, including policy and practice improvement, identifying strategies for better serving students and workers, and making wiser investments with public dollars.Yet the disjointed data systems governing these sectors make using data intelligently to inform these purposes extremely difficult.The Western Interstate Commission for Higher Education (WICHE) proposes a voluntary data use case that would inform the Federal Data Strategy through improved Use, Access, and Augmentation of existing data resources, leading to better Decision-making and Accountability by expanding a federal state partnership to better use federal and state data.The use case would allow states to take advantage of existing federal data collections to turn siloed data into actionable insights about postsecondary education and training programs. It would do so by providing aggregated outcomes data on employment outcomes back to states. There is currently an example of this usage in the partnership between the U.S. Census Bureau Longitudinal Employer Household Dynamics program and the University of Texas system. This process could be scaled, expanded, and automated in ways that allow states and systems to receive custom aggregations.Additionally, there could be a statistical engine employed in the system that could carry out basic econometric functions without allowing access to the individual-level data. This use case could create an effective data partnership that would leverage existing resources and transform research offices across the country overnight while also protecting student privacy and enhancing data security. This would fill major gaps in state data and allow policymakers at the state and federal levels, local workforce training boards, and colleges and universities to have a much clearer understanding of the connections between training and education and employment.BackgroundThe federal government, as well as state and local governments invest substantial tax dollars in education and training. However, there are no comprehensive data sources that are capable of following students through the education or training pipeline and into the workforce (and back and forth, assuming some degree of “swirl” among many students) should these students cross state lines.The data all exist, yet their siloes require Rube Goldberg-esque data linkages to comply with legal requirements, satisfy political concerns, and settle “turf” disputes. In some states, and for some research questions, this may not matter, but for others, particularly states with large urban areas or institutions of higher education near a border, this limits the utility of their data system. Relatively straightforward questions – such as, how much STEM majors earn upon graduation from college, or how well state policies designed to improve college success and prepare students to meet future employer demand are working – are answerable to some extent if states have good data systems, but must include a litany of caveats about students and workers who may have crossed state lines.Finally, as America’s workforce grays, more jobs require postsecondary credentials, and a new round of automation takes flight, questions arise about providing enough trained and educated workers and ensuring that future students can adapt to the next “new economy,” where success is likely to be predicated on the ability to learn and adapt. Yet our policymakers at both the federal and state levels cannot use their data systems to learn how to prepare the next generation of America’s workforce.Expanding the Concept of the Census LEHD Postsecondary Employment Outcomes Partnership One potential solution that would bridge these data system gaps is to broaden the availability behind the concept of the partnership between the Census LEHD and the University of Texas System. To simplify this partnership, the UT system provided individual-level data on students, including institution and program. Census used this file to look up employment data on students and provided back customized aggregated data. Results are available here:https://lehd.ces.census.gov/data/pseo_beta.html.This initial result has been well received by employers, policymakers, and throughout the postsecondary research community and is an excellent example of what can and should result from thoughtful partnerships between governments, the private sector, and higher education. But now, we should think about how to do more. The concept is excellent, but there are several additions and enhancements that would improve the utility of this type of partnership. The ideas are sketched out in general below. It should be noted that these ideas are agnostic to the source of employment information. In fact, the federal government has repeatedly used other sources of employment data for transactional uses (such as verifying earnings for financial aid eligibility) or research and reporting purposes (such as the College Scorecard). Thus, the general term “employment data source” is used, which likely means the Census LEHD collection, but could also refer to Department of Treasury data.The Federal-State Data PartnershipThe Federal Government could establish a secure process through which entities (state agency or system researchers) who have entered into a standard data sharing agreement could supply individual level data with social security numbers and additional custom data elements. The system could then  retrieve employment information from a federal employment data source as well as potentially other relevant federal education data (such as information on student loans) to construct a research dataset in a secure environment.Research staff from the entity would not be able to access individual level data (except, perhaps for data elements they submitted), but the system would have analytical tools available through which users could carry out simple analytical processes (aggregations on certain variables, ratios, basic regressions, etc.) This suite of tools could be similar to the Datalab offered by the National Center for Education Statistics (https://nces.ed.gov/datalab/index.aspx).Currently, states are working through numerous different avenues to cobble together data resources with limited effectiveness that would answer key questions in a similar fashion as this potential tool.This partnership could immediately replace WRIS, WRIS II, SWIS, WICHE’s Multistate Longitudinal Data Exchange, a good portion of the College Scorecard, and numerous bi-lateral data sharing efforts with more complete and more accurate information.Clearly, this would be a complex undertaking, but it could be implemented incrementally. This work would leverage existing resources and the heavy previous investments that states and the federal government have made in longitudinal data systems.Data SafeguardsAny new data use presents potential risks to data security and student privacy. First, this system would be voluntary, so no state agency or system would be required to participate. Those that do would likely be providing a small amount of data.The most important safeguards would be to ensure that the aggregate data and/or analytic results coming out of the system do not compromise the privacy of any individuals. The Census (as well as other federal agencies) have extraordinary expertise in this area. Through limits on access, the number of related queries that can be conducted with a dataset, and the procedures developed through its partnership with the University of Texas System, this challenge can be overcome.Results from this Use CaseThis use case would transform the postsecondary education and training landscape. The range of existing tools and dashboards designed to provide consumer information could be replaced with clear and consistent measurements. Students and their families could understand not only what the benefit of enrolling in certain programs at a school might be, but also what the debt load might look like. States will be better able to understand how their policies are working as they administer federal programs to train new workers in emerging industries and provide employers with the workers they desperately need. Institutions could better understand the relationship between their academic programs and student services and skills development. Employers could more readily identify talent pipelines that fuel growth and profitability.Unlike proposals to create a federal student unit record system (on which our organization remains neutral) this proposal would provide usable information to state leaders by allowing them to create the custom analyses they need.Ultimately, such a data system would make federal investments in Pell Grants more efficient, by giving states an improved ability to analyze not just outcomes for Pell students, but how well their own policies interact with Pell grants and how that combined suite of policies impacts students in the long run (further bolstering the concept of a federal-state partnership). Additionally, this would allow economic development and workforce training agencies working near state borders to better understand how their efforts might complement one another or be redundant, again improving the efficiency of both federal and state investments.WICHE’s Role and Next StepsWICHE recognizes that the comments provided here are only a surface level treatment of an extraordinarily complex (but extraordinarily powerful) tool. WICHE has years of expertise working with states on issues relating to cross-state data sharing through the development and implementation of our Multistate Longitudinal Data Exchange, a system that links individual-level identifiable education and employment data across state lines. As noted above, WICHE remains neutral on the creation of a federal student unit record data system, as has been envisioned in numerous proposals (most recently through the College Transparency Act). This neutrality comes with a caveat that if the federal government elects to construct such a system, we strongly believe that it must take into account states’ needs (and current proposals fall short on this measure because they do not allow for states to create custom disaggregations).Our expertise certainly does not match that of Census staff and others in the federal government, but our data team stands ready to assist if this concept is deemed worthy of further exploration. WICHE can convene a team of state data staff and state policy leaders from across the country, as well as nongovernmental organizations, academics, and others to further discuss the concept, especially the components that would require state agency participation (such as the construction and submission of state data files).""If the USDA's Economic Research Service (ERS) leverages NYU's Administrative Data Research Facility (ADRF) to provide broad access to its data, it would help researchers derive important insights. Submitted by: USDA.","USE CASE PROPOSAL: Increasing Accessibility and Value of USDA Food Data Resources: USDA’s Economic Research Service (ERS) is interested in leveraging the Administrative Data Research Facility (ADRF) infrastructure at New York University (NYU) to provide the widest and most responsible access of its confidential and sensitive data to the public.  The project would house the 2008-2018 Information Resources Incorporated (IRI) food store scanner date and USDA’s National Household Food Acquisition and Purchase Survey (FoodAPS) data. Although ADRF is secure and FedRAMP approved, ERS will ensure that all regulatory aspects of security are met to provide the efficient and effective access protecting the privacy and covered.  The project will build a modern governance structure to provide efficient and effective access along with disclosure risk review processes.  Because these data are complex, an important dimension of promoting the use of the data for program evaluation and efficiency as well as research is to expand and develop the community of existing and new users.  Therefore, the strategy to widen use and usability for decision-making and accountability targets building communities with similar interests. ADRF will design and manage training for to expand and develop such users. One important community is state agencies that operate and manage food assistance programs.  ADRF and ERS will seek a partnership with USDA’s Food and Nutrition Service under which a number of states and their staff will be selected to be trained by empirically examining a set of well-specified questions, developed in conjunction with those agencies, with the goal of creating new tools and approaches that would improve food assistance program management and operational efficiencies.  Another important community are academic power users. They will be invited to use the system and provide guidance and comments on usability.  The ADRF team will also create a series of scholarships for junior researchers to access and use the data in the ADRF environment.The public is another important community.  However, a more efficient approach, other than training, is needed to tap into the latent information, including for commercialization.  The ADRF team will provide the leadership to investigate initiating an innovation competition to develop applications that can analyze and summarize the data across varying entities, generate estimates for a variety of variables of interest, and display the results visually, graphically, and in table form.  The ADRF will bring together key stakeholders to determine what the important variables are, the potential products and the structure of the competition.""When agencies implement Enterprise Data Governance, they facilitate timely research supporting decisions and course corrections. Submitted by: Enterprise e-Support.","Homeland Security Use caseThere are use cases in law enforcement and Intel communities where stakeholders are hard pressed on time to research data related to a subject or an event and make decisions and/or make course corrections. A situation involving a suspect, or special operations or war fighting, may demand actionable data, on time. At such time, a smart organization will know where their data is, what it means, and how it is connected to a variety of data domains. In the government, legacy application portfolio and reports are the only way to gain visibility into data. The reality is, users questions cannot always be answered via application views. Data analytics is a primary user function for decision making and it must happen in a self-service manner. At Enterprise e-Support, Inc. (EeS), we believe, knowing where data is, and what it means can resolve a lot of problems. With Enterprise Data Governance, the stakeholders and the data architects can build an enterprise level taxonomy using common vocabularies such as NIEM and map all enterprise  data assets into it for sub-selection and visualization of enterprise data. This sub-selection or querying of data can happen in a self-service manner. It also important that Enterprise data governance happens all the time. EeS solution supports governance groups deal with ongoing changes to database portfolio and ingest them into the taxonomy as they happen. This ensures the queries generated automatically via enterprise taxonomy and mappings are accurate. A self-service query generation, execution, and business Intelligence culture will strengthen data stewardship because stakeholders now know how data is organized. Increased familiarity with data structure and relationships within data assets will help stakeholder mark sensitive data to its distribution. Elements that cannot be mapped to the enterprise taxonomy could be obsolete objects, stale or duplicate copies of data. Removal of obsolete objects and stale copies of data will optimize data environment. This may also help agencies progress towards a single source of data. The above approach also provides for a mechanism to build an enterprise data warehouse (EDW) for BI. With all the lineages and data context known, it is easier to select data for specific goals and provide necessary data and critical data points via EDW. Architectural visibility of data layer not only facilitates fast access to data, it also helps generate impact reports involving other layers. Governance of touch points such a what business function accesses and/or creates data via which systems or service and is hosted on what digital resource can be very useful in understanding the mission holistically. Use of common vocabularies will not only help with data sharing inside the agency, it will also help with information exchanges with external partners. Enterprise e-Support is also modeling its solution to include auto generation of classifications and data assets mappings. It is further developing solutions for automation and accountability by using immutable block chain for data sharing. The application is delivered via AWS public cloud and also tested for readiness on government-approved mobile private cloud in case a  special operations and/or war fighting team needs it on their vehicles and/or trailers and stay connected with team data via satellite access. With the above, we believe we can assist with all 4 data strategy areas, Enterprise Data Governance, Use, Access and Augmentation, Decision making and Accountability; and Commercialization, Innovation, and Public Use.  Below are some screenshots of EeS Enterprise Information Management (EIM) Solution to build taxonomies, mappings and sub-selection and visualizations.Enterprise Data GovernanceClassification, Mapping, Policies (Standard Vocabularies, Updates to Enterprise Taxonomy)<Diagrams excluded>Visualizations<Screenshots excluded> Use, Access and Augmentation Sub-selection and Query Generation: <Screenshots excluded>Decision making and AccountabilityThe above query is executed in analytics environment and data visualizations are generated via open source BI tool packaged with the solution. This provides better understanding of data for decision making. The solution removes and/or redacts data via a masking algorithms for sensitive elements if user does not have appropriate access or privilege level. Immutable block chain is being implemented for automation and accountability of data sharing.Commercialization, Innovation, and Public Use.The solution offered via AWS as software as a service. AWS offers swift deployment of hardware and software, user provisioning and billing to get our users up and running in no time. The solution has been tested on private and mobile clouds built on government approved hardware (ATO will be easy to obtain). AI and Block chain capabilities are being developed into the EIM solution. ""If data is linked among VA, DOJ, and Census, it will help the Veterans Justice Outreach initiative to avoid unnecessary criminalization of mental illness and extended incarceration among Veterans. Submitted by: University of Pennsylvania.","Veteran Homelessness:The goal of the federal Veterans Justice Outreach (VJO) initiative is to “avoid the unnecessary criminalization of mental illness and extended incarceration among Veterans by ensuring that eligible, justice-involved Veterans have timely access to Veterans Health Administration (VHA) services .” One key program that supports this mission is Health Care for Re-entry Veterans which connects previously incarcerated Veterans with health and human services supports to reduce recidivism. Unfortunately, recidivism rates can’t adequately be tracked across the population without linking service delivery data from the Department of Veterans Affairs with data from the Department of Justice on state prison entry and exit. In order to better evaluate this important program and make adjustments to improve implementation, the National Center on Homelessness among Veterans has proposed that linkage between VA and DOJ data occur at the U.S. Census Bureau, and is actively working with representatives from all three departments  to negotiate a path forward. ""If the government creates a decentralized immigration ecosystem allowing multiple agencies to access mission-related information while protecting individuals' records, it would help organizations perform better. Submitted by Anonymous.","As the former DHS/ICE Chief Data Officer and technologist at heart, I would like to present the notion of a decentralized immigration ecosystem that would allow multiple Federal Departments/Agencies to have access to relevant information needed to carry out their statutes but to protect the immutable records of the individual interacting with the US Government throughout the immigration life cycle.  Contrary to popular belief that the approach should be one large consolidation of data, industry is showing that blockchain reference architectures hold great value where a) the trust between entities is low b) interdependence on one system is an operational risk c) transparency in transactions is critical and d) there is a high volume of transmitted information.   This data incubator use case would provide numerous law enforcement, health, benefit, and national security entities with the information they need to perform rather disparate activities.""If administrative data sources linked across multiple state systems are leveraged to analyze parental employment, continuity of care, and other outcomes, the Administration of Children and Families (ACF) will be better able to determine impacts of Child Care and Development (CCDF) program changes. Submitted by: Chapin Hall.","Program Evaluation of Subsidized Child CareThis project aims to evaluate important statutory changes that resulted from the federal Child Care and Development Block Grant (CCDBG) Act of 2014 in the state of Illinois. The evaluation focuses on changes to the Child Care and Development (CCDF) program including extending the redetermination period for child care subsidy benefits and the implementation of new provider training and monitoring requirements. We will analyze the impact on children, their families and the workforce, specifically outcomes around parental employment; continuity of care; child outcomes; and employment outcomes for providers and monitors, by leveraging administrative sources linked across multiple state systems. Evaluation activities will be conducted in an environment that enables secure data access for the agency and evaluators. This resource will allow secure access to both state agency and external analysts to the same datasets and facilitate data sharing with the evaluators. This project is sponsored by the Office of Planning, Research, and Evaluation (OPRE), an Office of the Administration for Children and Families (ACF), U.S. Department of Health and Human Services (HHS). ""If the federal government provides guidance regarding inter- and intra-agency best practices and supports local governments with data access respectful of individual privacy, Kansas City would be better-able to navigate data-sharing challenges to evaluate areas of opportunity regarding high utilizers of emergency medical services. Submitted by: City of Kansas City, Mo.","My potential use case falls under the pillar of Access, Use and Augmentation. The City of Kansas City, Missouri is currently bringing together a network of locals actors in government (e.g. City and County), non-profits and hospitals to evaluate areas of opportunity regarding high utilizers of the City's emergency medical response system. Goals of this group include reviewing analytics in order to better understand and identify characteristics of high utilizers, identifying tools and strategies to more effectively reduce high utilizer issues, and to establish and monitor shared high utilizer measures of success . Data-sharing and navigating HIPAA compliancy between agencies has been one of the biggest challenges to the working team.  This is an area where the Federal Government could provide guidance regarding inter- and intra-agency best practices, and further assist local governments by leveraging a tool for data access that is respectful of individual privacy. This use case could help transform the effectiveness and insights of a local network of care for local/regional emergency service professionals and greatly benefit their patients and clients that are frequently exposed to trauma and negatively reinforcing fluctuations in living conditions.""When the Young Invincibles published the Student Agenda for Data Reform, it set out widely-supported principles for enhancing federal data systems to benefit young adults seeking education and training opportunities. Submitted by: Anonymous.","Thank you so much for allowing me to speak on behalf of Young Invincibles and our work to enhance federal data systems to benefit young adults seeking education and training opportunities. Here is the link to the Student Agenda for Data Reform, a set of principles now supported by organizations representing over one million students:http://younginvincibles.org/wp-content/uploads/2017/04/Student-Agenda-Data-Reform.pdfMaintaining and enhancing the College Scorecard, as well as pursuing further data matching between the U.S. Census' LEHD program and higher education systems like University of Texas and the state of Colorado are two specific examples that would advance this agenda and the federal data strategy overall.""When the National Head Start Association launched the Data Design Initiative to address the knowledge gap about which programs and practices are effective, it advanced the cause of making early childhood data more useful. Submitted by: Anonymous.","Early Childhood Use CaseNumerous studies have found that high-quality early childhood programs, birth to age 5, can greatly help children in poverty fare far better in life.  Perhaps most well-known among those studies is the Perry Pre-School Study (https://highscope.org/perrypreschoolstudy), a randomized control trial done in the 1960’s. Subsequent studies have yielded similarly positive findings. See, for example, http://www.people.fas.harvard.edu/~deming/papers/Deming_HeadStart.pdf) about the Head Start program, which serves over 900,000 children in poverty every year.Unfortunately, little is known about which programs and practices are likely to work better than others, especially for different children and different domains of learning (e.g., literacy, numeracy, social-emotional). Is the High Scope program, created to replicate the Perry Pre-School practices, better or worse overall than some of its competitors, such as Creative Curriculum and Frog Street?  If it is better overall (on average), is it also better in each domain?  Similarly, is it better for all children or do other programs work better for children with certain characteristics (e.g., learning styles)?  Is the success of some programs likely to vary by parent characteristics (e.g., language spoken at home, drug dependency)?  Little is known, too, about whether current programs have improved over time, and, more important, how they can be improved further.Despite an abundance of data being gathered and supplied annually or more often by Head Start and other programs and reams of research, it is surprisingly and distressingly difficult for early childhood providers to learn from their own and others’ experience which curriculum and practices (e.g., absentee-reducing) work well and, especially, which are likely to work better than others in different situations. Similarly, despite valuable research on adverse childhood experiences, current data systems do not provide early risk warnings to detect modifiable stressors in the home nor support priority-setting.  In short, it is far too hard to identify, objectively, lower cost, higher return ways to serve the nation’s most vulnerable children.A big part of the problem is the way early childhood data are gathered, shared, and analyzed.  For example, although Head Start programs gather data at the child level, they report it to the federal government only at the grant recipient level.  This is, in part, because the federal government is not allowed to gather child-level data. Others could combine child-level or classroom data, but few currently do.  The lack of child-level data makes it hard to look across programs using different curriculum, such as High Scope and Creative Curriculum, or different practices, such as absentee reduction or book-reading campaigns, to look for patterns, similarities, and relationships that will help those on the front line working with children and their families understand what is likely to work better for children with different experiences, learning styles, or other characteristics.  This absence of insights is true even in the few states starting to develop kindergarten assessments. Another problem for the 1600 Head Start providers is that they use several different programs to collect data for different purposes – such as administrative recordkeeping, attendance, teacher assessments, and child progress assessments. For all but a small subset of Head Start providers, the systems used to collect these data cannot currently communicate with one another, even within a single center. This makes it excruciatingly difficult to sense, for example, how attendance problems might be affecting a child’s progress.  Nor does it make it easy to detect patterns, such as when absenteeism goes up and whether some teachers have lower absentee levels than others. If it did, it would support discovery of programs and practices with greater potential to reduce problems.  Another problem is that the vendors providing Head Start assessment programs do not make it easy to compare children and programs in similar circumstances (e.g., language, rural) across locations to look for positive outliers who might be doing something that, if replicated, would yield similarly favorable results in other locations. To address these problems, the National Head Start Association, serving the 1600 Head Start providers, launched the Data Design Initiative in late 2017 to make early childhood data more useful. See https://www.nhsa.org/our-work/initiative/data-design-initiative for a description of the initiative, the problems that led to its creation, and projects underway. NHSA has embarked on a multi-stakeholder effort, involving Head Start providers, current and potential suppliers/vendors/trainers, multiple levels of government, and others in the non-profit and research community to generate greater insights from early childhood data to enhance results for Head Start children and their families.  Working with this federal cross-agency priority goal team would likely reveal new methods and data sources for harvesting more meaning and better outcomes using existing, refined, and evolving early childhood and related data sets. ""When South Carolina launched an Integrated Data System, it made it easier for the government and independent evaluators to assess the impact of government programs. Submitted by: J-PAL North America.","South Carolina’s Integrated Data SystemSome systems and infrastructure can be put in place to enable lower cost and more efficient research. An integrated administrative data system across government agencies is one such model that makes conducting randomized evaluations (and other forms of evaluation) more efficient. For example, the integrated data system South Carolina has built over time, which is housed at the Revenue and Fiscal Affairs Office, has made it easier for the government and independent evaluators to assess the impact of government programs. This has made South Carolina a promising location for researchers interesting in answering crucial policy-relevant questions. As just one example, South Carolina’s Department of Health and Human Services (SCDHHS) has partnered with J-PAL affiliates to evaluate how expanding the Nurse-Family Partnership through a pay-for-success contract affects the health and long-term economic outcomes of mothers and children enrolled in the program. The integrated data system South Carolina has built has been a crucial foundation for enabling a cost-effective, reliable evaluation design. From the perspective of the research team, it has been very helpful to work with a central entity (the Revenue and Fiscal Affairs Office, or RFA) for a number of reasons: ● clear data documentation and standard data request processes are in place.● staff are in place to receive data requests and shepherd them through the data use agreement process.● RFA can enable linkages across data sets from different departments and agencies and coordinate the data use agreement process. This is particularly important in cases where we are evaluating programs that may affect people’s lives across multiple outcomes in ways that do not match the silos of government agencies. ● This administrative data infrastructure enables us to measure the impact of the program without incurring the cost and the burden on study participants of following up with surveys and original data collection.● This example also highlights the value of having similar coordination at the federal level that would enable data access across states and across different federal sources of data.""If the National Association for State Chief Information Officers (NASCIO) and similar organizations were leveraged to share enterprise data governance practices, it would help make data sharing and oversight between federal and state governments easier. Submitted by: The MITRE Corporation.","Find easy ways to spur collaboration with state governments.Example: The National Association for State Chief Information Officers (NASCIO) is an organization of state governments that serves as the common forum for collaboration and the exchange of ideas for state CIOs.• Common Solution• Federal and state governments regularly need to exchange data in a number of different areas. Finding ways to make the sharing (and oversight) easier would be beneficial.NASCIO could be leveraged to share enterprise data governance practices. Alternatively, a similar organization can be created specifically supporting Chief Data Officers.30[30 Johns Hopkins University is already attempting to expand its local community of CDOs to include the U.S. stateCDOs.]""When the Center for Open Data Enterprise documented the policies, resources, and best practices they learned about through a series of Data Roundtables that they co-lead with the White House and Department of Treasury from 2016-2018 into a single library: usopendatatoolkit.org, then federal data providers and data users had a one-stop-shop to get a landscape view of open data related resources. Submitted by: CODE.","On July 24, 2018, CODE launched the U.S. Open Data Toolkit (usopendatatoolkit.org) as a resource to help ""federal data providers and data users better understand and harness the strategic value of open government data."" The Toolkit draws heavily on the Open Data Roundtables that CODE has held with the White House and federal agencies, particularly a series of four Roundtables held with the Office of Science and Technology Policy (OSTP) in 2016 and the three Roundtables held with OMB and the U.S. Department of the Treasury in 2017 and 2018. Three elements of the Toolkit are particularly relevant for the Federal Data Strategy: Best Practices: This section synthesizes recommended best practices for data collection, management, and publication based on input from hundreds of data experts at the Open Data Roundtables. These recommendations reflect the perspective of data users as well as providers, since the Roundtables have all included participants from both inside and outside of government. They represent an objective, nonpartisan synthesis of a wide range of approaches relevant to the issues the Data Strategy will address.""When the Department of Homeland Security formed a Data Stewardship Tactical Working Group of 700+ federal and industry members, then they had the authorities and enterprise to collectively coordinate with DHS data owners to appoint and train stewards and develop and support inter and intra-agency data: standards, glossaries, and integration quality assessments. Submitted by: Data Stewardship Tactical Working Group .","The DHS Data Stewardship Tactical Working Group – A Common Data Governance Model Use CaseCommon Operating Data Governance Model – The DHS Data Stewardship Tactical Working Group (DSTWG) and Data Community members established in January 2014 with less than a dozen members and now has over 700 federal and private industry voluntary members.  This Community consists of stewards, business operators, policy, privacy, developers, enterprise architecture, analyst, statisticians, strategist, lawyers, data scientist, oversight and auditing entities, data governance and data management practitioners.  This Community has come together and by building trust and promoting maturity through identifying and providing solutions this Community is continuously moving forward and growing.The information the government collects, utilizes, and disseminates needs to be done so under a model that allows for a successful implementation of a Federal Data Strategy that can be measured for health and maturity under a formal governance umbrella.  This is a flexible model that can be applied in any organization.This Community operates under the proverb “No Problems…Just Solutions” and offers to the Federal Data Strategy the opportunity to gain insight into this Communities approach and model.  The subsequent information outlines the DSTWGs techniques and strategy: 1) Collaborate, engage, and develop partnerships 2) Foster Stewards and the Stewardship Framework 3) Develop standards and definitions and remain flexible and fluid 4) Provide Services such as varying types of analysis, quality reviews, development, fit for use requirements development, and measures 5) Produce products, tools, processes, and services.This DSTWG Data Community, in collaboration with Data Governance entities across the Department of Homeland Security and our information sharing agency partners, has developed, applied, and accomplished the following:• Utilize Use Cases – Intake of the business processes that have apparent and identified capability gaps in information and information sharing • Appoint and Support Stewards – Internal control and management of a standard.  Stewards develop the standard and oversee the change management (as a collateral duty)• Support and Facilitate Standards Development – Agreed upon reference data for the enterprise – included in the DSP• Support and Facilitate the Glossary Development – Agreed upon terminology for use across the Immigration Domain and when communicating with external parties – Included in the DSP• Consolidate and Validate DHS Standards Package (DSP) – makes it possible to understand the context of data and document it.• Develop and Support Data Integration Quality Assessment (DIQA) - Measure the health of the reference data with the standards and the enterprise immigration systems where the reference data persists.• Develop the Structure from the Data Standards Package (DSP) into Collibra – Data governance tool that will house all the information from the DSP and expose the information for use.• Data Governance Training – Providing Data Governance Training to the Community for industry expert, John Ladley""If granular, front-line, real-time, and geographically diverse data sources on opioids are combined in a secure environment and shared in aggregate via a public Opioid Epidemic Surveillance and Response Dashboard, then community members, health professionals and policymakers at all levels, will be able to observe and react to prescription and overdose trends in a more timely and location specific manner. Submitted by: NY United Methodist Church.","COMPREHENSIVE AND ACTIONABLE DATA ON THE OPIOID EPIDEMIC:  HARNESSING CONFIDENTIAL MICRODATA TO BUILD A NATIONAL OPIOID SURVEILLANCE DASHBOARDThe country is in the midst of a rapidly shifting opioid overdose epidemic. In 2016, 1 in 65 of all US deaths were opioid related – opioid overdoses now claim more lives per year than HIV/AIDS did at the height of the epidemic. For every fatal overdose, an unknown but large number of individuals suffer from chronic opioid abuse and non-fatal overdose, with devastating individual and societal consequences. The face of the epidemic is also evolving and shifting in ways that challenge traditional epidemiologic surveillance methods—from prescription opioids in 1999-2010, to heroin in 2010-2013, to illegally manufactured synthetics like fentanyl currently. To support effective, forceful and nimble action to quell the opioid overdose epidemic across the United States, it is essential to harness the rich trove of confidential microdata on the opioid supply and on indicators of opioid-related harm to improve national capacity to reliably track its evolution, predict where and who it will affect next, and identify new and promising interventions.  Timely access is needed to comprehensive and geographically granular data over time on: (1) state- and local-level laws, policies, and services that may impact opioid-related harm; (2) indicators of the shifting legal and illegal drug markets; and (3) measures of opioid overdose and associated outcomes.  While multiple disparate efforts have assembled data on the opioid overdose epidemic, no single national database resource of requisite depth and scale exists at this time. Building such a resource would advance the Department of Health and Human Services’ 5 Point Strategy to Combat the Opioids Crisis, by addressing Point 2: Better Data. A national Opioid Epidemic Surveillance and Response Dashboard (OESRD) can integrate novel and existing data sources addressing: (1) laws and policies that regulate the prescription opioid supply and mitigate opioid-related harm (e.g., prescription drug monitoring programs, pain clinic regulations, prescription limits, naloxone access laws, overdose-related Good Samaritan laws, state restrictions on opioid treatment programs, and Medicaid coverage of medication assisted treatment); (2) access to opioid use disorder services and medications (e.g., overdose education and naloxone treatment programs, opioid treatment programs, clinicians with DEA DATA waivers to prescribe buprenorphine, treatment admissions for opioid use disorders, retail sales of naloxone and medication assisted treatment); (3) opioid supply (e.g., opioid prescribing and dispensing patterns, heroin price and purity, fentanyl and heroin law enforcement seizures); (4) opioid use, overdose, and related outcomes (e.g., opioid overdose-related deaths, opioid-related hospitalizations and emergency department visits, survey reports of opioid use, misuse and disorder); (5) social, economic, and physical environment factors associated with opioid overdose risk (e.g., high school graduation, unemployment, inadequate housing); and (6) compiled evidence on what works to prevent and treat opioid misuse, overdose, and opioid use disorders, the strength of such evidence, and examples of where prevention and treatment approaches are being implementedSpecific emphasis should be put on acquiring data that are:  1) granular, 2) front-line, 3) as close to real-time as possible; and 4) integrated across diverse sources and multiple levels of geographic resolution (as applicable), including states, counties, metropolitan statistical areas, cities, census tracts, and census blocks. An OESRD can provide a secure platform to host multiple datasets using confidential microdata, including hospital discharges, vital statistics, prescription retail sales, and drug seizures, among others. The platform can be dynamic, enabling government agencies to add confidential data as soon as it becomes available, and promoting collaboration across government agencies, universities, and community leaders, among others. Analytic, mapping and visualization tools can be embedded and made accessible online, and public versions of the data can be offered in easily downloadable, user-friendly spreadsheets to meet the needs of diverse constituents.  Building high-quality, public-facing dashboards to meet the needs of multiple stakeholders is a complex endeavor and requires a team with substantive relevant experience.The OESRD would allow community members, health professionals and policymakers at all levels, spanning rural and urban settings, to access timely data to identify trends in prescribing patterns, drug markets, and opioid overdoses, identify emerging geographic areas of risk, define targeted strategies to markedly reduce overdose rates, and track progress towards reaching strategic goals. At the national level, it would equip policymakers and researchers with the tools to track and anticipate the evolution of the opioid epidemic, to identify geographic and socio-demographic groups at highest risk, to refine hypotheses about key drivers of the epidemic, and to evaluate the effectiveness of current approaches to this public health crisis. ""If IRS and Census data was combined with employers (via voluntary public partnerships w/ CARRA, and the US Chamber of Commerce), then employers' ability to act as engines economic mobility could be measured by connecting new hiring cohort data with long-term labor market outcomes and earnings profiles to provide aggregates statistics on wage trajectories by SOC within employers or employer group Submitted by: T3 Innovation Network .","Enabling the Availability of Consumer Information on Employers as Engines of Economic Mobility. Building off of the partnership between IRS and Census in allowing new uses of tax records to create public-use datasets through CARRA, several members of the US Chamber of Commerce have expressed a strong interest in measuring their ability as employers to act as engines of economic mobility by voluntarily creation a public-private data collaborative with Census and providing hiring data for cohorts of new hires to connect with long-term labor market outcomes and earnings profiles and provide aggregates statistics on wage trajectories by SOC within employers or employer groups. This would be similar in spirit to College Scorecard in providing new consumer-facing information on the return on investment for early apprenticeship and employment at different large employers. Several funders have expressed interest in supporting this work. ""When NOAA made it's Earth-observing satellites and ground-based radar weather data available as open data, their public-private partnerships were able to create an economic return on the order of 5 to 1; in other words, for each $1 billion in federal investment, $5 billion in revenue is realized by NOAA’s private sector partners. Submitted by: SAS Institute Inc..","One example of such commercialization is NOAA making its weather data available for use by the private sector. The data is rich and derived from a variety of sources, ranging from Earth-observing satellites to ground-based radar. Access to such rich data sources has already facilitated the development of a profitable public-private partnership, with significant economic return on U.S. investment. Of note, the economic return on NOAA’s weather data alone is estimated to be on the order of 5 to 1; in other words, for each $1 billion in federal investment, $5 billion in revenue is realized by NOAA’s private sector partners in their Weather Enterprise (NOAA Climate Partnership Task Force 2011).""When Davies Ward Phillips & Vineberg transitioned from back up tapes to the public cloud, Total Cost of Ownership was reduced by half, data efficiency improved by a factor of 7, the physical data center footprint was cut by 91%, and Recovery Point Objectives were reduced from the previous 12 hours to just minutes. Submitted by: Rubrik.","USE CASE - DAVIES WARD PHILLIPS & VINEBERGA typical example of a fast-moving organization that was attracted to the cloud is Davies Ward Phillips & Vineberg,a large international law firm. Davies Ward believed cloud would allow the firm to invest in its core business ratherthan in IT. The firm’s IT staff had begun the transition away from legacy IT infrastructure by replacing old backuptapes with virtualized infrastructure and wanted to move more aggressively to the cloud.“Our previous solution wasn’t meeting our SLAs,” said Evans Vogas, the firm’s network operations analyst.“Backups were going longer than the backup window,” he said. “Second, replication did not work very well in ourprevious solution; it would create a backlog and require manual cleanup. It was also incredibly complex to set upand manage, especially archival to public cloud. You had to be a rocket scientist.”To help manage the transition, the firm turned to Rubrik. Quickly, according to Vogas, the firm saw numeroustangible benefits: TCO was reduced by half, data efficiency improved by a factor of 7, the physical data centerfootprint was cut by 91%, and RPOs were reduced from the previous 12 hours to just minutes.By using Rubrik for backup, disaster recovery, replication, public cloud archival and test/dev across physical, virtualand cloud environments, the firm was able to modernize its IT framework and cut costs in the process. Archiving,in particular, was a workload where Rubrik helped the firm move aggressively to the cloud. “We had been lookinginto public cloud archival for over a year, but the ones we found were complex to set up,” Vogas said. Onerequirement with Rubrik was that it had to work easily with any public cloud. “Since Rubrik encrypts data sent tothe cloud, you are protected,” he said.""When the State Department implemented MeterNet, a smart-metering platform for collecting real-time utility and facility equipment data, they began saving $32K annually at each site and reduced hundreds of thousands in annual travel costs by enabling engineers to conduct remote performance audits.If the State Department uses its automated MeterNet utility data for predictive analysis, they will be able to forecast equipment failures months before they may occur, prevent catastrophic failures, and extend the lifespan of equipment by reducing frequency of replacements and related costs.    When the State Department, in partnership with the science community and EPA, implemented DOSAir, a network of air monitoring sensors that reports hourly on the air quality conditions of nearly 40 posts, they were able to begin developing health messaging and standard operating procedures for U.S. personnel and citizens living abroad whose health is increasingly threatened by air pollution. If the State Department combines its automated DOSAir air quality data with NASA satellite data and applies machine learning to forecast air pollution, then U.S. personnel and citizens abroad can reduce prolonged exposure.  When the State Department implemented Telematics, a secure sensor network that automatically collects data from Department vehicle computers, they could apply centralized analytics to: review fleet performance, reduce costs by identifying anomalies in vehicle performance such as dangerous driving and fuel theft, predict maintenance needs to extend vehicle lifespans, determine which vehicles are best suited for each post, and profile the safety record of each fleet driver for managers.   Submitted by: The Office of Management Policy, Rightsizing, and Innovation:.","Internet of ThingsM/PRI uses data from the Internet of Things (IoT) devices in combination with cloud computing, machine learning and interactive data modeling. This data helps the Department enable better management controls, optimize resources, and improve security.  The Department of State's IoT programs include:MeterNetMeterNet is the Department’s smart-metering platform for collecting real-time utility and facility equipment data.  Its primary benefits are the ability to measure, report, and verify the performance of worldwide facilities, enact management controls over Department resources, and embed additional layers of security on facilities.  On average, MeterNet data saves $32K annually at each site it is deployed and reduces hundreds of thousands in annual travel costs by enabling engineers to conduct remote performance audits.  The next stage for MeterNet data includes predictive analytics to forecast equipment failures months before they may occur.  This forecasting would help prevent catastrophic failures and extend the lifespan of equipment, reducing frequency of replacements and related costs.    DOSAirDOSAir is a network of air monitoring sensors that reports hourly on the air quality conditions of nearly 40 posts.  Over the past decade, air pollution has become an increased health risk to U.S. personnel and citizens overseas.  Partnering with the science community and EPA, the Department is working to develop health messaging and standard operating procedures for U.S. personnel and citizens living abroad.  The next stage for DOSAir includes combining DOSAir ground sensor data with NASA satellite data and applying machine learning to develop forecasting of air pollution.  This will help U.S. personnel and citizens reduce prolonged exposure while showcasing U.S. technology and science.  TelematicsTelematics is a sensor network that automatically collects data from Department vehicle computers and enables centralized analytics and review of fleet performance.  Telematics will reduce costs by identifying anomalies in vehicle performance such as dangerous driving and fuel theft, and predicting maintenance needs to extend vehicle lifespans.  Telematics can determine which vehicles are best suited for each post, and profile the safety record of each fleet driver for managers.  The devices being deployed are built with two new industrial property patents related to cybersecurity and were invented by the Department.  ""If cities, like Kansas City, MO, had access to USPS vacancy data, they could combine that with other vacancy proxy data, such as utility (water/sewer) data, to effectively address vacant and mitigate economic distress related to blight.  Submitted by: City of Kansas City, MO.","Vacancy in housing is a major problem that influences blight and economic distress. To effectively address vacancy, however, requires understanding where it is occurring. As a dynamic phenomenon, it can be hard to track and understand over timeThe City of Kansas City, MO has endeavored to achieve a better understanding of vacancy on a static, snapshot basis, but could be better positioned to focus resources on areas with higher vacancy if we could track it as it occurs. Local utility data (water/sewer) provides one set of insights into potential vacancies, but needs to be cross-referenced with another set of data to distinguish lack of water use that is due to vacancy as opposed to other factors (i.e. water theft, broken meters). The USPS vacancy data is utilized by many cities in some format, but is hard to obtain access to. Setting up better access to USPS data that would allow for cross-referencing with other vacancy-oriented administrative data at the city level would allow cities to obtain a much better real-time understanding of the dynamics of real estate and residents in their city, and react with the appropriate resources.""When the Department of Homeland Security spearheaded the National Information Exchange Model (NIEM.gov),  a common approach to multi-agency data sharing for the Departments of Homeland Security, Justice, and Health and Human Services, they were able to build interoperable extensible data standards for multi-agency counter-terrorism and law enforcement projects.If the Federal Government endorses NIEM as the government-wide default for data standardization, in coordination with OMB and GSA, they may be able to build interoperable extensible data standards for multi-agency projects, beyond counter-terrorism and law enforcement. Submitted by: Data Coalition.","The federal government has very few data formats for sharing mission or programmatic related data across multiple agencies. One prominent exception is the Department of Homeland Security lead National Information Exchange Model (NIEM).To build a common approach to multi-agency data sharing, the Departments of Homeland Security, Justice, and Health and Human Services created the National Information Exchange Model (NIEM) (see: https://www.niem.gov/), which publishes a data dictionary of common fields (the ""NIEM Core"") and helps agencies create formats using those fields. (In the summer of 2017, version 4.0 of the NIEM Core will incorporate the Legal Entity Identifier.) NIEM is now used extensively to build formats for multi- agency counter-terrorism and law enforcement projects, but has tremendous potential for use beyond these mission areas.ACTION:The Administration should consider endorsing NIEM as the government-wide default for data standardization and publication projects. The Administration should explore moving the NIEM management team (three-five full-time equivalents) into a government-wide leadership office such as OMB. In coordination with the data standardization work of GSA’s US Data Federation (an outgrowth of the Data.gov effort) and Project Open Data, NIEM stands poised to foster a base of standardized material data to inform the natural harmonization of common mission data within agency environments.INTERNAL BENEFITS:Adopting a NIEM-first policy for federal mission or programmatic related data sharing projects will ensure that agencies at least consider reusing the common data fields of the NIEM Core, rather than building their own, when they initiate data exchanges.EXTERNAL BENEFITS:The broader use of the NIEM Core will enable serendipitous reuse of federal open data at a far greater scale. ""If the Federal Government regulatory agencies form a Task Force led by Commerce Department’s Trade Finance Advisory Council to implement the Standard Business Reporting (SBR) data standards and new technologies like distributed ledgers, they may be able to reduce compliance cost savings to the private sector like Australia and the Netherlands have.  Submitted by: Data Coalition.","U.S. regulatory agencies employ document-based forms to collect information from the private sector. However, the concept of Standard Business Reporting (SBR) demonstrates that regulatory agencies can reduce the compliance burden on the private sector by replacing forms with standardized data, governed by common data standards across multiple regimes. Australia and the Netherlands have implemented SBR, with their regulatory agencies agreeing on a common data structure of common fields and formats for the information they collect. As a result, Australian and Dutch business software can automatically compile and submit regulatory information to multiple regulators at once, saving compliance costs.In April 2018, the Commerce Department’s Trade Finance Advisory Council (TFAC) recommended to the Secretary of Commerce that the federal government should pilot and implement a standardized reporting regime similar to the SBR concept (see: https://www.datacoalition.org/wp- content/uploads/2018/07/TFAC_Data-Standardization_Final.pdf). The TFAC’s recommendations provide an opportunity to move forward.ACTION:The Administration should convene a task force representing all major regulatory agencies to create a road map for standardizing the data fields and formats that they use to collect information from the private sector, modeled on best practices learned from foreign success cases like the Australian Standard Business Reporting project, to reduce compliance costs. The task force should evaluate ways to deploy new data reporting technologies, particularly distributed ledger/blockchain, throughout U.S. regulatory reporting. The Administration should consider a role for the Department of Commerce in the housing and leadership of the task force as recommended by the TFAC.INTERNAL BENEFITS:Although the full implementation of a U.S. SBR program would require a multi-year effort, the creation of an exploratory task force would help scope out the required policy barriers and necessary investments, and encourage regulatory agencies to voluntarily begin regulatory data standard setting work where applicable.EXTERNAL BENEFITS:The adoption of SBR in the U.S. has the potential for enormous regulatory compliance cost savings to the private sector. For instance, Deloitte has estimated that Australia's SBR program will have accrued$5 billion AUD in compliance cost savings to the private sector by the end of 2018 (see: https://comms.dfsco.com/LP=1923).""If LEHD data is integrated with local and regional transportation data, there could be more robust and timely ""Access to Jobs"" metrics to inform federal decision-making and transit initiatives, such as autonomous vehicles, to empower vulnerable populations with frictionless mobility. Submitted by: ArgoLabs.","Integrate LEHD Data with Local/Regional Transportation data to develop more robust and timely ""Access to Jobs"" metrics. For example: a low-income residential area that is classified as a transit desert and also farther away from local/regional economic hubs can receive special status so that mobility and transit companies are incentivized  to fill the gaps by providing mobility to these areas.Not only will this use-case can inform federal decision making into supporting cutting edge transit initiatives today but also into a future where public service autonomous vehicles can empower vulnerable populations with frictionless mobility.""If the Department of Veterans Affairs includes ""provider readiness"" measurements in their new electronic health records, then Congress and third parties would be able to cross-reference this with the provider readiness training data reported to Congress to assess the returns on investments (ROIs).  Submitted by: Alpine Labs.","Veterans’ health is supported by patient data and provider readiness. As the Department of Veterans Affairs adopts a new electronic health record in support of acquiring and managing patient data, it is prudent to also improve the provider readiness through data analyses. Provider readiness requires efficient and effective training that allows healthcare providers to prepare for functional priorities in their roles, the operational locations, and the region’s legal requirements. As reported in a paper by the Data Coalition, the relevant data for such training is managed in digital event-based business cases, and reported to Congress. The data specifies the expected target effects, which justify the investments. The missing data are assessments of the effects of the training at the service locations, thus the returns on investments (ROIs). If these simple data are collected, the ROIs will provide ongoing, pinpointed clarity on necessary changes to optimize provider readiness in Veterans’ health outcomes. Paper: https://www.datacoalition.org/why-quality-data-should-inform-ombs-government-wide-management-reform-agenda/""If the Federal Government added more income, industry, and occupational granularity to LODES/LEHD/OntheMap data collection, then local governments may be able to use this information for more nuanced economic development and labor insights and planning. If the Federal Government added more granularity to PatentsView and augmented it with additional data sources, then local governments may be able to use this information to determine where they have competitive advantage with patent issuance and assignment.If the Federal Government added more industry granularity to  the GDP and GSP (beyond the  3 digit NAICS level), then local governments may be able to use this information for more nuanced economic development and labor insights and planning.If the Federal Government collected granular data on industry recognized credential (non-degree) and possession (supply) by type at the state and regional levels, then local governments may be able to use this information for more nuanced economic development and labor insights and planning.If the Federal Government developed methods and tools to examine regional talent pools and next highest skills and gaps in training delivery, then local governments could determine the shortest paths for workforce development.  Submitted by: Jobs Ohio.","Other possible priorities include the following:Priority Level 1•        Add more income, industry and occupational granularity to the LODES/LEHD/OntheMap programs•        Help states and regions determine where they have competitive advantage with patent issuance and assignment at a more granular level than discipline (PatentsView is a start)•        Produce GDP and GSP by industry at more granular than 3 digit NAICS level•        Track and report industry recognized credential (non-degree) possession (supply) by type at the state and regional levels•        Develop methods and tools to examine shortest paths to get regional talent pools to next highest skills and gaps in training delivery ""If the Federal Government used data to assess the highest densities of transit underserved populations in low unemployment regions, then local governments may be able to use this information for more nuanced economic development and labor insights and planning. If  the Federal Governments used data to identify pedestrian routes most likely to reduce congestion, then local governments may be able to use this information for more nuanced economic development and transit insights and planning.If the Federal Governments created guidance and used data to determine which direct flights will add most value to regions, then local governments may be able to use this information for more nuanced economic development and transit planning.If the Federal Governments created created a consistent method of measuring and reporting components of GDP at the federal, state, and metro levels using expenditure methods, then local governments may be able to use this information to track gross private investment. Submitted by: Jobs Ohio.","Priority Level 2•        Find highest densities of transit underserved populations in low unemployment regions•        Identify pedestrian routes most likely to reduce congestion•        Create guidance and data to determine which direct flights will add most value to regions•        Create a consistent method of measuring and reporting components of GDP at federal, state, metro levels using expenditure method (goal is to track gross private investment, and ideally separate the non-residential part)""If the Federal Government partnered with industry on metrics  to predict timing of company needs for expanding locations, then companies may be to leverage this information to expand more effectively.If the Federal Government mined social media data by region, then local governments may be to leverage this information to determine regional development priorities.  If the Federal Government collected data on mentors and advisors to schools, then third parties may be to leverage this information to predict propensity of top talent to serve as mentors and advisors to schools.  If the Federal Government expanded the sample size of the monthly CES data, then third parties may be to leverage this information for more granular economic development and labor insights and planning.   Submitted by: Jobs Ohio.","Priority Level 3•        Determine best metrics to predict timing of company needs for expanding locations•        Mine social media to determine regional development priority•        Predict propensity of top talent to serve as mentors and advisors to schools•        Find a way to increase the sample size for the monthly CES data""When the Illinois Department of Employment Security worked with the Illinois Department of Corrections in the Administrative Data Research Facility, a remote-access cloud-based platform, they were able to securely virtually connect confidential microdata to determine workforce connectivity of the formerly incarcerated with greater precision, a critical policy area of criminal justice.  Submitted by: State of Illinois.","Research staff at the Illinois Department of Employment Security have been working with their colleagues at the Illinois Department of Corrections in the Administrative Data Research Facility, a remote-access, cloud-based platform. The focus of their collaborative research in the ADRF has been the workforce connectivity of the formerly incarcerated, a critical policy area of criminal justice.  Their research has been comprehensive of the relative impact of human capital acquired while incarcerated, and destination of release (regional and interstate).""When the Illinois Department of Employment Security worked with the Illinois Department of Human Services in the Administrative Data Research Facility, a remote-access cloud-based platform, they were able to securely virtually connect confidential microdata to determine workforce connectivity of different welfare client groups, including SNAP/TANF and parents who receive childcare subsidies. Submitted by: State of Illinois.","Research staff at the Illinois Department of Employment Security have been working with their colleagues at the Illinois Department of Human Services in the Administrative Data Research Facility, a remote-access, cloud-based platform. Their collaborative research in the ADRF has centered on the workforce connectivity of different welfare client groups, including SNAP/TANF and parents who receive childcare subsidies.  The latter is an initiative under the auspices of the Governor’s Cabinet on Children and Youth.  The principal analytical focus is the contribution of workforce connectivity to self-sufficiency.  ""When the Illinois Department of Employment Security worked with the Illinois Department of Revenue in the Administrative Data Research Facility, a remote-access cloud-based platform, they were able to securely virtually link employer records between the Unemployment Insurance tax system (IDES) and the Income tax system (IDOR) to develop predictive machine learning models for employers who report to IDES and not IDOR and vice versa.   Submitted by: State of Illinois.","Research staff at the Illinois Department of Employment Security have been working with their colleagues at the Illinois Department of Revenue in the Administrative Data Research Facility, a remote-access, cloud-based platform. Their collaborative research in the ADRF links employer records between the Unemployment Insurance tax system (IDES) and the Income tax system (IDOR).  The principal analytical focus is to develop predictive machine learning models for employers who report to IDES and not IDOR and vice versa.  ""When the Illinois Department of Employment Security worked with public (Illinois Student Assistance Commission, Illinois State Board of Education, Illinois Community Colleges, and Illinois Board of Higher Education) and non-profit education in the Administrative Data Research Facility, a remote-access cloud-based platform, they were able to virtually connect confidential microdata to determine learning pathways to intergenerational mobility and workforce outcomes (workforce connectivity, career job flows, and career earnings).   Submitted by: State of Illinois.","Research staff at the Illinois Department of Employment Security have been working with their colleagues in public (Illinois Student Assistance Commission, Illinois State Board of Education, Illinois Community Colleges, and Illinois Board of Higher Education) and non-profit education to link learning pathways to intergenerational mobility and workforce outcomes (workforce connectivity, career job flows, and career earnings).  The learning pathways is comprehensive of post-secondary enrollment/completion and program-of-study.  This effort would benefit from a singular, remote-access, collaborative research environment that promotes inter-agency and interstate record linkage.""If the Federal Government made the Export Controls for Conventional Arms and Dual-Use Goods and Technologies list or conformance feedback accessible to third party researchers, then researchers  maybe be able to determine data science and AI methods to protect the parties involved in transfers of conventional arms and dual-purpose items under Wassenaar Arrangement (WA).  Submitted by: keywcorp.com.","The Wassenaar Arrangement (WA) focus is on the protection and security of the transfers of conventional arms and dual-purpose items. Although there exists a maintained list Export Controls for Conventional Arms and Dual-Use Goods and Technologies, there is no publicly available nor centralized information system that provides conformance feedback based on publicly and readily available documentation.The research will aim at identifying methods for the Federal Data Strategy and for agencies surrounding the protection of the Wassenaar members using multiple data science and AI techniques.""If the Federal Government developed a common interface for ingesting Supplemental Nutrition Assistance Program (SNAP) application and outcome data from third parties as well as sending data to USDA for additional evaluation of the SNAP Program, then third parties would be able to directly input SNAP application and outcome data to provide national reporting mechanisms to the federal government. Submitted by: mrelief.com.","Project Name: National SNAP Web Service Template InitiativeProblem Statement - We do not have a common interface for ingesting Supplemental Nutrition Assistance Program (SNAP) application and outcome data from third parties as well as sending data to USDA for additional evaluation of the SNAP Program. The publication of SNAP eligibility requirement data on data.gov has already catalyzed innovative technological solutions to enhance service delivery in the form of mRelief.com the easy-to-use platform that enables Americans to find out if they meet basic SNAP requirements on web, text messaging and voice. Enabling SNAP application and outcome web service access is consistent with the federal data strategy of Commercialization, Innovation, and Public Use as it spurs innovative technological solutions and fills gaps in government capacity and knowledge.Key Goals of a National Web Service:Provides the capability for an accessible end-to-end service that enables third parties to inform governments about effective outreach methods for identifying eligible SNAP applicants to enable capital efficiency.Providing high quality and timely information to inform evidence-based decision-making and learning in support of program integrity facilitating national database querying at the point of application submission before additional time is expended by state administrators and SNAP applicants. Facilitating external research on the effectiveness of SNAP Delivery by measuring the length of application processes in real time, and linking SNAP participation to communication platforms that facilitate accessible surveys to participants on response to policy changesReducing costs on the state in upgrading their systems Similar use cases:The IRS has an end point that takes submissions from Turbo Tax.Ag Gateway’s Initiatives standardizing fertilizer reporting processExisting Infrastructure:Acre Crop Reporting - https://usda.github.io/data-standards/index.htmlSome Standard Data Elements that overlap with SNAP application and outcome information already exist here: https://usda.github.io/data-standards/data-elements/index.htmlDeliverable: A web service that would enable third parties to feed SNAP application and outcome data and provide national reporting mechanisms to the federal government.""If the Department of Veterans Affairs maps veteran suicide at the census tract level and leverages geospatial predictive analytics, then they may be able to: identify areas to deploy additional mental health services, identify veteran suicide groupings and map their location by classification (i.e., urban, rural, and highly rural), and  determine the scope of the problem when considering mental health care availability in urban versus rural areas (e.g., it may be necessary to deploy more Community Care resources to certain rural and semi-rural areas). Submitted by: Anonymous.","Use Case: Veteran Suicide MappingPreventing suicide among America’s veterans is a high priority both at the U.S. Department of Veterans Affairs (VA) and within the current administration. At Leidos, our mission is to make the world safer, healthier, and more efficient through information technology, engineering, and science. We offer this use case for Veteran Suicide Mapping in support of this mission and in support of America’s veterans.Twenty veterans commit suicide every day according to a 2018 analysis [1]. In fact, peacetime veteran suicide, which used to be well below the rate of the general public, rose above the civilian rate for the first time in history in 2008 [2]. It is a high priority at the VA to provide care to all veterans, regardless of their geographic location. In particular, a new presidential executive order directs the VA, Department of Defense, and the Department of Homeland Security “to collaborate to provide, to the extent consistent with law, seamless access to mental health care and suicide prevention resources for Veterans” [3]. According to a 2016 research report on veteran suicide, there is a need for “comparison of suicide rates for residents of urban and rural areas” [4]. We propose mapping veteran suicide at the census tract level. This enables the VA to 1) identify areas to deploy additional mental health services, 2) identify veteran suicide groupings and map their location by classification (i.e., urban, rural, and highly rural), and 3) determine the scope of the problem when considering mental health care availability in urban versus rural areas (e.g., it may be necessary to deploy more Community Care resources to certain rural and semi-rural areas).  For this geospatial data analytics use case, we will collect veteran suicide data from the VA Open Data Portal [5] and census tract data for urban, rural, and highly rural areas from the U.S. Census Bureau [6]. After aggregating and refining the data, Leidos will leverage our Global Monitoring and Planning System (GLIMPS), which is currently being used by America’s intelligence and special operations communities to predict global hotpots at a 96% accuracy rate, five years into the future. About Leidos - Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world’s toughest challenges in the defense, intelligence, homeland security, civil, and health markets. Recognized as a Top 10 Health IT provider, Leidos draws on decades of success to deliver a range of solutions and services designed to meet the healthcare challenges of today. A company of scientists, engineers, and technologists, we deliver a broad range of customizable, scalable solutions to hospitals and health systems, biomedical organizations, and every U.S. federal agency focused on health, including the Department of Defense (DoD), Veterans Affairs (VA), and Health and Human Services (HHS). References:[1] National Academies of Sciences, Engineering, and Medicine. Evaluation of the Department of Veterans Affairs Mental Health Services. National Academies Press, 2018.[2] Bachynski, Kathleen E., et al. ""Mental health risk factors for suicides in the US Army, 2007–8."" Injury Prevention 18.6 (2012): 405-412.[3] U.S. Department of Veterans Affairs. Office of Public and Intergovernmental Affairs - https://www.va.gov/opa/pressrel/pressrelease.cfm?id=4064[4] Department of Veterans Affairs. ""Suicide among Veterans and other Americans 2001-2014."" Washington, DC, Office of Suicide Prevention (2016). https://www.ment""If the Federal Government provided a process that would allow researchers to send local data for linkage with federal wage records and then analyze the results through a Federal Statistical Research Data Center (RDC) or equivalent, then researchers may be able to fill in gaps in the current state and local Integrated Data Systems (IDS) related to welfare and workforce development programs, to be able to understand the earnings trajectories of residents working outside of their state, or those who work for the federal government or military. Submitted by: University of Penn.","Facilitating Linkages Between State and Local IDS and Key Federal Data Sets:State and local governments are increasingly utilizing Integrated Data Systems (IDS) that link administrative data across domains to evaluate federal policies and programs. Often, such efforts are unable to account for individuals who cross state lines or, for other reasons, are not captured in local data sets, and therefore would benefit from a streamlined process for linking existing IDS data with key federal data sets. For example, in order to evaluate federal welfare and workforce development programs, states may seek to understand the earnings trajectories of residents working outside of their state, or those who work for the federal government or military, who will not be captured in state administrative data sources. A process that would allow researchers to send local data for linkage with federal wage records and then analyze the results through a Federal Statistical Research Data Center (RDC) or equivalent would open up the possibilities for many more important uses of state administrative data. ""If many states form a Multi-state Temporary Assistance for Needy Families (TANF) Research Collaborative in partnership with the Family Self-Sufficiency Data Center to share and securely link data, then there would be comparable analyses on whether: working recipients earn sufficient income to close cases, households that earn their way off benefits main self-sufficiency or do the cases reopen, and workers in households closed due to earnings maintain a stable income after case closings to inform and improve program administration. Submitted by: chapinhall.org.","Multistate TANF Research Collaborations  This project aims to demonstrate a use case for family self-sufficiency data to inform and improve program administration by working with cohort of six state TANF agencies to conduct comparable analyses. The analysis will describe the interaction between work participation and receipt of benefits by addressing questions such as: What working recipients earn sufficient income to close cases? Do households that earn their way off benefits main self-sufficiency, or do the cases reopen? Do the workers in households closed due to earnings maintain a stable income after case closings? Participating agencies receive extensive technical assistance, including assistance with data preparation, data sharing, and/or data analysis, and the opportunity to collaborate and develop a common understanding of best practices together with other partner agencies. We will clearly and transparently document how these analyses are performed (including providing code and detailed definitions) so that other agencies can duplicate these metrics on their own data. This project, an initiative of the Family Self-Sufficiency Data Center (https://harris.uchicago.edu/research-impact/centers/family-self-sufficiency-data-center) is sponsored by the Office of Planning, Research, and Evaluation (OPRE), an Office of the Administration for Children and Families (ACF), U.S. Department of Health and Human Services (HHS).""If the Federal Government works with multiple local jurisdictions to create a Dual System Youth Design Study to analyze administrative records on youth who are involved in both the juvenile justice and child welfare systems to create national estimates, then local governments and third parties could be informed to create strategies to improve coordination across systems. Submitted by: chapinhall.org.","Dual System Youth Design StudyThis project aims to develop a sound research design for generating national estimates of dual system youth who are involved in both the juvenile justice and child welfare systems and to identify strategies to improve coordination across systems. The project meets these aims by analyzing administrative records from multiple jurisdictions to identify pathways that lead to multiple system involvement and to better understand characteristics of the dual system populations in those jurisdictions; and conducting case studies to identify examples of successes and challenges associated with cross-system collaboration and data integration in local jurisdictions. Analyses were completed using linked administrative data from three sites Cuyahoga County, Ohio, New York City, and Cook County, Illinois. A significant focus of this study was to assess the feasibility of using administrative data linked across systems to produce the incidence of dual system youth (https://www.ojjdp.gov/research/design-study-of-dual-system-youth.html). This project is sponsored by the Office of Juvenile Justice and Delinquency Prevention (OJJDP). ""If researchers work with governments to conduct a Pathways from K-12 to Work Study connecting longitudinal education data and to employment and earnings data, data on public benefit receipt, and mortality, then schools and the postsecondary institutions would be able to strategically adjust early life education programming to mitigate low earnings, unemployment, not attending post-secondary education, and incarceration. Submitted by: chapinhall.org.","Pathways from K-12 to Work. This project characterizes students’ pathways from K–12 schooling to work, with particular attention to those students whose trajectories are negatively affected by failure to graduate from high school or fail to engage in work or school after graduation. The study uses a longitudinal cohort of students from a large, urban school district with linked demographic information, school achievement test scores, school attendance, high school graduation, college attendance and graduation, and juvenile and criminal justice involvement. They intend to match this dataset through the Census Bureau infrastructure to employment and earnings data, data on public benefit receipt, and mortality. The resulting combined data can answer the question “what early life educational and out-of-school experiences predict low earnings, unemployment, not attending post-secondary education, and incarceration?” Results of this study can provide actionable results for both schools and the postsecondary institutions, be it employment or education.""If the Federal Government and states work with researchers to conduct an Effects of Public Need-Based Aid for College Study by augmenting longitudinal state college enrollment and eligibility for college financial aid databases with American Community Survey income and earnings data, then local governments will be able to evaluate the impacts of the state programing that lowers the price of college for low-income residents. Submitted by: chapinhall.org.","Effects of Public Need-Based Aid for College. This project augments an existing large, longitudinal state database by adding income and earnings measures to measures of college enrollment and eligibility for college financial aid. The core of the project is an evaluation of the impacts of a state program that lowers the price of college for low-income residents. The original database consists of all state residents applying to receive state and federal financial aid at state colleges and universities, during the school years 2007–08 through 2014– 15. They observe college outcomes for applicants and request matching to employment and earnings data. An additional match to the American Community Survey will provide a fuller picture of education and employment for a subset of applicants. The study is poised to answer questions about the longer-outcomes of the public financial aid program and inform future state investments.""If the Federal Government and states work with researchers to conduct an Does Eviction Cause Poverty? Study by combining local court records data and school district records with Census Bureau held data (to study earnings, employment, public benefit receipt, and homelessness for evicted adults; and chronic absenteeism, misconduct, and performance on standardized tests among children of evicted households, respectively), then local governments will be able to create evidence-based policy on housing assistance, by highlighting the costs of eviction, and by providing solid evidence on the long-run impact of eviction on families. Submitted by: chapinhall.org.","Does Eviction Cause Poverty? This project evaluates the causal impact of eviction on employment and schooling outcomes in a large urban county using a database containing the universe of eviction case court records from 2000–2015. The research design leverages the random assignment of eviction court cases to judges, which creates a natural experiment for studying the causal effect of eviction on a wide range of short- and long-run household outcomes associated with poverty. They propose combining court records data with Census Bureau held data to study earnings, employment, public benefit receipt, and homelessness for evicted adults. Using school district data, they also propose to study how eviction affects chronic absenteeism, misconduct, and performance on standardized tests among children of evicted households. Results will inform the policy debate on housing assistance, by highlighting the costs of eviction, and by providing solid evidence on the long-run impact of eviction on families. This study will contribute towards the debate on tenant-based versus place-based housing assistance.""If the Federal Government and local governments work with researchers to conduct a Service Utilization of Families Experiencing Homelessness Study by securely combining confidential local social services, school records, and law enforcement data to Census Bureau and federal housing and change of address data to determine longitudinal housing patterns and risk and resilience factors (for subsequent homelessness, child welfare involvement, criminal justice involvement, and child academic and behavioral outcomes), then local governments will be able to create evidence-based policy on homelessness assistance, by highlighting the costs of homelessness, and by providing solid evidence on the long-run impact of homelessness on families. Submitted by: chapinhall.org.","Service Utilization of Families Experiencing Homelessness. The goal of this study is to provide evidence to the experiences of families experiencing homelessness to inform policy efforts to better serve this population. The project uses a linked database of social services, school records, and law enforcement records for one large county. They intend to use the Census Bureau infrastructure to develop a comparison group using the American Community Survey, and link to federal program data including housing data and change of address data. This data linkage will allow the researchers to longitudinally examine housing patterns, compare these patterns among families who were housed and those who were homeless, and to identify risk and resilience factors for subsequent homelessness, child welfare involvement, criminal justice involvement, and child academic and behavioral outcomes.""If the Federal Government and local governments work with researchers to conduct a Health at Birth and Later Life Outcomes Study by securely combining a large sample size of birth records over several decades with Census-held administrative data to examine income, educational attainment, health, and use of government services later in life, then the Federal government and local governments would be informed on how many large social programs affect lifelong improvements in health and productivity. Submitted by: chapinhall.org.","• Health at Birth and Later Life Outcomes. This study aims to document the relationship between health at birth and later life outcomes, while evaluating the returns to policy-driven early health investments. First, they will use within-family variation in birth weight to estimate the effect of health at birth on adult health and productivity. Second, they will analyze two specific public health investments that target the prenatal and neonatal periods for low-income pregnant women in one state. They propose combining birth records for over 25 million individuals born in one large state over six decades with Census-held administrative data to examine income, educational attainment, health, and use of government services later in life. This project analyzes the role of policy interventions in promoting lifelong improvements in health and productivity. Given the many large social programs that exist in the U.S. to promote the health of pregnant women and infants, additional information on the relationship between public policy, health at birth, and later life outcomes will add substantial value to ongoing policy debates.""If the Federal Government and local governments work with researchers to conduct a Program Utilization by Formerly Criminalized Youth Study by securely combining state agency data on juvenile justice, criminal justice, and recidivism with Census Bureau-held data (with respect to young persons who have been involved in criminal or juvenile justice in one state) to identify government programs whose utilization is associated with nonrecidivism, higher incomes, better health, more stable housing, or other socially desirable outcomes, then the Federal government and local governments would be informed on how to serve young offenders and may be able to identify strategies for reducing recidivism. Submitted by: chapinhall.org.","• Program Utilization by Formerly Criminalized Youth. This project proposes to combine state agency data on juvenile justice, criminal justice, and recidivism with Census Bureau-held data with respect to young persons who have been involved in criminal or juvenile justice in one state. The study brings juvenile recidivism data integrated from three state agencies for nearly 83,000 individual juveniles who were involved in juvenile or criminal justice between 2000 and 2014 with information about arrests, charges, disposition, sentencing, incarceration, and rearrests. This project seeks matching with federally held data resources on income and employment, housing assistance, health insurance and health care, and location. By combining state juvenile recidivism data with data resources held at the Census Bureau, the study aims to identify government programs whose utilization is associated with nonrecidivism, higher incomes, better health, more stable housing, or other socially desirable outcomes. Results of this study can contribute to policy discourse about how to serve young offenders and identifying strategies for reducing recidivism.""If the Employment and Training Administration (ETA) and the Bureau of Labor Statistics (BLS) build on their previous efforts to combine traditional survey-based methods with natural language processing methods using large pools of public-private jobs data to increase the availability, granularity, and use of structured, machine-readable skill and competency information, then they can accelerate the pace of improvement for federal occupational ontologies like SOC and O*NET.If the Federal Government builds on their efforts like College Scorecard, as well as public-private collaborations like LEHD's partnership with the University of Texas system, to scale up the voluntary provision of programmatic and credential information from states and individual education and training providers to connect individual participant records with long-term labor market outcomes, then Federal Government data collection costs of credential and competency information may be able to be dramatically reduced, while broadening education and training programs capabilities. If the Federal Government builds on their partnership between IRS and Census in allowing new uses of tax records to create public-use datasets through CARRA, then US Chamber of Commerce employers may be able to measure their ability as employers to act as engines of economic mobility by creating a public-private data collaborative with Census and providing hiring data for cohorts of new hires to connect with long-term labor market outcomes and earnings profiles and provide aggregates statistics on wage trajectories by SOC within employers or employer groups. Submitted by: T3 Innovation Network .","On behalf of the T3 Innovation Network (https://www.uschamberfoundation.org/t3-innovation) a public-private talent pipeline data innovation initiative organized by the US Chamber of Commerce Foundation and Lumina Foundation, we'd like to submit three use cases along with accompanying best practices in Federal Agencies that can be build on to enable each of these use cases. A common thread through each of these is the creation of public-private data collaborative that allow for the creation and enhancement of new, consumer-facing, public-use datasets.1. Catalyzing Structured Competencies Bridging Education to Work. Federal agencies can play an essential role in public-private data collaborative for increasing the availability, granularity, and use of structured, machine-readable skill and competency information in education and employment contexts, and accelerate the pace of improvement for federal occupational ontologies like SOC and O*NET. Building on past collaborations by ETA and BLS combining traditional survey-based methods with natural language processing methods using large pools of public-private jobs data.  These efforts can be accelerated through the US Chamber of Commerce's recently launched Jobs Data Exchange effort, and has been discussed with Federal partners in a convening July 27th, 2018. Stakeholders in this work may include historical collaborators such as the National Labor Exchange, University of Chicago's Center for Data Science and Public Policy, LinkedIn, US Chamber of Commerce's Job Data Exchange Initiative, Google, as well as other new collaborators who have shown interest.- Increasing the Availability of Consumer Information on Return on Investment for Programs and Credentials. Building on inter-agency efforts like the College Scorecard, as well as public-private collaborations like LEHD's partnership with the University of Texas system, there is a significant opportunity to scale up the voluntary provision of programmatic and credential information from states and individual education and training providers to connect individual participant records with long-term labor market outcomes and make metrics around return on investment in education more broadly available. Several foundations and funders are actively working in creating data standards for program, credential, and competency information and make it easier for education and training institutions to be able to share this data as part of data collaboratives and those efforts could dramatically reduce the costs to federal agencies for receiving and connecting individual and programmatic data, while broadening the set of programs and providers connecting their data. Stakeholders could include NASWA, IMS Global, AACRAO, Dxterra, the Lumina Foundation, Gates Foundation, and others.- Enabling the Availability of Consumer Information on Employers as Engines of Economic Mobility. Building off of the partnership between IRS and Census in allowing new uses of tax records to create public-use datasets through CARRA, several members of the US Chamber of Commerce have expressed a strong interest in measuring their ability as employers to act as engines of economic mobility by voluntarily creation a public-private data collaborative with Census and providing hiring data for cohorts of new hires to connect with long-term labor market outcomes and earnings profiles and provide aggregates statistics on wage trajectories by SOC within employers or employer groups. This would be similar in spirit to College Scorecard in providing new consumer-facing information on the return on investment for early apprenticeship and employment at different large employers. Several funders have expressed interest in supporting this work. ""When the Federal Government created the he National Information Exchange Model (NIEM), a common vocabulary that enables efficient information exchange across diverse public and private organization, then they were able to connect  communities of people who share a common need to exchange information in order to advance their mission. Submitted by: Joint Staff J6, DDC5I, Data and Services Division.","The National Information Exchange Model (NIEM) is a common vocabulary that enables efficient information exchange across diverse public and private organizations. NIEM connects communities of people who share a common need to exchange information in order to advance their mission. NIEM provides rules and methodologies around the use of the model as well as a standardized Information Exchange Development Lifecycle that can be reused by everyone. NIEM also includes governance, training, tools, technical assistance, and an engaged community to support users and organizations in adopting NIEM. NIEM is community-driven. As new communities join NIEM, they bring with them innovative ideas and fresh perspectives which contributes to the advancement of the program. Organizations can reuse NIEMs common vocabulary, the engineering behind it, and standardized exchange development process to meet their specific requirements. This reduces the number of development efforts, eases long-term maintenance, conserves resources, and promotes consistency ultimately leading to enhanced mission capabilities.More information: https://www.niem.gov/""When AIR developed and piloted an easy-to-use, evidence-based promising practices database and dashboard of cross systems opioid-related interventions (which includes emerging policies and practices from CrimeSolutions.gov, SAMHSA’s National Registry of Evidence-Based Programs and Practices, and other sources), then communities and states were able to conduct free text searching of interventions in prevention, treatment, harm reduction, and recovery; and across a variety of geographic areas, populations (e.g., children), sectors, and implementers. Submitted by: American Institutes for Research.","Opioid Promising Practices DashboardThe opioid crisis is a fast-moving epidemic, and new interventions emerge and are evaluated on a frequent basis.Based on prior experience in the field, AIR ascertained a pressing need for communities at the frontlines to access cross-system, evidence-based practices and policies in a user-friendly format. We have developed and are piloting an easy-to-use, evidence-based promising practices database and dashboard of cross systems opioid-related interventions (Figure 1). This dashboard allows for free text searching of interventions in prevention, treatment, harm reduction, and recovery; and across a variety of geographic areas, populations (e.g., children), sectors, and implementers. The dashboard covers evaluations and meta-analyses on CrimeSolutions.gov, SAMHSA’s National Registry of Evidence-Based Programs and Practices, and emerging policies and practices from other sources. The dashboard provides a mechanism by which communities and states may look to national programs and policies to assess what could work in their communities. The tool is expected to be beta-tested by experts and community users in August 2018.""If Federal statistical agencies implement secure cloud-based analytical platforms that incorporate the latest big data technologies (e.g., Hadoop and Spark) and cloud computing technologies (Kubernetes native cloud computing architecture), then they will be able to centralize data lifecycle management, analyze data at an unprecedented scale, improve data security from all angles (including granular access controls), and manage centrally the tools and code to provide a personalized computing environment. Submitted by: American Institutes for Research.","Secure Cloud-Based Analytical Platform Our analytics platform incorporates the latest big data technologies (e.g., Hadoop and Spark) and cloud computing technologies (Kubernetes native cloud computing architecture) to provide researchers and data scientists with a powerful integrated analytical platform (Figure 2). With this type of solution, federal statistical agencies may better control access to restricted data by co-locating processing and analytical functions with the data. This tiered architecture allows AIR and our clients to:• Centralize data lifecycle management• Analyze data at an unprecedented scale• Improve data security from all angles, including granular access controls• Manage centrally the tools and code to provide a personalized computing environment""If states and institutions update their programs to careers resources (CIPSOC crosswalk tools) with Integrated Postsecondary Education Data System (IPEDS), College Scorecard, student aid, Bureau of Labor Statistics (BLS) and Census American Community Survey (ACS) data, on postsecondary graduates (plus workforce training recipients, as available), then like AIR's LaunchMyCareer tool, they would be able to better analyze students’ choices, postsecondary institution support and resource provision, employer strategies, cross-sector collaboration, and state resource allocations. Submitted by: American Institutes for Research.","Mapping Postsecondary-to-Occupation ExperiencesClosing the gap in information linking credentials to jobs is needed now more than ever as potential students continue to demand a clearer return on investment from postsecondary institutions. In AIR’s work developing LaunchMyCareer (Figure 5), we have noted that the resource for states and institutions to map their programs to careers – so called CIP-SOC crosswalk – is 10 years old and is based on principles, not data. Exploring microdata on postsecondary graduates (plus workforce training recipients, as available) and their transition to the workforce can produce a data driven, experienced-based resource that describes the observed transitions from postsecondary education/workforce training programs to the workforce. Such a resource would inform students’ choices, postsecondary institution support and resource provision, employer strategies, cross-sector collaboration, and state resource allocations. The tool uses federal education data from the Integrated Postsecondary Education Data System (IPEDS), College Scorecard, the current CIPSOC crosswalk, and student aid sources. Similarly, it uses a variety of Bureau of Labor Statistics (BLS) and Census American Community Survey (ACS) data.""When researchers leading the Moving to Opportunity (MTO) experiment used access to tax records to comprehensively examine the long-term impacts of housing voucher programs and moves to lower-poverty neighborhoods, then they were able to conduct the critical long-term follow-up analysis to assure equivalent data coverage for both treatment and control groups without incurring the formidable and expensive task of long-term surveys. Submitted by: J-PAL North America.","Understanding Effects of NeighborhoodsResearchers leading the Moving to Opportunity (MTO) experiment used administrative data to comprehensively examine the long-term impacts of housing voucher programs and moves to lower-poverty neighborhoods.  Through MTO, families were selected by lottery to receive housing vouchers, some of which required families to move to neighborhoods with low poverty rates. Initial and mid-term studies, which leveraged unemployment insurance data, arrest records, college enrollment data, benefits data, and census data found that adults who moved had no change in employment, income, or use of government benefits (although surveys showed they were happier and healthier). However, a long-term follow-up study, made possible by access to tax records, revealed that for young children (less than 13-years-old at the time of the move), moving to a low-poverty neighborhood increased expected lifetime earnings by about $300,000. This study also found that children who moved were more likely to attend college, attended better colleges, and lived in better neighborhoods as adults, and women were less likely to become single mothers. Administrative data allowed for long-term follow-up and was critical in assuring equivalent data coverage for both treatment and control groups, a formidable and expensive task with long-term surveys. For instance, recipients of vouchers may be more likely to have moved and thus harder to locate, or they may be more or less likely to be in jail, employed, etc. These concerns of differential coverage across treatment and control group can be mitigated using administrative data.""When South Carolina replaced their quasi-random, “round robin” procedure, to a explicitly random procedure for Medicaid beneficiaries, then researchers were able to analyze the effect of different managed care plans by matching records of beneficiaries’ plan assignments to various sources of administrative data, including data from Medicaid claims, the Department of Mental Health, the Department of Alcohol and Drug Abuse Services, and vital records for “cream skimming” effects—i.e., whether differences in health outcomes across MCOs is the result of different features of the MCOs. Submitted by: J-PAL North America.","Implementing True Randomization in Medicaid Managed Care Organization AssignmentIn South Carolina, policymakers embedded true randomization into the Managed Care Organization (MCO) assignment of Medicaid beneficiaries to assess the efficiency of the program. Prior to 2017, the state automatically assigned Medicaid beneficiaries to different managed care plans using a quasi-random, “round robin” procedure. South Carolina worked with its Medicaid enrollment broker so that beginning in 2017, the state started to automatically assign beneficiaries using an explicitly random procedure. Researchers estimate that roughly 60,000 beneficiaries will be randomly assigned each year.Random assignment will allow them to distinguish the MCO’s impact from any possible “cream skimming” effects—i.e., whether differences in health outcomes across MCOs is the result of different features of the MCOs, or the result of MCOs enrolling individuals who were more or less healthy to begin with. Researchers will analyze the effect of being randomly assigned to different managed care plans by matching records of beneficiaries’ plan assignments to various sources of administrative data, including data from Medicaid claims, the Department of Mental Health, the Department of Alcohol and Drug Abuse Services, and vital records.""When Fujirebio transitioned from on-premises data centers to virtualization and the cloud, then they were able to achieve speedy recovery and eliminate tape complexity and costs by instantly archiving all our mission critical production systems to the public cloud. Submitted by: Rubrik.","USE CASE - FUJIREBIO DIAGNOSTICSAs the global leader in producing in vitro diagnostics and biomarkers, Fujirebio was feeling increased pressure to manage backup and archiving in its on-premises data centers. The company didn’t want to continue to pay costly upgrades with its legacy vendor, and it needed to reduce management complexity and test backups more quickly. The company realized it was unable to upgrade its system to the next version of VMware vSphere because of compatibility issues with its existing vendor. Also, it was running out of space and didn’t want to incur the cost of a forklift upgrade.“It was essential that we have a scalable solution that allows for growth year after year,” said Tim Gable, network systems manager. “Rubrik integrates seamlessly with our shift to virtualization and cloud, following the same scale-out philosophy of cloud computing with pay-as-you-go economics.“The beauty of Rubrik is that it is built for the user instead of the technology, with just a box bolted on top,” he continued. “Two of the main results we have seen with Rubrik is that the speed of recovery is phenomenal, and we can eliminate tape complexity and costs by instantly archiving all our mission critical production systems to the public cloud. This gives us more resiliency in case of a failure.” ""When the Departments of Justice and Homeland Security developed the National Information Exchange Model (NIEM), they were able to create a common framework and vocabulary for information exchanges among differing missions and business domains. Submitted by: The MITRE Corporation.","Investigate and leverage governance and stewardship practices from similar domains.Example: National Information Exchange Model (NIEM), 29 which was initially developed by the Departments of Justice and Homeland Security• Best Practice/Common Solution• NIEM is a use case example of data governance, stewardship, and information exchanges for all levels of government (federal, state, local, and tribal) Consider leveraging and expanding NIEM as a framework and common vocabulary for data sharing and information exchanges beyond its current mission/business domains.[29 https://www.niem.gov/]""When the Pew Charitable Trusts published its 2018 report on ""How States Use Data to Inform Decisions,"" they revealed ideas and use cases that could be applicable to the federal domain. Submitted by: The MITRE Corporation.","Leverage insights gained from prior data investigations. Example: the Pew Charitable Trusts’ 2018 report, “How States Use Data to Inform Decisions.” 34 • Missed Opportunity • The Pew report includes ideas and use cases that the federal government may be able to leverage and adopt. Explore the Pew report for ideas and use cases that could also be applicable in the federal domain.[34 http://www.pewtrusts.org/en/research-and-analysis/reports/2018/02/how-states-use-data-to-inform-decisions]""If the government considered itself to be an information broker, allowing citizens and organizations to update their information, it could improve data quality. Submitted by: The MITRE Corporation.","Many municipalities are also exploring open data activities. For example, the city of Syracuse, N.Y., is using and sharing municipal data via a portal that provides a central location for open data, maps, and visualization tools to help residents understand what is happening in their city.35• Common Solution• The government should consider allowing citizen and organizations to own and update information about themselves, as the credit bureaus do. Such engagement of the public will significantly raise data management awareness and ultimately improve data quality. If the government considered itself to be such an information broker, it could reconcile mistakes in the data across multiple systems and agencies.Explore and adopt ideas from municipalities’ open data initiatives.[35 http://data.syrgov.net/pages/tntdata]""If the government investigates public-private opportunities, it might uncover data that could assist in development of healthcare enhancements. Submitted by: The MITRE Corporation.","While using data to support healthcare enhancements is common, opportunities exist to investigate potential new efforts for communities that tend to get less attention, such as those with disabilities36 or need the assistance of caregivers.37• Missed Opportunity• Navigating requirements and obtaining information from numerous sources is often an onerous process for those requiring assistance. Can agencies work together to develop interfaces to data that would help?Initiate public-private efforts to uncover data stores that could assist the public on these complex issues, and develop easy to use interfaces to access them.[36 For example: http://www.who.int/news-room/fact-sheets/detail/disability-and-health37 For example: http://www.who.int/news-room/fact-sheets/detail/assistive-technology]""If the government studies the Department of Commerce's Low Earth Orbit Situational Awareness and Space Traffic Control system, it could realize data management and commercialization lessons applicable elsewhere. Submitted by: The MITRE Corporation.","Support Commerce’s Low Earth Orbit (LEO) Situational Awareness and Space Traffic Control systems.• Game Changer• Data management and commercialization will be key to this effort’s success.Investigate to determine if it could become a useful case study, where lessons learned through supporting this initiative could be applied elsewhere.""If the Federal Data Strategy team worked closer with the Improper Payments CAP Goal, both teams would be mutually benefited. Submitted by: The MITRE Corporation.",Improper Payments is a massive ($141 billion annually) and data heavy federal issue. It’s also the focus of CAP Goal #9. • Game Changer • Data-centric efforts on this problem space will reap benefits for two CAP goals. Establish connections with managers of CAP Goal #9 to develop a mutually beneficial project."Adopting and executing the Integrated Economic Indicator State Longitudinal Data System (IEI-SLDS) described in this comment would leverage economic data to help policymakers put Americans to work. Submitted by: National Minority Technology CouncilLooking Forward Research & Development.","Algorithms that link calculated datasets as predictive exchanges will soon prove the value of smart contracts, governed workflow and merit-based entitlement payout systems focused on outcomes rather than outputs. In order to frame the possibility of a new roadmap to wealth and prosperity for every American we need a baseline that can keep up with iterative change and industry input.The Integrated Economic Indictor State Longitudinal Data System (IEI-SLDS) Use Case frames a new methodology that leverages data to support the mission of preparing Americans for success in a globally competitive world by creating digital outcomes that will be able to predict program success and investment RIO.As a 501(c)6 non-stock trade association we recognize that our members and stakeholders exist in an ecosystem where human capital growth is a function of love, learning and livelihood. Our petition that the U.S. Government consider industry as a valid input towards change and innovation is matched with deep respect with regards to the depth of knowledge born and incubated in our federal research complex.We further ask consideration to the ISI-SLDS Use Case submitted as our Public Comment (attached)Outcome Harmonization – Re-Imagining the American Safety NetOver the past eight years the National Minority Technology Council (NMTC) has investigated the merit of integrating the State Longitudinal Data System (SLDS)1 data sets found in each State with other key economic data in an effort to both create a data scheme to connect State and Local funding to Federal expectations, while focusing on the need for the processing power of federal High-Performance Computing.This Integrated Economic Indicator Use Case utilizing SLDS (ISI-SLDS) affords the contemplation of a national data harmonizing effort to bring together thedisconnected “Social Safety Net” here in America into a transformational predictive system that allows for better policy decision making and innovation, in areas that help put American’s to work.Education and training today are a converging landscape. Over the past three years NMTC has worked with the Virginia Career Education Foundation (VCEF www.vcefworks.org) 2 to develop interoperability methodologies that work towards connecting purpose to mission. Given the recent reform plan and reorganization recommendations provided in the document, “Delivering Government Solutions in the 21st Century, Reform Plan and Reorganization Recommendations”3 for the Department of Education and the workforce which articulates the recommendation to merge the Dept. of Education (ED) and Labor (DOL) into a single Cabinet agency, the Department of Education and the Workforce (DEW), the IEI-SLDS Use Case seems timely. The IEI-SLDS Use Case frames a new methodology that leverages data to support the mission of preparing Americans for success in a globally competitive world by creating digital outcomes that will be able to predict program success and investment ROI. Below (Figure 2) is a schema to harmonize the fragmented workforce development programs and processes into a comprehensive workforce data management ecosystem to harmonize and leverage data and data patterns within and across P-20W (early learning through the workforce).Our industry trade association took on the task to communicate and collaborate with P-20W institutions and sectors within the human capital sector to develop public value mapping to conceptualize an approach that could be adopted and improved with federal government stewardship. Above is a framework that was jointly developed through collaboration between the industry subject matter experts associated with NMTC and the State and Local Education and Labor partners located in Virginia associated with VCEF. This high-level framework was envisioned to help education and workforce stakeholders conceptualize that data harmonization will allow for new “dashboards” that will generate new contexts for decision making not currently available.Given the emergency need for a skilled workforce and the disconnect between learning outcomes and industry requirements and the desire for American citizens to enjoy the benefits of work more resources need to be targeted towards training and development. In order to ensure targeted grant funding makes a difference the IEI-SLDS Use Case has a unique opportunity to align top level stakeholders into a room to discuss change. This is by no means a start-up effort as both DOL and ED have, albeit in silos, worked towards solving the workforce challenge utilizing a data-driven approach. DEW allows for a more robust mission that would more directly serve a growth mission. The Common Education Data Standards Initiative (CEDS Initiative)4 “includes a common vocabulary, data models that reflect that vocabulary, tools to help education stakeholders understand and use education data, an assembly of metadata from other education data initiatives, and a community of education stakeholders who discuss the uses of CEDS and the development of the standard”. Stakeholder engagement is critical on many levels; form follows function. This industry recommendation is asking the U.S. Department of Commerce, Under Secretary for Economic Affairs to establish a federal working group to include DOL, DE, DOC, HHS, USDA, HUD, DHS and OPM to improve their ability to understand how programs aimed to reduce poverty can be in part transformed into programs that lift citizens from poverty to high skilled jobs and entrepreneurial activities like cyber security.Data is indeed a strategic asset and it must be leveraged to grow our economy. NMTC further recommends adding state, local, industry and NGO partners to the working group at the beginning. This Administration’s appetite for Public/Private partnership is encouraging and we believe this ISI-SLDS Use Case will build confidence from the citizenry that their vices are being heard that government needs to reform in a way that promotes growth and reduces the silos in the federal government that can be attributed to the systemic moral hazard found in government program implementation and lack of outcome metrics.The ISI-SLDS Use Case will also promote grater commercialization and innovation in the areas of workforce readiness. NMTC believes the investment community feels locked out of the education community. Models like “Pay for Success” require both qualitative and quantitative outcome measurements. Currently the “graduate and done” system of check and balance does not monitor the longitudinal nature of learning in the 21st Century Knowledge Economy. Initiating ISI-SLDS will spawn additional research activities in Artificial Intelligence, Performance Modeling, and Outcome Research geared to measure the nexus between education relevance and economic growth.At the core the U.S. is a global source of hope and wonder. We as a nation have solved problems in a manner that has changed the standard of living for those who enjoy the right to live in this great country. Now more than ever we need not fear the size of the climb born from the idea that we can frame a new way of measuring learning success through the measurement of economic trends gathered at the individual level. In fact now is the time to utilize the full capacity and intellect of our nation to support the idea that every American has the right to learn and be prosperous. As our economy continues to boom our need for improved human capacity will rise. It is our hope as an industry codified to support the minority technology industry that our federal government will take this baton we have carried and support our idea that to build our Republic we must have all hands-on deck.Thank you for the opportunity to bring voice to innovation. The National Minority Technology Council (NMTC) is actively serving our nation by engaging in strategic design based research while acting as a steward to the over 65,000 U.S. owned minority technology companies with combined annual sales of over $100 Billion. This fast growing U.S. based industry sector employs over 500,000 people both here in America and around the world. NMTC is organized as a 501c6 Virginia non-stock industry trade association. NMTC recognizes that building consensus around outcomes requires an integrated approach. Given our industry perspective that our workforce system is not keeping pace with demand and in our more disenfranchised communities this is a national crisis that is in proportion to an issue to our national security as it pertains to our economy. As we move further into the 21st Century we must embrace our national capacity for self-determination, prosperity, and global resilience. As you consider the validity of our need as a nation to better align our data assets with outcomes born from public dollars we as industry will continue to innovate and bring the natural resource of human intellect. Our team of Sr. Fellows, the existence of the many minority technology companies and the will of government, education and the non-profit and philanthropic communities will bring governance to status quo and results to our efforts to create wealth to our nation and our citizenry.REFERENCES1 Statewide Longitudinal Data Systems Grant Program (SLDS)https://nces.ed.gov/programs/slds/2 Virginia Career Education Foundation (VCEF)http://www.vcefworks.org3 Delivering Government Solutions in the 21st Centuryhttps://www.documentcloud.org/documents/4548493-Delivering-GovernmentSolutions-in-the-21st.html4 Common Education Data Standards Initiativehttps://ceds.ed.gov/whatIsCEDS.aspx5 Fueling High-Tech Growth through Minority Businesshttps://www.commerce.gov/news/blog/2015/08/fueling-high-tech-growth-throughminority-businesses""If the Department of Commerce and other data-holding agencies reduced the burden of redundant security and other controls on researchers, meaningful research could be conducted on linked data. Submitted by: American College of Cardiology.","Social Security Death Master FileClinical data registries such as NCDR often have difficulty obtaining access to mortality information on patients who have undergone procedures. Obtaining information pertaining to patient death, along with other outcomes, is critical to understanding those outcomes, assessing the quality of care patients receive, identifying appropriate patient selection criteria, improving clinician education and more. The Social Security Death Master File (DMF) would provide this information when combined with data maintained by the College through NCDR. Specifically, the ACC views the DMF as a unique source because it contains vital status information on the US population, regardless of how healthcare is paid for, and is provided on a more frequent basis than any other data sources containing this type of information, providing more timely, and thus, more meaningful feedback on patient outcomes.Unfortunately, because of statutory and regulatory changes, the College is unable to access the DMF directly at this time, despite being able to access other sensitive sources of data maintained by the government.Under 42 USC § 405(r)(9), the College believes that HHS has the authority to match Medicare claims data with death data contained in the SSDMF file and to provide the linked dataset to clinical data registries for use in improving care quality, thus providing clinical data registries with this vital information. Unfortunately, despite clear interest by CMS to incorporate clinical data registries into patient care for the purpose of improving outcomes, HHS has declined to provide this access. The ACC believes it is in the public interest to provide organizations, such as the College, access to this information and urges the DOC to encourage HHS to do so, as well as to ensure that such situations are addressed under the Federal Data Strategy. Granting this access will improve accuracy, which will augment the ability of clinical data registries to provide clinicians with accurate information on improving the quality of care patients receive.Individualized security programs DOC is not the first federal government department or agency to address the question of how to ensure appropriate public access to sensitive government-maintained. CMS collects a significant amount of data that it makes available to researchers through a defined process. While it does not have a formal certification process, what it requires of researchers is analogous to the process created by the National Technical Information Service (NTIS) for access to the DMF. Specifically, CMS requires that researchers and those seeking access to its data enter into a Data Use Agreement (DUA)1 with the Agency. This DUA requires applicants to provide a detailed study protocol that specifies information regarding the requestor’s rationale for obtaining the data, security procedures, operational information, financial commitment and more. All individuals who have access to the data must complete and file a DUA with CMS. Additionally, there are detailed procedures that must be followed upon study completion pertaining to destruction of the data.The burdens associated with current government processes for accessing clinical data are significant. In particular, the level of burden associated with compiling CMS and CDC data requests is sizable. Simply demonstrating that all of the requirements have been met takes a significant investment of time and resources, between the development of the study protocol, completing the required forms and describing security protocols, not to mention making any required changes to current organizational protocols. Additionally, tracking all of the individual DUAs is administratively burdensome. While CMS does not officially “certify” organizations for access to its data, it will not allow access until organizations demonstrate compliance with its procedures. The same holds true for CDC. Rather than forcing organizations to comply with multiple sets of security requirements, the administrative burden will be reduced significantly for researchers if they can turn to one set of requirements. As such, the ACC recommends that DOC create standard programs for providing the public with access to sensitive data sets based on the requisite data security requirements, deeming individuals or organizations that have met those requirements as certified for access to requested datasets. Medical researchers often must contend with multiple mechanisms for ensuring data privacy and security. Because the ACC maintains individually identifiable personal health information (PHI), the College is required to comply with all of the privacy and security requirements of the Health Insurance Portability and Accountability Act (HIPAA). Additionally, the ACC has obtained data for research from CMS and must comply with the terms of its DUA. The College would be happy to furnish DOC with information regarding its privacy and security protocols to assist in the development of the DMF certification process, as we have done as part of a DUA with CMS. However, to do so in a public forum could potentially jeopardize the privacy and security of data maintained by the College. As such, we would welcome the opportunity to follow up with DOC directly to explain the various measures that are employed by the College to ensure data privacy and security.Redundant and burdensome fee structuresAdditionally, the cost for obtaining both the CDC NDI files and the CMS data can be quite expensive.Both require payment on a per-record basis with additional flat fees required to be paid by all requestors, regardless of the number of records requested. This does not factor in the costs associated with the resources required to apply for the data. Such high costs reduce the ability of researchers without significant resources to access the data for research and other legitimate purposes. The College urges the DOC to consider the government’s fee structure associated with access to such datasets and to impose only such fees as are necessary to pay for the resources needed for the creation and maintenance of the certification process.Currently, access to certain files used for medical research, such as those of CMS and CDC, is controlled based on specific study protocols. In these instances, separate requests are required based on the intended use of the data, along with separate fees and repetitious demonstrations that the College’s protocols meet the security requirements. Additionally, access is limited to the specific data sets required to conduct the specific study. This imposes unnecessary and potentially cost-prohibitive burdens upon researchers, particularly those interested in using the data to improve patient care and outcomes. The ACC urges the DOC to address these redundancies and limit the related burdens as it develops the Federal Data Strategy""If the Department of Commerce exercised its discretionary authority to share data with outside parties, citizens would be empowered to assess the quality of services they access. Submitted by: American College of Cardiology.","Medicare claims data Quality measurement is key to improving patient care. Matching the high quality clinical data from NCDR with claims data from CMS would permit the College to model/calculate risk-adjusted outcomes measures such as 30-day complication rates, 30-day readmission rates and potentially others for Medicare patients with the ultimate intention of including these measures in our voluntary public reporting program, a program that began in September 2015 and grew to include more than 200 hospitals within the first six months. However, current regulations restrict access to the Medicare claims dataset to select groups that meet the requirements of the qualified entity (QE) program. Under the QE program, groups seeking access to the Medicare claims dataset must combine the dataset with another claims dataset. Recent statutory changes permit qualified clinical data registries (QCDR) to access the Medicare claims dataset without the second claims dataset. To improve existing metrics and to develop additional ones, the College applied to become a QE under the existing program and was rejected because it did not propose to combine the Medicare data with another claims dataset. Of ACC’s ten registries, only two meet the requirements to be QCDRs, meaning that the College, despite its widely acknowledged commitment to quality measurement and service as a trusted source of information pertaining to quality patient care, has limited ability to access the Medicare claims dataset to develop quality measures. It is clear from the recent statutory changes, as well as recent regulations pertaining to access to Medicare claims data that CMS intends for medical specialty societies to obtain access to the Medicare claims dataset and for the aforementioned organizations to use this data for the purpose of developing clinical quality measures. However, all will face the challenge of identifying an additional reliable and valid nationwide claims dataset with which to combine the Medicare claims data. Based on the College’s extensive research into this problem, there are some reliable local and regional claims datasets available for purchase, but no reliable national database of private payer claims with which to combine the Medicare data. While incomplete datasets are useful for a variety of purposes, including signal development and research, quality measure development and reporting are not among them. Those gaps in the datasets would produce unreliable and invalid metrics that could tarnish the reputations of high quality hospitals and clinicians, harming patients in the long run. Providing the ACC and other medical specialty societies with access to the Medicare dataset would allow such groups to match the dataset with high quality, validated clinical data collected in clinical data registries, such as the NCDR, to develop quality metrics that would provide patients with information regarding the quality of care they can expect to see from hospitals and clinicians. The College believes that the public benefit of this effort is greater than any reservations CMS has regarding reliability and validity of metrics developed only with one source of claims data. Additionally, the ACC believes that combining the claims data with NCDR’s clinical data will offset any other concerns that CMS has with respect to the use of only one claims dataset in future analysis of the linked datasets. The College urges the DOC to examine, as part of the Federal Data Strategy, any potential statutory hurdles to furnishing access to parties that Congress clearly intended to include. Additionally, the ACC urges DOC to work with relevant agencies to encourage the exercise of discretionary authority granted to them to provide access to the data to as many individuals or organizations as possible. ""If the Census Bureau adopted new data sources and collection techniques, it could reduce respondent burden in increase sample representation while maintaining confidentiality and privacy. Submitted by: Census .","Objective 1.2Use new data sources and collection techniques to reduce respondent burden and increase sample representation, while maintaining confidentiality and privacy.  The Census Bureau is committed to continually improving data collection and data products, while exploring ways to reduce respondent burden.  Some of the methods include: using alternate data sources for survey supplementation, moving toward 100% Internet response, where possible, and enhancing data products based on emerging customer needs.  The Census Bureau is also committed to disclosure-protection, and will develop and implement modernized disclosure-protection policies, procedures and systems to support rigorous privacy protections for data providers, while also ensuring timely, efficient, and high-quality dissemination of our most important data products.Outcomes for Objective 1.2: 1. Products are created using new data sources, with no additional respondent burden. 2. Respondents report a decrease in burden from responding to Census Bureau data requests. 3. Data collections via surveys or censuses improve digital response rates annually. 4. Data providers willingly provide data to the Census Bureau, and indicate trust that the Census Bureau will protect their data from disclosure. ""When CMS Office of Minority Health developed a tool called Mapping Medicare Disparities, it leveraged CMS data to help understand health disparities geographically. This information may be used to target interventions and improve health outcomes. Submitted by: HHS.","The CMS Office of Minority Health has developed an interactive mapping tool named the Mapping Medicare Disparities (MMD) Tool.  The MMD tool is designed to identify areas of disparities between Medicare Fee-for-Service (FFS) sub-populations (e.g., racial and ethnic groups) in health outcomes, utilization, and spending. The tool presents various health-related measures by state/territory, county, race and ethnicity, age, sex, and dual eligibility status (beneficiaries eligible for both Medicare and Medicaid programs).  Currently, the tool provides data from calendar year 2012 to 2015.  The tool leverages CMS data to provide a starting point for understanding and bringing awareness of health-related data and health disparities geographically.  This information may be used to target vulnerable populations for potential interventions, decreasing health disparities, and resulting in better health outcomes.  The MMD tool can help inform the three strategic areas out of the four outlined by the Federal Data Strategy at >https://strategy.data.gov/<:1.      Use, Access, and Augmentation2.      Decision-making and Accountability3.   Commercialization, Innovation, and Public Use""When the Department of State's Bureau of Consular Affairs shares information via the U.S. Visas webpage, it helps traveling U.S. citizens know whether to apply for routine or expedited service. Submitted by: DOS.","The Bureau of Consular Affairs:Facilitating Travel – Foreign nationals seeking nonimmigrant visas to the United States can use the U.S. Visas webpage to obtain up-to-date information on appointment wait times for all visa categories.  For U.S. citizens traveling abroad, the Processing Times and Get a Passport in Hurry webpages use processing times to help customers know whether they need to apply for routine service, expedited service by mail, or expedited service at a passport agency or center.""When the Department of State's Bureau of Consular Affairs works with its four directorates and 13 partner bureaus, it is able to verify that fees are in line with the cost of providing services and that resources are aligned with priorities. Submitted by: DOS.","Responsible Stewards of Resources – CA collects information from its four directorates, 13 partner bureaus, and custom-built overseas employee surveys to recommend fees in line with the cost of providing consular services.  CA also uses this information for planning and budget formulation to verify that resources are properly aligned with Department priorities.  Finally, CA administers surveys to identify risk factors that could impact CA’s vision, mission, goals, and objectives. ""When the Department of State's Bureau of Consular Affairs analyzes information from its social media platforms, it is able to strategically reach out to citizens. Submitted by: DOS.","Public and Congressional Engagement – CA tracks questions from the public submitted via social media platforms; inquiries from Congress on visas, passports, and issues involving U.S. citizens traveling overseas; and questions from the press.  CA analyzes this information to formulate outreach strategies and craft targeted messaging for private citizens domestically and abroad.""When the Department of State's Bureau of Consular Affairs completes its Enterprise Data Warehouse, users will be able to update their own datasets and employees will deploy more effective analytics and relevant metrics. Submitted by: DOS.","CA Enterprise Data Warehouse (CA EDW) – CA is building an enterprise data warehouse to support reporting, analytics, and data-driven decisions.  When complete, the CA EDW will enable users to update their own datasets and CA will be able to reallocate technical resources previously used to support these updates to other projects.  This initiative will support:• Data and analytics to assist visa adjudicators and consular managers make quicker, more efficient decisions and prevent fraud. • Data to analyze the effectiveness of contractors to accept passport applications at passport agency counters.• Metrics on all services provided to U.S. citizens to assist with training, staffing assignments, and identification of systems improvements.   • Data for audits of financial transactions at U.S. embassies and consulates.""When the Department of State's Directorate of Management Innovation  uses data from logistics, maintenance, and other systems to generate metrics and dashboards, it helps improve management support overseas. Submitted by: The Office of Management Policy, Rightsizing, and Innovation:.","Management Performance MetricsThe Directorate of Management Innovation provides metrics and dashboards to help improve management support services overseas. These metrics and indicators help posts in the field view their staffing footprint, operations, performance metrics and customer satisfaction scores. The data that is pulled to create the metrics and indicators comes from several enterprise systems including logistics, maintenance, accounting, and the service management system.""When Department of State developed the S/ES Memo Dashboard on a data model build around decision-memo characteristics, it allowed staff assistants to prioritize documents for principals in the Department. Submitted by: The Office of Management Policy, Rightsizing, and Innovation:.","S/ES Memo DashboardThe Line is a major clearinghouse for important policy and operational documents, including those tasked out on topics where knowledge management gaps exist. However, they lacked a way to track the volume and quality of paper by bureau/office, reviewing principal, and topic. The S/ES Memo Dashboard and associated data model allow the Executive Secretariat to visualize trends in the quantity, type, and quality of documents sent to principals. With new insights into the type and quality of paper sent to the 7th floor, The Line's Staff Assistants can spend more time reviewing high priority documents en-route to principals and addressing knowledge management gaps. ""When the Department of State used cluster analysis to group its 270 distinct locations into 5 major archetypes, it helped planners make resourcing decisions more effectively. Submitted by: The Office of Management Policy, Rightsizing, and Innovation:.","Post Level Cluster AnalysisThe Department of State’s operations span over 270 different locations and thematic missions, yet managing resources across strategic interests requires an understanding of common operating models across all posts. The Post-Level Cluster Analysis grouped posts into 5 major archetypes along the mission functions and management support executed at each post. This provides strategic planners and decision-makers with a key resource to identify peer posts for comparisons, and identify posts which execute truly unique functions for the U.S. Government.""When the Department of State completes a self-service enterprise analytics model, subject matter experts will be more data-driven. Submitted by: The Office of Management Policy, Rightsizing, and Innovation:.","Shared Service ModelA/EX established a shared service model for self-service enterprise analytics which will provide cost avoidance by centralizing licensing under one enterprise environment for Tableau Server.  The shared service model expands accessibility to all offices by scaling cost.  The focus on self-service enables subject matter experts to be more data-driven at the business level, ensuring an iterative and scalable approach for the agency.""When the Department of State created a fraud analytic forensics program and related data tool, it helped posts develop process and policy improvements. Submitted by: Bureau of Administration:.","Data ForensicsIn FY 2016, the Office of Logistic Management (A/LM) began a fraud analytic forensics program which developed a system-based data analytics tool along with a three-point data forensics plan to identify, combat, and prevent non-compliant behavior and potential fraud at post. The team implemented the plan starting with the Department’s personal property and it analyzed 3.4 million assets worth $2.4 billion across 244 posts. In FY 2017, the forensics team expanded the program and began analyzing the $7 billion of domestic and $2 billion of overseas goods and services procured annually by the Department.  A/LM’s data forensics team communicated the results of its analysis to many individual Posts, which lead to process and policy improvements for tighter management controls as well as the identification of fraud. Additionally, training sessions were developed and provided at extensively attended Regional Bureau training events.""When the Department of State established a shared service platform for cloud business applications, it allowed managers to improve processes and reduce data silos. Submitted by: Bureau of Administration:.","Shared Service Platform of Cloud Business AppsA/EX established a shared service platform for cloud business applications, which provide robust real-time reporting to business and process owners. These metrics allow managers to identify bottlenecks, and see opportunities to continually improve their processes. By embracing a shared platform all applications utilize the same core data, which is the fundamental to establish enterprise datasets and architecture. The ease by which data can be shared within the platform greatly reduces data silos.""When the Department of State's Office to Monitor and Combat Trafficking in Persons completes development of a web-based Information Management System, it will leverage algorithms to help analysis review the most pertinent information. Submitted by: Office to Monitor and Combat Trafficking in Persons:.","Accuracy in ReportingThe Office to Monitor and Combat Trafficking in Persons (J/TIP) has developed a proof of concept tool to support the efficiency and accuracy of its analysts as they develop their yearly report on Trafficking in Persons. The tool, named “Wilber” after the famous abolitionist William Wilberforce, is a web-based Information Management System that will support J/TIP analysts as they conduct complex review of a combination of open source and internal State Department cable data. The tool uses natural language processing algorithms to analyze ALDAC memos on the topic of human trafficking, to help nudge analysts towards the most pertinent information as defined by an international framework on human trafficking.""When PEPFAR uses dashboards, it harnesses the power of data to improve program performance and provide a transparent lens into its work. Submitted by: Office of the Global AIDS Coordinator:.","PEPFARThe PEPFAR Dashboards demonstrate PEPFAR's commitment to using data to drive greater impact, transparency, and accountability. Harnessing the power of data, PEPFAR has improved partner performance and increased program efficiency and effectiveness. With the PEPFAR Dashboards, you can explore PEPFAR's global investments and achievements in responding to the HIV epidemic. The dashboards present information related to PEPFAR’s annual results, planned funding, central funding initiatives, and program expenditures. You can view data in a variety of formats, from a comparison of sub-national level results to country-wide trends to global patterns, including quarterly data (https://data.pepfar.net/quarterlyData/).  Through the PEPFAR Dashboards, data can be disaggregated by age, sex, and geography, helping to target and tailor our efforts, which include addressing the disproportionate burden of HIV/AIDS borne by women and girls. ""When ForeignAssistance.gov launched a data analysis tool powered by foreign assistance data, it allowed users to explore trends and spark insights. Submitted by: The Office of U.S. Foreign Assistance Resources:.","The Office of U.S. Foreign Assistance Resources (F) provides dynamic tools and resources to assist the U.S. government in foreign assistance implementation, and for the American public to better access and understand foreign assistance.ForeignAssistance.govForeignAssistance.gov recently launched a data analysis tool that helps users create custom visualizations powered by U.S. foreign assistance data and nearly 300 expertly curated country performance indicators. Users can plot this information to explore trends and spark insights. Users can also build charts to investigate development trends over time or across countries, regions, income groups, and more. www.foreignassistance.gov/analyze""When the Department of State launched the Foreign Assistance Data Review, it helped track and standardize foreign assistance data across Department bureaus.  Submitted by: The Office of U.S. Foreign Assistance Resources:.","Foreign Assistance Data Review (FADR)The Department of State continues to make progress in its efforts to improve tracking and reporting of foreign assistance data through the Foreign Assistance Data Review (FADR). The FADR was chartered in September 2014 to evaluate the capture and tracking of foreign assistance activity data from budgeting, planning, and allocation through obligation and disbursement. This full report outlines findings and recommendations for improving the Department's ability to track and report foreign assistance data, and approaches to enable more accurate, streamlined, and standardized foreign assistance data across Department bureaus. A FADR Data Element Index identifies 57 data elements that must be standardized and incorporated into the enterprise-wide data management systems.""When the Department of State uses the Standardized Program Structure and Definitions as an interagency tool, it allows multiple organizations to manage foreign assistance with a common vocabulary. Submitted by: The Office of U.S. Foreign Assistance Resources:.","The Standardized Program Structure and Definitions (SPSD)The Standardized Program Structure and Definitions (SPSD) is F’s “dictionary,” providing a common set of definitions and a consistent way to categorize and account for State Department and USAID managed foreign assistance. This common language allows us to establish indicators for measuring performance, and to develop a comprehensive body of knowledge regarding program effectiveness. The SPSD is an interagency tool that provides a common vocabulary, and the capability to respond quickly and transparently to stakeholders.""When the Department of State deployed the Program Design and Performance Management Toolkit, it helped the Department design more effective programs. Submitted by: The Office of U.S. Foreign Assistance Resources:.","The Program Design and Performance Management Toolkit The Program Design and Performance Management Toolkit provides the State Department with step-by-step instructions and templates for designing programs, projects, and processes that align with and advance their broader strategic goals, monitoring and evaluating progress and results, and using that information to conduct internal learning.""When the Department of State collects Performance Plan and Report submissions from all missions and bureaus implementing foreign assistance, it helps the Department assess progress against strategic objectives. Submitted by: The Office of U.S. Foreign Assistance Resources:.","Performance Plan and Report (PPR)An annual report submitted by all missions and bureaus implementing foreign assistance resources to convey progress towards their foreign assistance strategic objectives.""When the Department of State uses the Capability Maturity Model Integration Data Management Maturity model, it improves data management across the mission. Submitted by: Bureau of Political-Military Affairs Directorate of Defense Trade Controls:.","Capability Maturity Model IntegrationPM’s Directorate of Defense Trade Controls (PM/DDTC) is using the Capability Maturity Model Integration (CMMI) Data Management Maturity (DMM) model to improve data management practices across the full spectrum of our mission.  DDTC has implemented a standard set of data best practices to assess capabilities, strengthen our data management program, and develop a roadmap for data improvements that align to our mission goals.""When the Department of State created an interactive dashboard powered by export data, it helped inform senior officials decisions and generate responses to requests for defense trade information. Submitted by: Bureau of Political-Military Affairs Directorate of Defense Trade Controls:.","PM/DDTC DashboardOver the last several months, PM/DDTC created an interactive Tableau dashboard that filters export data to show historical defense trade trends, comparison across regions and countries, as well as defense article commodities.  The dashboard produced reports used in Information Memoranda for senior State officials, as well as part of a presentation made by a U.S. delegation at a multilateral conference on conventional munitions.  The dashboard also graphically displays trends over time and developments with defense trade partners that could not have been detected by our traditional PDF depictions of Excel spreadsheet information.  As a result, PM is able to respond faster (within minutes, not days) to requests for defense trade information by utilizing new filtering and data visualization techniques, both for internal customers within State as well as in “data” diplomacy efforts abroad.  ""The Administration should leverage existing standards efforts, such as GAAP, DAIMS, and NIEM. Submitted by: Data Coalition.","As the Administration makes plans to improve management practices we hope leadership will consider leveraging existing standards efforts, such as the US GAAP Taxonomy, the DATA Act Information Model Schema (DAIMS), and the National Information Exchange Model (NIEM), which have each begun the important work of using government data to drive oversight, reform, and accountability.""If the Administration works with the SEC and the FASB to assure that the US GAAP Taxonomy is the true taxonomy element reference and minimize the creation of extension data elements, the SEC and external parties will benefit from the resulting improvement in data quality. Submitted by: Data Coalition.","USE CASE #1: FIX PUBLIC COMPANY FILINGS (ACCESS, USE, AND AUGMENTATION) Since 2009, the Securities and Exchange Commission (SEC) has been requiring public companies to report their financial statements in a standardized, non-proprietary machine-readable format called eXtensible Business Reporting Language (XBRL). Maintaining the standard is the independent Financial Accounting Standards Board (FASB), which has developed and continually worked to update a taxonomy, or list of data tags, reflecting the accounting concepts of U.S. Generally Accepted Accounting Principles (US GAAP). (For details, see: https://www.fasb.org/cs/ContentServer?c=Page&cid=1176169699514&d=&pagename=FASB%2FPage% 2FSectionPage) (see also: https://www.workiva.com/blog/your-guide-2018-us-gaap-taxonomy-update).However, the implementation of this taxonomy has not fully delivered the promised benefits of comparability, transparency, and market efficiency. First, the US GAAP Taxonomy is too complicated to allow easy comparability and has thousands more electronic tags than are needed to accurately reflect corporate financial statements. Second, the SEC permits companies to create custom-built extension tags in far too many circumstances. The major information data aggregators, such as Bloomberg, Thomson Reuters, and Morningstar, still do not ingest the SEC's XBRL corporate financial data without significant edits. Smaller, creative intermediaries are making progress, but the reality is that the U.S. capital markets do not efficiently absorb XBRL data. As a result, U.S. public companies are not experiencing lower costs of capital from filing XBRL reports. Until the SEC fixes the US GAAP Taxonomy and addresses the overuse of extensions, its XBRL open-data reporting regime for financial statements will not deliver the promised benefits.The Data Foundation’s “Open Data for Financial Reporting: Costs, Benefits, and Future” (see: http://www.datafoundation.org/xbrl-report-2017) provides the history of the US GAAP Taxonomy, outlines the benefits to the SEC’s disclosure modernization reform efforts, and provides actionable recommendations for improvement.A recent report by member company Donnelley Financial Solutions, “A New Approach to Data Quality: How the SEC Can Prepare for a RegTech, SupTech and AI Future” (see: https://www.datacoalition.org/wp-content/uploads/2018/07/DFS-Summary-June27Final.pdf), outlines additional steps to address taxonomy improvements, while also outlining the future benefits to the regulators, including the applications of emerging tech for supervisory technology workflow (i.e., SupTech; the technology regulators use internally to conduct their enforcement, supervisory, and research activities).ACTION:The Administration should work with the SEC and the FASB to assure that the US GAAP Taxonomy:First, enforces FASB codifications as the true reference for all elements in the taxonomy, thereby eliminating unnecessary tags, reducing the complexity of the taxonomy. For example, incorrect references should be replaced with missing references. The filer is in the better position to know the relevant codifications that shape their filing, and can help make self-tagged data more accurate than service-provider tagged data. Additionally, accessing elements through the codifications will make it easier to find the standard elements to use, thereby eliminating unnecessary extension elements. The SEC should make sure filers follow the recommendations of the FASB’s Efficiency and Effectiveness preparers report.Second, minimizes the creation of extension data elements, thereby improving comparability and data quality.INTERNAL BENEFITS:By reforming the US GAAP Taxonomy, the SEC will gain the benefits of ingesting more accurate and comparable corporate information to conduct anti-fraud investigations and identify systemic market risk.EXTERNAL BENEFITS:Investors and data aggregators will benefit from more comparable market information. Filers will benefit from a taxonomy that more closely mirrors the requirements of US GAAP and their internal accounting and audit practices.""If the Administration endorses the Legal Entity Identifier as the default, agencies and external parties will benefit from increased ability to match data. Submitted by: Data Coalition.","The federal government currently uses a wide variety (see: https://www.datacoalition.org/a-jungle-of- entity-identifiers-and-what-were-going-to-do-about-it/%5C) of codes to identify companies, nonprofits, and other non-federal entities. Because the same entity is identified differently with each agency it reports to, entity matching is a time-consuming and expensive threshold challenge for nearly every federal data analytics project.In 2010, to address the expense and opacity of incompatible entity identification systems, the U.S. Treasury Department and foreign financial regulators started a global nonprofit organization to create and maintain the Legal Entity Identifier (LEI) (see: https://www.financialresearch.gov/from-the- director/2017/02/02/breaking-through-barriers-impeding-financial-data-standards/), a versatile and non-proprietary code that can be adopted within any regulatory regime. Several federal agencies are now considering adopting the LEI alongside existing non-interoperable identification codes.ACTION:The Administration should establish a policy statement endorsing the LEI as the default identification code for legal entities. The White House OMB Office of the CIO should direct all agencies to report on the disparate identification codes they currently use and the feasibility of a full transition to the LEI. The federal government should adopt an open and common universal identification code to be integrated into agency systems. These actions should be directed by a universal entity identification working groupINTERNAL BENEFITS:The establishment of a universal entity identification working group will speed up current federal agency initiatives seeking to standardize entity identification and encourage the formation of new ones.Ultimately, the adoption of a universal, non-proprietary entity identifier will enable agencies to aggregate, compare, and match data sets critical to their regulatory or programmatic missions.EXTERNAL BENEFITS:This will facilitate, where appropriate, the publication of bulk data sets which would otherwise be inhibited by the proprietary nature of currently-used entity identification systems.""If the Administration utilizes DAIMS for modernizing budget formulation and agency financial reporting, both agencies and Congress will be more empowered to make data-driven decisions. Submitted by: Data Coalition.","The U.S. Treasury and the Office of Management and Budget have spent over four years working with agencies and outside stakeholders to establish and integrate the DAIMS into agency systems. With over 400 unique data elements the DAIMS represents the most comprehensive and widely adopted data standard for federal operations in the U.S. government. The DAIMS links budget, accounting, procurement, and financial assistance datasets that were previously segmented across agency systems and databases. OMB should rely on the DAIMS’s open documentation architecture which allows for ready expansion and linkage to other administrative datasets. Furthermore, in his Senate confirmation hearings OMB Director Mick Mulvaney suggested that future budget requests could be published in a machine-readable and DATA Act compatible format.ACTION:The Administration should utilize the DAIMS for modernizing the annual budget process, agency financial reporting, and agency performance reporting:Annual Proposed Budget - The Administration should centrally publish the Administration's annual budget request including the corresponding standardized agency Congressional Justifications (CJs) in a bulk downloadable, machine-readable, and DAIMS-compatible format.Congressionally Approved Budget - The Administration should publish the Congressionally produced federal budget in a manner that integrates with the DAIMS to enable comparisons with current agency finances.Agency Financial and Performance Reporting - Machine-readable data, in a common schema (e.g.,integrated with the DAIMS), should be part of OMB’s Annual Performance and Annual Financial Report which agencies are required to submit. Data-driven reporting provides an opportunity to improve efficiency while creating a more sustainable, meaningful, and repeatable process. Creating the critical connection between finance and performance information in a compelling, engaging, and informative way provides the opportunity for agencies, constituents, OMB, Congress, and others to derive greater value and insight from these reports. OMB would need to replace the document-based reporting methods with a consistent data schema that enables data-centric agency budget and financial reporting. OMB and agencies should also tie their goals to broader program areas.INTERNAL BENEFITS:Annual Proposed Budget - Will enable the Administration to easily use data to compare and justify the proposed budget in support of strategic goals.Congressionally Approved Budget - Will help agencies monitor current spending against the enacted budget, thus enabling visibility into funding distribution and timely planning.Agency Financial and Performance Reporting - Will help agencies adopt efficient data-driven strategies, diminish duplication of efforts, increase intra- & inter-agency information sharing, and reduce the resources required for information collection.EXTERNAL BENEFITS:Annual Proposed Budget - Will enable Congress and the public to more efficiently ingest linked data compromising the annual federal budget, parse proposed changes to programs, quickly compare budget plans against current fiscal year spending activity, and perform external budgetary analysis utilizing accurate data linkages. Will enrich the national conversation around the government’s budget by assuring more accurate interpretations and enable more informed discussions about the performance of particular government programs. Congressionally Approved Budget - Will help users more efficiently interpret the enacted budget in the context of specific agency program funding and award opportunities that matter to their own organizations.Agency Financial and Performance Reporting - Benefits include transparency and accountability from publishing public sector data which fosters innovation across the academic, research, and technical developer community. By putting digitally integrated performance and budget information in the hands of the public, agencies will have the opportunity to articulate what’s working and what’s not, and develop necessary buy-in for improvement plans.""If the government built a platform supporting access to multiple data sources and user groups, agencies would benefit. Submitted by: IBM Global Business Services .","1.3 Implement Supporting Platform(s)The Federal government should consider a platform (or platforms) that facilitates the use of multiple sources of data by multiple user groups. The platform should be flexible for future expansions in number and types of data, users, and use scenarios. Federal agencies would benefit from data architecture large enough to hold vast quantities of data, especially as we move to more real-time data collection through smart devices and sensors. The infrastructure needs to be accessible to a large number of users and robust enough to provide open access to multiple users at one time if they simultaneously try to access the same data. Such is the case with NOAA weather data that many users access concurrently when incorporating real-time weather data into their severe weather warnings. Finally, the infrastructure needs to be large and accessible to ensure sufficient storage capacity in order to archive the data.The following items support this task:Architecture – Assess current and future architecture states, explore data demand profile, and provide roadmap and recommendation to address the architecture strategy.Archive and Streamline Applications – Consolidate applications, retire outdated databases, and modernize existing systems to focus on important projects, thus reducing risk and enhancing application quality.""When analytics are applied to data, they can be transformed into tangible business value. Submitted by: IBM Global Business Services .","1.4 Employ Effective Analytics ToolsAnalytics is a crucial piece of the strategy. Through analytics, the Federal government and those with access to the data can transform this data into tangible business value. Analytics tools enable data scientists, data engineers, machine learning engineers, analysts, and other users to collaborate with best-in-class open source tools and visual tools, along with the most flexible and scalable deployment options. Analysts can combine machine learning models with advanced prescriptive modeling to optimize complex business decisions. Figure 2 illustrates how various analytics methods enable users to leverage data.""When NIST standards and guidelines are used, policy, privacy, security, regulation, and compliance related to data use will be protected. Submitted by: IBM Global Business Services .","1.5 Establish Data Policy and SecurityData policy and security encircle this entire process. Risks are carefully evaluated and mitigation plans provided. Data security is diligently addressed and policy carefully followed. The following items are considered for this task:Policy – Identify policy gaps and recommend ways and means to close these policy gaps.Privacy & Security – Develop privacy and security best practices, process automation and policies.Regulation & Compliance – Identify external policy drivers, determine internal requirements and compliance measurements. Data usability standards require interoperability of data with a consistent format that is usable across systems and institutions. For the special cases of data related to privacy, national security, or proprietary origins, federal standards on privacy and cybersecurity should also be applied. National Institute of Standards and Technology (NIST) standards and guidelines including NIST SP 800-53 Security and Privacy Controls for Information Systems and Organizations, and the NIST Cybersecurity Framework are currently used to address and support the security and privacy needs of U.S. Federal government information and information systems.""If the government selects a proven Data Governance Framework, such as the Information Governance Operating model described by IBM in this comment, roles for data stewardship and data policies could be defined to facilitate use of disparate data sources. Submitted by: IBM Global Business Services .","2.1 Enterprise Data GovernanceTo implement Enterprise Data Governance the Federal government should select a robust, proven Data Governance Framework that is informed by a Data Strategy that aligns with the Government vision to leverage data as a strategic asset. IBM recommends developing an Information Governance Operating model adapting the concepts shown in the Figure below to work in the Federal government; for example, by emphasizing better decision making, more use of government data by industry, etc. A Data Governance Leadership Counsel would lead, execute, and enforce the model working with all other organizations. NIST could take a leadership role in formulating the Data Governance Framework similar to its role in the development of the successful framework for Cybersecurity. The data governance program components are enabled and enhanced by the use of technology. The rigorous implementation of this operating model will help to define roles for data stewardship, establish a data governance framework to consolidate and streamline data, de- conflict data, and develop consistent data policies so that the data management system can achieve meaningful use from disparate data sources across the Federal ecosystem.""If the government creates a platform using education and job data, job-seekers and employers will benefit along with the economy. Submitted by: IBM Global Business Services .","4.1 Use Case 1A COGNITIVE JOB MATCHING PLATFORM EXPANDS OPPORTUNITIES FOR AMERICAN WORKERSPROBLEM: American workers struggle to find jobs that match their skills, experience, interests, location and other needs. And while apprenticeship, vocational training, and other similar talent development and training program resources and opportunities are available, information about them is disjointed and inefficiently disseminated. Access is managed in a stove-piped, parochial manner instead of one aimed at the job seeker, resulting in an information-sharing environment that is sub-optimized, wasteful and ineffective in achieving its main objective: to match a willing, able and skilled workforce with gainful employment opportunities.OBJECTIVE: Create a Federal user-centric, intuitive and modern platform integrating job vacancy, apprenticeship, vocational training, and other relevant skill and talent development programs to match opportunities with individual users based on their submitted profiles and needs (job history, education, skills, experience, location, career path interest, etc.). The new platform will combine the latest capabilities, solutions, and technologies implemented in the private sector with federal, state and local, trade, and non-profit resources and programs to build a cognitive job matching platform that significantly improves and accelerates access to information and optimizes resource allocation and impact. SOLUTION• Deploy a new platform to connect job/apprenticeship seekers with the most suitable opportunities, resources and information by:• Collecting, integrating and organizing published vacancy, apprenticeship, training, and other relevant information from federal, state, local, trade, and non- profit agencies and organizations using data integration and data virtualization micro-services into a federated data store that is aligned with data governance• Building out a robust user profile capability which is integrated with existing job portals/aggregators (including but not limited to USAJOBS.gov, monster.com, careerbuilder.com and others) and provides intuitive, cognitive assistance with career path and skill development tracks• Using cognitive data science models and Machine Learning techniques to access information, identify patterns and relationships across structured (metadata) and unstructured data (documents, etc.) and present tailored results to each individual user• Using Natural Language processing and Sentiment Analysis to provide enhanced conversation capabilities to users through a Cognitive Advisor, thus enabling enhanced search and self-service capabilities• Enabling job/apprenticeship seekers, recruiters and hiring professionals, and educators and training program professionals to connect virtually via a collaborative web platform and provide tailored advice and assistance to users• Setting up an enterprise-level Web and mobile platform that will drive collaboration and invoke conversation services to enhance customer interaction and satisfaction through multiple channels (consider opening other channels, such as a “311”-type hot line, for job seekers without internet or mobile access)• Enhance platform with a digital career advisor to help guide job seekers to hot skills in high demand. This advisor provides job seekers with opportunities to understand how to transform their skills individually through learning and targeted career changes and in doing so, help them achieve ongoing employability• Extend digital platform services to offer online and in-person training and boot camps. Learning and re-training is a strategic competitive advantage in industry and government alike. A digital learning experience platform provides distributed learning capability anytime and anywhere, through desktop or laptop and fully enabled mobile devices. Boot camps for candidate selection and employee re-tooling can be delivered by way of certifying entities. The focus of these activities is on technical skills that have been identified to be in demand in the new digital economy• Incorporate certification and/or credentialing of STEM and other critically needed skills. By certification, organizations can identify visible skill gaps and recognize skill impediments to achieving certifications. Digital credentials create a registry of verified skills that employers and the market value most. Developing career paths supported by defined certifications and digital credentials will educate job seekers about their skill gaps and incentivize them to progress toward their attainment. Prospective employers will gain insight into the upcoming “pipeline” of qualified or near-qualified candidates• Creating public/private partnerships between industry and government will open the door to innovation and cost savings on a scale currently unattainable. This model will accelerate adoption of industry best-in-class practices and solutions and maximize existing resources at lower cost and risk for the government. It will also provide accurate insight into talent supply and demand to help drive appropriate policy-making and inform hiring organizations about the current and future availability of talent. CRITICAL SUCCESS FACTORS & NEXT STEPS• Start small and quickly expand on early successes – focus on advanced manufacturing and IT jobs and apprenticeship programs as a proof of concept and quickly expand into other high impact areas• Enable mobile capability in the design of the solution allowing all seekers to access the resource anytime, anywhere• Foster public-private partnership and leverage best-in-class commercial practices, existing government (federal, state, local and tribal) programs and resources, and non- profit, trade, and education research and experience• Assess the viability/suitability of, and consider leveraging existing digital assets to accelerate platform development and adoption• Focus not only on intuitive UI and user-centric design but also on setting up a powerful analytics/cognitive engine to derive maximum value from structured and unstructured data• Leverage the new governance and accountability framework established under the new Executive Order on Expanding Apprenticeships to ensure stakeholder participation, collaboration, and buy-in.""If the government invests in an intelligent assistant to ingest unstructured data and analyze it using modern NLP, the government would be able to capitalize on its wealth of underutilized unstructured data. Submitted by: IBM Global Business Services .","4.2 Use Case 2AN INTELLIGENT ASSISTANT FOR STANDARDIZING, ANALYZING AND CONTINUALLY IMPROVING DATAPROBLEM: Government agencies have an abundance of valuable data, but much of that data is stored in non-standard formats, largely because the data is received from third parties, be it other government agencies or commercial sources. This data enters the agency in both structured and unstructured formats but is largely not processed or standardized for use.Unstructured data, which accounts for 80% of the world's data, is underused because traditional approaches to harvesting and standardizing are expensive and have not dealt well with variable formats. Without standardization of both structured and unstructured data, agencies, especially those with limited resources miss out on developing data as a strategic asset, failing to capitalize on an opportunity that can reinvent the way government delivers mission outcomes.OBJECTIVE: Create a cost effective Intelligent Assistant (Robotic Process Automation technology integrated with AI, Natural Language Processing (NLP) and Machine Learning) that processes, standardizes, analyzes and acts on structured and unstructured data. The Intelligent Assistant will be trained to read and standardize both structured and unstructured data into a common format for use, and load into a database, distributed database, distributed ledger, block chain, or other format for use. The innovation of the Intelligent Assistant is a cost-effective data harvesting and structuring solution that extends the capabilities of its human counterparts to capture and standardize entities from a number of sources. Unlike traditional ETL and NLP solutions that require a great deal of programming and expense, Intelligent Assistants are more user friendly, quickly programmed, and able to work flexibly with business users in environments responding to change in data capture and standardization task. Oftentimes these assistants are a great compliment to larger scale IT standardization efforts that are not as flexible in dealing with more dynamic source structures and lower volume. Beyond harvesting and standardizing data, Intelligent Assistants are able to perform operations on the standardize data set they built, other operations include, data cleansing, quality analysis, and deep analysis on structured data sets that can lead to valuable insights that improve mission outcomes.SOLUTION• Use Robotic Process Automation (RPA) and Optical Character Recognition (OCR) to automate the standardization of structured data in disparate formats into a common format. Static data that is collected from various sources can be defined by different parameters (order of characters, number of characters, etc.) – automations can be programmed using OCR to read, sort, and structure data into a common format that makes data useable.• Use Natural Language Processing (NLP) to harvest entities of unstructured data into a common format in support of various use cases. Additionally, NLP can be used to capture and organize additional context of data that can be used integrated with other data sets to provide powerful insights that may have otherwise been missed.• Establish a database to store and manage standardized data. One option is a standard database, which allows agencies to visualize the data that is available and analyze to figure out how to leverage that data to improve mission delivery. Another option is a distributed ledger, the Blockchain, which provides a constant, immutable method of storing and tracking data to make sure standardized data remains in that format and its path through analysis and output is managed within a business network.• Determine what value can be extracted from existing data and automate analysis to continue to gather actionable insights as data is changed, added and subtracted throughout the data lifecycle.• Automate the continual standardization and analysis of data and create automations that take necessary action on insights drawn from the data.CRITICAL SUCCESS FACTORS & NEXT STEPS• Find a use case and supporting data set(s). The data set should have a large volume of high value data in order to prove value. It should receive data from multiple sources, structured and unstructured, and should currently be in a form that is not standardized or not accessible. For example, Federal employee data is gathered from each individual agency and stored largely in PDFs by OPM – the automation solution could process PDFs, combine the data in those forms with structured data, and place it in a location that would be accessible and useful to OPM. This type of data issue can affect all agencies, not only in the back office and shared services space, but also in the mission space.• Start small and scale. Intelligent Automation efforts work based when they start with a small, high value use case to prove value. In this case, that may be starting with one type of unstructured data input and automating its processing and standardization into a database. From there, additional data sources would be tackled one by one until the solution encompasses all potential data sources.• Have a strategy for data management. As stated above, the data can be stored in a variety of ways, from a traditional database to the Blockchain. Standardizing without determining the proper storage and usage mechanism will fail to show the value.• Invest in Intelligent Assistants as a Service. The “as a service” model allows the government to buy automation outcomes without the time and costs required to purchase licenses, hosting, and resource training in order to deliver successful automation. IAaaS will allow this effort to succeed at low cost, in a short time frame, and will allow successful efforts to build on each other more quickly and scale.SUGGESTED PILOT USE CASES:• Patent & Trademark Office: This office within the Department of Commerce collects and uses data from various third party sources for patent and trademark application, processing, adjudication and management. Current processes to standardize and use data are expensive and cumbersome. Conduct an Intelligent Automation pilot program to harvest and standardize PTO data that can be used not only to analyze and identify patterns and trends, but also to improve PTO processes such as making routing recommendations for incoming applications, identifying commonalities and redundancies, and ultimately reducing processing time and management efforts.Grants Management: The Department of Commerce currently manages grants in several disparate systems and collects data from thousands of sources. Use the Intelligent Assistant to intake, harvest and standardize data across systems and store and manage that data in a common platform. That data can then be analyzed and acted upon to improve the speed and accuracy of grants processing while reducing overall costs.""If the government created a standard healthcare provider identifier that functioned across agency and domains, payment timing would improve and fraud detection would be more effective. Submitted by: IBM Global Business Services .","4.3 Use Case 3HOW MASTER DATA MANAGEMENT and IDENTITY RESOLUTION OF HEALTH CARE PROVIDER ‘IDENTIFIERS’ WOULD IMPROVE PROGRAM INTEGRITY AND REDUCE FRAUD, WASTE, ABUSE AND ERROR IN HEALTH CARE CLAIMS AND PRESCRIPTION PAYMENTSPROBLEM: Health care is one of the largest expenditures for the US Federal Government. Despite new approaches and programs to reel in health care costs and reverse fraud, waste and abuse (FWA), the need for healthcare services is growing. Compounding this challenge is the aging population and new Military and Veteran Health services which are now comingled with commercial health and add even greater costs. The ‘Opioid Epidemic’ has increased the need to determine identities and relationships with patients, other prescribers and pharmacies. More than 10 separate federal agencies (CMS – Medicare/Medicaid, SSA, VA, DHA, HHS, IHS, etc.), 50 states and 9 territories pay claims or reimbursements for health care services and prescriptions for over 140,000,000 Americans. In addition, each agency has its own processes and procedures to achieve program integrity and prevent fraud, waste and abuse and erroneous payments of these claims. There is NO single identifier for these Healthcare providers (i.e. SSN) which is capable of identifying the provider across the health care continuum.Short of a single consolidated shared services Center for Healthcare Claims Processing, there IS one entity that is a critical key to bring the agencies together for virtual shared services, claims and payments: The Provider (aka Practitioner, Prescriber, Facilitator).Depending on the agency, the “Provider” is known by many ‘identifiers’: CMS has the National Provider Index (NPI – a 10-digit randomly generated, intelligence-free unique identifier). Other agencies have their own way to identify a provider (State license numbers, DEA registration numbers, SSN, or others) allowing for error or intentional submission of claims for payment under other provider identifiers (name, address, etc.)OBJECTIVE: Create a virtual Federal healthcare provider/practitioner/prescriber “golden identifier” (in lieu of a National Provider Identifier) to ensure identity matched to a healthcare service provider to be used for all claims and reimbursements for payment, regardless of agency or commercial entity providing the payment for service. The new solution will combine the latest capabilities, solutions and technologies implemented in the public and private sectors that significantly improves accuracy and timeliness of payments and greatly reduces fraud, waste and abuse of bad actors, errors and omissions as they exist throughout the industry.SOLUTION• Deploy a proven Master Data Management and identity proofing resolution solution to produce a single ‘golden identifier’ of all healthcare providers, practitioners, prescribers and facilitators across all federally funded claims paying systems regardless of agency or existing identifier (i.e. NPI, SSN, State License, etc.)o Healthcare organizations that are looking for ways to enhance the quality and efficiency of care by increasing the meaningful use of patient and provider information residing in their health systems.o Building out a robust provider profile record integrating federal and state licensures and existing identifiers to ensure a complete view of each individual provider.o Determine linkages and disconnections of providers as prescribers across localities, relationships, pharmacies, identities/patients in priority on opioid use, abuse and prevention of death. Coordinate and collaborate with DEA and other Opioid Task Force programs.o Allow for existing agency program integrity and FWA solutions to accelerate from Pay-and- Chase to Prepayment reviews, thereby preventing improper payments being made.o Provides a better tool for Agency Inspector Generals to identify and pursue individuals who have committed acts and received payments for services not rendered or scamming the system.o Using Natural Language processing powered by predictive analytic markup language functions an artificial intelligence to provide enhanced capabilities to agencies, enabling enhanced search and self-service capabilities• Link with the National Practitioner Data Bank (NPDB) to ensure timely and accurate information on sanctions, malpractice, adverse impact. (This NPDB is a static repository and needs to be highly accurate and current as practitioners are delivering healthcare services via multiple agencies’ funding). Allow and engage state health agency reporting.• Create public/private partnerships between commercial insurers, pharmaceutical companies, industry and governments to open the door to innovation and cost savings on a scale currently unattainable. This model will accelerate cost avoidance and savings through best-in-class practices and solutions without standardizing on a single provider identifier in very short term. No changes will be required to existing identifiers, for minimal disruption of existing resources. It will also provide accurate insight into ALL government payments for healthcare and help drive appropriate policy-making to further reduce fraud, waste and abuse by healthcare service and prescription providers. CRITICAL SUCCESS FACTORS & NEXT STEPS• Start by understanding each agency’s provider ”Identifier” to ensure existing identifier is not a problem in and of itself. Then begin with CMS (existing NPI identifier for all providers of Medicare and Medicaid services) and one additional agency for a proof of concept and quickly expand into other agencies.• Foster public-private partnerships, and leverage best-in-class commercial practices, existing government (federal, state, local and tribal) programs and resources, and non-profit, trade and education research and experience.• Assess the viability/suitability of, and consider leveraging, existing assets to accelerate platform development and replication.Leverage new governance and accountability framework established under the new Executive Orders and Programs: PMA, Reorg and Reform, TMF (Technology Modernization Fund), etc. to ensure stakeholder participation, collaboration, and buy-in.""When Ivy Tech Community College implemented data governance and predictive analytics, it was able to proactively address struggling students to significantly decrease failure rates. Submitted by: Hitachi Vantara Federal.","Ivy Tech Community College, the state community college system in Indiana, is the largest community college in the United States with 175,000 students each year, 120 campuses across the state generating 100 million new records every day. The institution is using predictive analytics to combat a nationwide trend of fewer than 66 percent of Americans who enroll in a four-year college program graduate within six years. Thirty percent of students fail to complete their first year. To address this, Ivy Tech created “NewT” (for the “New Thing”), a system of curated data sets across student records, financial aid, and accounts payable data to eliminate data silos and put actionable data in the hands of educators who could assist at-risk students. To guarantee data quality, Ivy Tech established data governance and data preparation practices and data cleansing shifted away from the warehouse and into production with functional areas continuing to keep their own data sets clean. Basic data reporting was self-serve, with users having the power to drag and drop data elements into and out of existing templates, reports, graphs, and dashboards and share new models between users and departments without IT’s help. The technical team specifically created an innovative predictive data model and identified a data set of 16,247 at-risk students in the first two weeks of a sixteen-week term. Over 800 faculty, staff and administrators then interviewed 5,118 of those students over the next two weeks. The team learned failure was sometimes associated with healthcare, transportation and childcare challenges, and these students were steered toward resources that could help.  By eliminating data silos and using predictive analytics to identify students at risk of failing or dropping out Ivy Tech decreased the number of students with Ds and Fs at midterm by 3.3 percent, or 3,100 students""When NOAA's Office of Response and Restoration developed the Data Integration, Visualization, Exploration, and Reporting application, it empowered researchers and the public to assess disaster-related damage and restoration efforts. Submitted by: Hitachi Vantara Federal.","NOAA’s Office of Response and Restoration (OR&R) developed the Data Integration, Visualization, Exploration, and Reporting (DIVER) application to empower researchers and the public with data to assess the environmental damage caused by manmade and natural disasters and to monitor restoration efforts. Through the application’s publicly available portal, researchers and interested individuals search terabytes of data and gain insight into the impact of contaminants on the environment. DIVER ingests structured and unstructured scientific data collected by researchers in various forms including field observations, lab analysis results, photographs, geospatial data and instrument outputs. As updated or new information becomes available from each source system, data are automatically audited and pulled into the common repository. The application then blends and standardizes these disparate files and databases together. This new data is then integrated with historical data from legacy systems, which is most of NOAA’s data sets. In DIVER there are records of over 20 years of toxicity data and contaminant chemistry from sediment and tissue samples. DIVER uses this blended data to produce interactive reports made available to the public through the application’s website. On DIVER’s online browser, researchers and the public can query the data and search interactive reports. Users visiting the website can query over 40 million results through the self-service DIVER Explorer tool. Researchers collaborate across differing data standards, differing storage and organization methods, and varying levels of documentation to gain a picture of state of the environment and begin to assess impact and injury5.""When BNY Mellon replaced a proprietary system with an open-source heritage tool, it reduced costs and improved flexibility. Submitted by: Hitachi Vantara Federal.","BNY Mellon is a leading international investments company managing $1.9 trillion in assets in 35 countries with 52,100 employees. In accordance with a company-wide strategy to improve efficiency, reduce costs and minimize risk, BNY Mellon optimized its technology processes by reducing the instances of redundant, often-siloed functions, like data management. This was accomplished by replacing a proprietary system of custom tools with an open-source heritage tool to decrease costs and improve flexibility.For more information on BNY Mellon’s data governance efforts please refer to these resources, also noted in the footnotes of this document:1. Scott Stewart, ETL Architect for BNY Mellon NEXEN Analytics Interview at Pentaho World https://www.youtube.com/watch?v=mmfgapIf6yU.2. Scott Stewart, ETL Architect for NBY Mellon ""Pentaho Best Practices."" Address,Pentaho World 2017https://www.pentahoworld.com/sites/default/files/2017-11/BNYMellon-Pentaho-Best-Practices- PWorld2017.pdf""If the government works with outside collaborators to create a national Opioid Epidemic Surveillance and Response Dashboard powered by state and national health data, it will empower policymakers and individuals to make more data-driven, effective decisions. Submitted by: NYU School of Medicine .","A proposed surveillance toolTo support effective, forceful yet nimble action to quell the opioid epidemic across the United States, it is essential to harness the rich trove of confidential microdata on the opioid supply and on indicators of opioid-related harm to improve national capacity to reliably track its evolution, predict where and who it will affect next, and identify new and promising interventions.  Timely access is needed to comprehensive and geographically granular data over time on: (1) state- and local-level laws, policies, and services that may impact opioid-related harm; (2) indicators of the shifting legal and illegal drug market; and (3) measures of opioid overdose and associated outcomes. A user-friendly, web-based national Opioid Epidemic Surveillance and Response Dashboard (OESRD) can offer an effective platform for these data. It can store multiple sources of confidential microdata (i.e., hospital discharges, vital statistics, prescription retail sales, drug seizures) securely, and provide a user-friendly platform for government agencies to add confidential data as soon as it becomes available. It can allow immediate lookup of data at the local level, provide for comparisons within and across states and time, and enable local, state and federal policymakers and agencies to target policies and programs to those factors that have the greatest impact on opioid overdose rates. We propose a dashboard that focuses squarely on overdose as a lead indicator of the opioid epidemic, while also supporting efforts to track and combat other dimensions of the epidemic, including opioid misuse and opioid use disorder. Building such a resource would advance the Department of Health and Human Services’ 5 Point Strategy to Combat the Opioids Crisis, by addressing Point 2: Better Data. While disparate efforts have sought to integrate certain data on the opioid epidemic, no single national database resource of requisite depth and scale exists at this time. Proposed objectivesOverall goal1. To create a comprehensive national database of confidential microdata on fatal and nonfatal opioid overdoses, national, state and local drivers of and responses to the opioid epidemic, and to describe and analyze trends in opioid overdoses in the United States. Specific objectives1. To compile and link data on: opioid laws and policies; access to opioid use disorder services and medications; the opioid supply; opioid use, overdose, and related outcomes; factors associated with opioid overdose risk; and evidence on what works to prevent and treat opioid misuse, overdose, and opioid use disorders. 2. To create a database that provides a secure, centralized, user-friendly, and up-to-date resource to track patterns of the opioid overdose burden and local and state responses to this problem over time.3. To develop a sustainable, dynamic, public-interfacing queryable website accessible to the public, policymakers and researchers.Proposed audiences and intended applicationsThe primary audience for the OESRD will be local, state, and federal policymakers. At the local and state level, policymakers such as mayors, city managers, governors, and agency leaders can use its timely data to identify trends in prescribing patterns, drug markets and opioid overdoses, identify geographic areas of risk within and across states, develop a strategic plans to address local opioid overdose trends, and track progress towards reaching strategic goals. While several states have already developed such dashboards (see, for example: http://preventoverdoseri.org), each one captures different measures, diminishing the ability to generalize and apply lessons learned across state lines.  At the national level, policymakers and researchers can use this data to track how the opioid epidemic is evolving across states, to identify geographic and socio-demographic groups at highest risk, to test hypotheses about the potential determinants of the epidemic, and to evaluate the effectiveness of current approaches to address this public health crisis. Innovation To date, multiple efforts exist to develop national databases addressing the opioid epidemic. These include, among others: ODMAP, the amfAR Opioid & Health Indicators Database, the Centers for Medicare and Medicaid Services Medicare Part D Opioid Prescribing Mapping Tool, the Agency for Healthcare Research and Quality resource on opioids, and the Centers for Disease Control and Prevention data on overdose surveillance and opioid laws. OESRD would expand on these efforts in two important ways. First, it would provide substantially expanded data on determinants of, indicators of, and approaches to combatting the opioid epidemic. This would include: (1) information on the full range of state laws that affect opioid prescribing and access to opioid use disorder services and medications; (2) confidential microdata on local access to the supply of medication-assisted treatment and naloxone and to services to treat opioid use disorders and reduce associated harms; (3) confidential microdata on the legal and illegal opioid supply; (4) confidential microdata on the distribution of overdoses across and within states by type of drug, including types of opioids and other drugs associated with opioid overdose (i.e., cocaine, stimulants, benzodiazepines); and (5) available evidence on the effectiveness of current prevention and treatment approaches to address the opioid epidemic. In addition, OESRD could be linked with the City Health Dashboard and County Health Rankings and Roadmaps, so that OESRD could also integrate extensive data on social and economic factors, the physical environment, and health indicators within and across states. This type of secure data linkage is critical if we are to identify the potential drivers of the opioid epidemic, and how it may be influenced by other determinants of population health. Second, it would offer tools and resources so that potential end-users, including public health officials, policymakers, community members and researchers, could interpret the data. This would include, for example: (1) downloadable datasets of public access versions of the data; (2) downloadable infographics; (3) maps showing the distribution of the prevalence and incidence of overdose rates across cities, counties, and states, by age, sex, race/ethnicity, local features of the social, physical and economic environment, and type of drug; and (4) state and local profiles with local overdose statistics, downloadable charts and local prevention and treatment resources available to address the epidemic in a specified geographic area. DesignThis project is founded on the premise that to track the opioid epidemic, identify future hotspots of opioid-related harm before they occur, and identify effective state and local responses, it is critical to develop a comprehensive resource of securely linked datasets on the determinants of opioid overdose risk, indicators of opioid-related harm, and prevention, treatment and policy approaches. We propose a systematic approach to identify, compile, and link confidential microdata sources to create the first comprehensive national database on the predictors and outcomes associated with the opioid epidemic. The dashboard would coordinate efforts with existing national sources of confidential microdata on opioid use and overdose, including the Centers for Disease Control and Prevention opioid overdose surveillance efforts, the Healthcare Cost and Utilization Project hospitalization and emergency department data, the National Survey on Drug Use and Health, DEA drug seizure data, and IMS Quintiles, among others. The dashboard would harmonize data available from multiple sources, in order to provide a comprehensive, granular, and timely assessment of the profile of the opioid epidemic. Data sourcesDrug laws and policies. The OESRD would draw data from a variety of existing central sources (examples of a more extensive set of laws and policies are listed in Table 1) to create a repository of laws and policies likely to affect opioid prescribing and mitigate opioid-related harm. These data would be updated on annual basis.  Table 1. Examples of law and policy measures proposed for the dashboardConstruct Measure Years Data sourcePrescription drug monitoring programs  PDMP enactment date; PDMP features including whether registration and access are mandatory, number of drug schedules reported, frequency of required data updates, proactive provision of unsolicited reports on outlying patterns of prescribing and dispensing to users; PDMP latent classes12001-on PDAPS, NAMSDL, contacts with state PDMPsPain clinic regulations  State requirements to register with the state/obtain a license/certificate from the state; state clinic inspection authority; requirements regarding ownership and operation of clinic 2006-on PDAPSPrescription limits  Limits on duration of opioid prescription supply for acute pain; limits on long-term opioid prescribing to <90 MED/d;  2001-on Law review2,3Naloxone access laws Prescribers immune from civil/criminal liability, removes criminal liability for possession of naloxone. 2001-on PDAPSOverdose Good Samaritan laws Protection from controlled substance protection laws; drug paraphernalia laws, probation/ parole violations 2007-on PDAPSMAT with methadone laws State restrictions on opioid treatment programs 2006-on PDAPSMedicaid eligibility Medicaid eligibility categories by year, and expansions following the Affordable Care Act ; Medicaid coverage of methadone 2001-on Kaiser report; 4Opioid use disorder services and medications. Using a variety of central data sources, the dashboard would provide access to data on the capacity of health services available to reduce opioid-related harms (i.e., overdose education and naloxone dispensation programs), and to provide opioid disorder treatment (i.e., clinicians with waivers to prescribe buprenorphine, opioid treatment services), as well as on the local naloxone and medication assisted treatment supply. Table 2 lists some examples of the type of information the dashboard could provide on local access to opioid use disorder services and medications. Table 2. Opioid use disorder service and medication access measures proposed for the repositoryConstruct Measure Years Data SourceBuprenorphine treatment capacity Number of physicians at substance abuse treatment facilities and DATA-certified physicians 2018-on SAMHSA Substance abuse treatment/MAT capacity Number of substance abuse treatment facilities that provide MAT services (some, two, all) 2005-2011; 2018-on SAMHSAMAT dispensed supply Monthly number of prescriptions dispensed for buprenorphine and naltrexone by formulation; monthly number of patients receiving a prescription for buprenorphine or naltrexone 2006-on IMS Health and QuintilesNaloxone dispensed supply Monthly number of prescriptions for naloxone; monthly number of patients receiving a prescription for naloxone 2006-on IMS Health and QuintilesEmergency Medical Services  Administration of naloxone; complaints associated with administration of naloxone, with demographics 2014-on Request to states for NEMSIS dataThe opioid supply. The dashboard would provide access to data on trends in the legal and illegal drug supply across the United States, and track shifts in drug markets in so far as they may affect opioid overdose risk. This includes DEA drug seizure data, as well as provider- and patient-level data on retail sales of opioids and associated controlled substances, such as benzodiazepines. Table 3. The drug supplyHeroin price and purity Price and purity estimates, from DEA samples and purchases, and corrected for selection bias 2001-on DEA System to Retrieve Information for Drug EvidenceFentanyl law enforcement seizures Number of illicit drug seizures tested and confirmed to be fentanyl to DEA  2015-on DEA Reports 5Opioid prescriptions Rate of opioid prescriptions per 1000 residents; mean opioid dose (MME) per patient; % of patients getting >90MME; opioid-benzodiazepine overlap; multiple provider episode rates by quarter and year; mean number of opioid prescriptions written per day per prescriber percentile rank;  2007-on IMS QuintilesOpioid use, overdose, and related outcomes. A critical component of the OESRD would be indicators of opioid-related harm. These would include data on fatal overdoses and hospitalizations and emergency department visits associated with overdoses, as well as survey data on the precursors to overdose, namely medical and non-medical use of prescription opioids, misuse of prescription opioids, heroin use, and opioid use disorder. This data would be provided as overall rates, but also broken down by demographic subgroups (sex, race/ethnicity, age), and presented at the finest level of geography available, so OESRD users can track the progression of the opioid overdose epidemic across time, space, and subgroups. Table 4. Indicators of opioid-related harmOverdose deaths Monthly number of deaths from all drugs; from natural/semi-synthetic opioids; methadone; synthetics other than methadone; other drugs 2001-on CDC WonderInpatient hospital stays for overdoses Monthly number of hospital discharges associated with overdose by type of drug 2001-on Statewide Inpatient Sample; individual requests to statesEmergency department admissions for overdose Monthly number of ED admissions associated with overdose by type of drug 2001-on State Emergency Department Databases Opioid use, misuse, and disorders Nonmedical and medical prescription opioid use, heroin use, benzodiazepine use, opioid use disorder 2004-0n National Survey on Drug Use and HealthSocial, economic, and physical environment factors associated with opioid overdose risk. The OESRD could be linked with the City Health Dashboard and County Health Rankings and Roadmaps to access relevant local data on social and economic factors (e.g., education, poverty, housing costs, unemployment, residential segregation, violent crime), the physical environment (e.g., housing conditions, access to parks), health conditions (e.g., mental distress, high blood pressure, cardiovascular disease, premature deaths), and clinical services (e.g., insurance, primary care access, preventive care services access). OESRD users can apply this data to examine potential drivers of the opioid epidemic. What works in prevention and treatment.  A central question of policymakers, public health officials and community leaders is what action to take to most effectively to combat the opioid epidemic. OESRD can provide key actors with the state-of-the-art information on the types of prevention and treatment approaches and the strength of the evidence underlying each.  This section would also provide case examples where such approaches were implemented, possibly including contact information for practitioners involved in implementing such approaches across states. An example of this type of information is available here.Data linkage and analysisOESRD would provide an integrated data infrastructure to compile and harmonize data across a wide variety of already centralized sources of confidential microdata, including many enumerated above. However, for certain measures, data would need to be extracted from individual states (e.g., prescription opioid and Medicaid laws; EMS data on naloxone administration) or from reports/online queries (e.g., buprenorphine treatment capacity, substance abuse treatment capacity, drug seizure data, National Survey on Drug Use and Health), or purchased (IMS Quintiles data on retail sales of naloxone, MAT, and prescription opioids).  OESRD would provide a secure environment within which government agencies providing data could place and share their data. A module system would control who had access to which data, and how the different data sources were connected through a secure platform. Sparse data would be suppressed to ensure confidentiality and stability of the data. Mapped state and local data would be compiled in a project geographic information system (GIS) that supports all reporting and statistical analysis activities.  GIS compilation enables mapping of all data elements, characterization of spatial topologies for statistical analysis, and support for the assessment of unit misalignments that affect analyses of county level data.  Data would be linked by available identifiers – with the level of resolution varying by data source - and the database would be flexible enough to accommodate multiple linkage options, including state, county, city, year, quarter, and month. Data would be stored and accessed with a secure database management system: once entered, it would be stored in tables. The OESRD would allow users to calculate and map, among others, annual rates of opioid overdose-related deaths and hospitalizations, overall and by age, sex, race/ethnic and type of opioid (i.e., natural/semi-synthetic, non-methadone synthetic, methadone, and heroin). Users could create maps of, among others, the geographic distribution of MAT treatment capacity, MAT dispensed supply, and naloxone dispensed supply across local areas and states over the study period. Data could be downloaded, used to conduct comparisons across geographic or demographic units, or to prepare site-specific in-depth profiles of the opioid overdose problem in specific areas (i.e., states, counties, cities). Project teamTo develop a comprehensive tool that responds to the data and policy needs of a wide range of audiences, it is important to involve a team that has substantive expertise in the opioid epidemic, including policy experts, epidemiologists, and physicians, as well as expertise in using big data to build usable, secure, web-based, granular dashboards. Such a team, including national experts on substance use epidemiology, treatment and policy, and with experience in successfully launching national data dashboards, exists at New York University. The team recently launched a City Health Dashboard, capturing diverse measures of health, health behaviors, social and economic and environmental determinants of health, and equity, for the largest 500 US cities (population 66,000 and above).  These data, presented at the city and census tract level, are now freely and easily accessible to city and community leaders, national officials, researchers, business leaders, and other stakeholders.  A similar paradigm, with different, significantly enhanced and opioid-focused inputs and the ability to aggregate data by diverse geographies and attributes, could provide the backbone functionality for the OESRD.    NYU’s Administrative Data Research Facility (ADRF) could be a key incubator for this project, given their extensive experience building systems to compile, store, and use confidential microdata. ADRF could act as the incubator under the leadership of the NYU Center on Opioid Epidemiology and Policy.Key stakeholder entities would include: federal agencies, such as CDC, SAMHSA, NIDA, and CMS, as well as foundations, such as Robert Wood Johnson Foundation, Bloomberg Philanthropies and Gates Foundation. ""LEHD Origin-Destination Employment Estimates empower local economic development initiatives to develop policies and effectiveness metrics. Submitted by: Chief, Policy and Data Stewardship BranchPolicy Coordination OfficeU.S. Census Bureau.","• LEHD Origin-Destination Employment Estimates (LODES), which provides block-level employment data by where workers live and work, is utilized by many local 'smart growth' economic development initiatives, to develop policies and to measure the effectiveness of business zone improvement programs.""LEHD Origin-Destination Employment Estimates empower regional planning associations to better target improvement efforts. Submitted by: Chief, Policy and Data Stewardship BranchPolicy Coordination OfficeU.S. Census Bureau.","• LODES is also used by regional planning associations in the development of their transportation planning models, to better target transportation improvement efforts to provide better transit access for critical sectors of the workforce""Quarterly Workforce Indicators allow economic development and workforce planning agencies to better identify promising sectors. Submitted by: Chief, Policy and Data Stewardship BranchPolicy Coordination OfficeU.S. Census Bureau.","• Quarterly Workforce Indicators (QWI), which provide detailed hiring data by worker demographics, are used by economic development and workforce planning agencies to identify specific sectors of the local economy where demand for workers appears strong or growing.""Job-to-Job Flows data are used by economic development planners to estimate how new employers will hire their workforce. Submitted by: Chief, Policy and Data Stewardship BranchPolicy Coordination OfficeU.S. Census Bureau.","• Job-to-Job Flows (J2J) data, which show worker flows across labor markets, are used by economic development planners to estimate the extent to which new employers will rely on the local workforce for hiring and identify other markets that are a potential pool of labor.  ""When companies want to exchange HIPAA-related data, they need a Business Associate Agreement. Submitted by: HHS.","HIPAAWe relied on this guidance as the foundation of Blue Button 2.0:https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html Without being a policy wonk on HIPAA, the key takeaway is to understand when two companies exchange PHI they need to have a Business Associate Agreement and be “HIPAA compliant”.  When a “Covered Entity” like CMS or a Provider gives PHI to a beneficiary or patient, no BAA is required.""When requesting electronic copies related to HIPAA, legal and privacy experts can be consulted. Submitted by: HHS.","OAuth flowYou can look back at this same guidance to better understand the OAuth flow:https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html You will see sections like “Request for Electronic Copies”.  We worked closely with General Council and our Privacy team to understand the specifics of this stuff.""If the government thinks of data exchanges as typologies, such as aggregation or agency to agency exchange, common legal and technical solutions can be developed around those repeating patterns. Submitted by: GSA.","In the use cases described below with CMS on Blue Button 2.0 and with SSA for verification of consumer information with a financial institution, it's not a matter of data sharing between agencies, but brokering data sharing between the user and a third party in the private sector. I can try to share this better through the official process for the Data Strategy, but this got me thinking it'd be good to identify some common patterns for data sharing and exchange and then work to develop and highlight the best practices and standards both from different perspectives including: legal/administrative/agreements, privacy/security, data interoperability, etc. A few of the key data flow patterns I'm thinking of are:- aggregation - typically a hierarchical aggregation flow, such as from local->state->federal often with many statistical agencies or for reporting on the use of federal funds, or from federal agencies to OMB including things like POD metadata or the Data Act. These are the main flows we're starting with on the Data Federation project- agency to agency exchange - this is where agencies are creating agreements to share data directly with one another, benefits eligibility is a common scenario where this plays out. - user consent based data exchange - this is where the user actually provides consent explicitly for data exchange by granting one agency/entity access to their data from another agency/entity as with the examples below. I'm just spitballing here, but I've been starting to coalesce my thinking around these types of common patterns more and more and I feel like they're likely to have common legal and technical solutions when viewed that way. ""If the government developed a common design language for data visualization, it could enhance government data products and services at scale. Submitted by: U.S. Department of Education.","• Establish a design language for data visualization, U.S. Web Design Standards-style. In the same way that the U.S. Web Design Standards empowers teams across the federal government to “build fast, accessible, and mobile-friendly government websites backed by user research,” a well-resourced data visualization language could bootstrap and en-hance government data products and services at scale.o Leading private-sector companies have invested in design systems to drive data innovation, including IBM, which partnered with a renowned information design consultancy to “create a new set of guidelines to help IBMers from all over the world design innovative and effective data-driven products . . . The guidelines in-clude directions, original designs, animation samples, and interactive examples.”o The federal government could conduct a similar exercise, tapping internal and external expertise to develop lower- and higher-order guidance on chart models, data storytelling, and interface design for data-driven products and services. Guiding principles could be translated into repurposable components, from the simple (e.g., Excel chart templates) to the relatively complex (e.g., code snippets for building interactive dashboards), and organized in a common repository as a universal resource.""If the government deployed text categorization, it could report representative results to the public without violating sensitivities or overloading the public with too much granular data. Submitted by: High Point LLC.","LEVERAGING DATA AS A STRATEGIC ASSETAutomated text categorization is a powerful tool that allows agencies to efficiently summarize useful information buried in mountains of text. Wherever comments are recorded (public or internal), text categorization methods can be used to better understand what people are saying, and to identify trends, providing opportunities for a more focused response. READING PUBLIC OPINIONText categorization, specifically sentiment tagging, can improve agency awareness of public concerns. For example, sentiment tagging feedback from the public could quickly identify comments for & against an initiative by locating very positive and negative sentiment. With similar comments automatically grouped together, identifying common patterns would become a much simpler task for administrators.Sentiment analysis also readily forms a framework for assessing changes in public opinion – with sentiment measures in place, an alert system for sudden swings in comment sentiment would become trivial to implement. Contrast this with manual comment audits (which are labor-intensive and would not cover all comments), and the value of an automated approach quickly becomes clear: effectiveness of current programs/solutions can now be measured using feedback directly from the public.COMMUNICATING WITH THE PUBLICCategorized text allow agencies to report representative results to the public without releasing an entire free-text dataset. In the right situation, summarization (1) protects sensitive data from comments while simultaneously (2) organizing findings into a simple, reader-friendly overview for the public, which (3) prevents “information overload” and public confusion.Pairing text summarization with a limited set of real examples makes policing sensitive data exposure much less onerous while also providing public transparency into the data being considered. Providing easy-to-understand, relevant information provides the public a “big picture” view of the current discussions and the steps being taken in response. As an agency, this openness reinforces the message, “We hear you.”""When data from multiple sources is combined and monitored, more timely and cohesive reactions can be triggered. Submitted by: Qlik.","Public HealthAgencies can gain better insight into existing and emerging public health crises with the right data. Take the opioid crisis, for example. Imagine if federal, state and local agencies had a mechanism for easily sharing and analyzing their combined data. The collective data would yield huge insights the could provide local first responders with the ability to save more lives.The City of Surry, in British Columbia, is using data to identify and respond to Opioid outbreaks. When a series of events occurs indicating an outbreak (more than 4 emergency calls for heart failure within a 1 hour time period within a half-mile radius), an alert goes out to the chiefs of police, fire, ambulance and social workers. All four organizations then team up to respond to the crisis:https://blog.qlik.com/city-of-surrey-addresses-opioid-crisis-with-help-of-qlik.Here’s another example leveraging Centers for Disease Control & Prevention data, highlighting how data can inform where the problems are most pervasive: https://demos.qlik.com/qliksense/OpioidCrisisAnalysis.""If the Federal Procurement Data System is modernized, citizens, industry, and agencies will derive more valuable insights from it. Submitted by: Qlik.","Federal SpendingInsight into the Federal Procurement Data System can be modernized to enable citizens, industry, and agencies to readily gain insight into overall spend, agency breakdowns, vendor business, procurement trends, etc. The following example sources data from 2011-present from usaspending.gov and enriches it using geospatial mapping capabilities:https://fed.goqlik.com/anon/sense/app/09219f6f-6be0-468c-af3b-0dfe1d43dc96""When Brazil empowered procurement officials with historical spend data, it was able to uncover corruption. Submitted by: Qlik.","Waste, Fraud and AbuseCutting down on waste, fraud and abuse is a constant area of focus for government. Brazil has a long history of fighting corruption, and over the past two years has made a significant effort to cut down on corruption by shining a spotlight on their spending data. By arming procurement officials (called “price estimators”) with historical spend data, the Brazilian Ministry of Planning is uncovering corruption at its source (read the story: https://www.govloop.com/community/blog/fighting-fraud-analytics-lesson-brazil/). They’re then passing that information along to the police for further investigation.""When Team Rubicon, the FDA, and the EPA share data with the public, it supports their missions and builds public trust. Submitted by: Qlik.","Citizen TransparencySharing data with the public is often critical to an agencies mission and helps build public trust. Here are a few examples where agencies are providing interactive analytics to the public:• Financial Transparency. Team Rubicon, a non-profit disaster relief organization that helps FEMA, makes all donor information available to the public, promoting trust and encouraging future donations: https://teamrubiconusa.org/open/• Mission Transparency:o The Food & Drug Administration recently made all adverse events to medical devices and pharmaceutics available to the public: https://fis.fda.gov/sense/app/777e9f4d-0cf8-448e-8068-f564c31baa25/sheet/wmUCJ/state/analysiso The Environmental Protection Agency shares data around toxic waste so citizens are better informed about pollutants in the air, water and ground:https://edap.epa.gov/public/extensions/TRINationalAnalysis_dashboard/TRINationalAnalysis_dashboard.html""If problems are identified first, rather than what data is available, agencies will be more successful in navigating legal barriers to solve meaningful problems. Submitted by: Qlik.","Cross-Agency, Governmental ApproachSolving problems using data requires a cross-agency and intergovernmental approach. We believe the federal data strategy shouldhave a strong focus on addressing critical problems faced by the federal government, using data.To address these problems, we want to stress the importance of cross-agency policies and data sharing within the federal government as well as the need to share data across governmental bodies, i.e. feds sharing with states and local governments.Despite the legal impediments to sharing certain data, the goal should be open access by default, with plans to address the protection of confidential and personally identifiable information, while encouraging an analytics mindset. We encourage identification of issues/problems which the government aims to solve, a review of the data available associated with those problems, and the development of plans to use data to better understand and address – in that order. Too often we look first at what data is available and then try to identify what problems can be addressed. This approach leaves a lot on the table as far as how data can help agencies solve problems.""If an office for data management were created, it could serve as a single front door for federal data and improve data access for all. Submitted by: Third Sector Capital Partners, Inc.","Availability and usefulnessWe have found that there are inconsistent guidelines for privacy protecting data aggregation. When we report aggregate or summary statistics it is helpful if there is a common standard for when aggregate data is no longer considered private. There is no general community consensus and no consistent federal guidelines on privacy protecting data aggregation. This lack of guidance creates reticence in otherwise eager local and state privacy, risk, contracts, and general counsel offices. Partners in local- and state-level data projects (like academic institutions, advisory firms, and external experts) often hold the liability for accidental exposure and are risk intolerant. Akin to how Treasury provides capital retention requirements that allow the banking system to understand what level of risk is generally acceptable, guidelines for privacy management would spur innovation. Another barrier to using data is the variation in format of data use agreements, even between different federal agencies. The differences in requirements and processes between agencies inhibits replicating successes made with one agency. Even when the requirements are substantially the same, agency-specific process and forms force users to start near the bottom of the learning curve each time they would like to access a new data set. This presents an unnecessary barrier to data access. Even a single standard request approach with agency-specific appendices would make it much easier to navigate from the outside. Given the above constraints, availability is primarily dependent on knowing who to contact within the agency, the relationship with that person, and their available time to guide you through the process. Making the front door more accessible is an important starting point in unlocking the potential for federal data to improve government performance and life outcomes for constituents. As it stands today, each agency (and sometimes each dataset) has its own front door, making it difficult for new entrants to know who to talk to, how to find out about access procedures, and where to get additional information about the data. Much like the Office of Personnel Management professionalized and standardized civil service, a similar office for data management could make federal data a national strategic asset as opposed to an asset available only to those who have existing relationships and agreements with the sponsoring agencies.""When the Commonwealth of Massachusetts and partners launched a Pay for Success initiative, data helped agencies create more sustained change in the lives of those they serve. Submitted by: Third Sector Capital Partners, Inc.","Massachusetts Juvenile Justice PFS Project https://www.thirdsectorcap.org/portfolio/massachusetts-juvenile-justice-pfs-initiative/In 2014 the Commonwealth of Massachusetts, Roca, Inc., and Third Sector Capital Partners, Inc launched a Pay for Success initiative with up to $28 million in success payments. Over seven years, about 1000 young men in the probation or parole system, exiting the juvenile justice system, or leaving custody of the House of Correction will receive targeted life skills, education, and employment programming. The U.S. Department of Labor (DoL) awarded the first-of-its-kind PFS grant of $11.7 million for the Commonwealth to extend the project, should it prove successful, to an additional 391 young men, thereby serving over 1,300 young men over nine years. DoL uses the state’s Unemployment Insurance (UI) database to validate the number of quarters of employment rather than using the IRS data (https://www.irs.gov/statistics/soi-tax-stats-individual-tax-statistics). The recidivism metric is based on Massachusetts Department of Criminal Justice Information Services (DCJIS) Criminal Offender Record Information (CORI) data. This use case shows how data can support DoL and Department of Justice (DoJ) goals through planned access, sharing, and analysis to support resource allocation decisions and accountability for service providers creating sustained change in the lives of those they serve.""If the government provides assistance in navigating data access barriers, WIOA Pay for Performance contractors could be more impactful in affecting positive change in key outcome metrics like justice involvement. Submitted by: Third Sector Capital Partners, Inc.","Opportunity Youth Workforce Development Programs https://www.thirdsectorcap.org/sifcohort2/ On June 30, 2016, the Department of Labor (DoL) released the Final Rule on the Workforce Innovation and Opportunity Act (WIOA). The innovations provided through the WIOA Pay for Performance (P4P) provisions, allow local workforce boards to focus on longer term strategies for improving the lives of young people who are not connected to school or work. We worked in five communities to expand beyond the Primary Indicators of Performance (https://www.doleta.gov/performance/guidance/tools_commonmeasures.cfm) and build pathways to longer term metrics and non-workforce outcome metrics (https://www.thirdsectorcap.org/social-innovation-fund/early-wins-and-challenges-in-implementing-wioa-pay-for-performance-to-improve-outcomes-for-opportunity-youth/). One community has already announced a P4P contract following our work. SkillSource, a Northern Virginia nonprofit, (https://www.thirdsectorcap.org/n-va-skillsource/) wanted to move to long-term employment and non-employment outcomes. Data access barriers led them to focus on primary indicators in this first P4P contract while navigating the access issues complicated by multijurisdictional sources because of proximity to Virginia, Maryland, and DC. Federal data for outcome metrics would streamline and simplify follow-on efforts to improve future iterations. SkillSource would like to explore measuring wage growth beyond 12-months of employment, reduced justice involvement, and public benefit utilization among other possible outcome metrics.""When researchers analyzed data related to economic security of veterans with service-connected disabilities, they found that public, non-sensitive data can inform better decision making despite obstacles. Submitted by: Third Sector Capital Partners, Inc.","Project (re)Launch http://www.payforsuccess.org/project/san-diego-project-relaunchThe goal of (re)Launch is to improve employment prospects and economic security for veterans with service-connected disabilities (SCDs) by providing intensive case management and wraparound supports to participating veterans. Our analysis partner, American Institutes for Research (AIR), examined several data sources to determine whether the datasets are representative of the target population of veterans who are eligible for Vocational Rehabilitation & Education program (VR&E) funded training, including the American Community Survey (https://www.census.gov/programs-surveys/acs/), National Longitudinal Study of Youth 1997 (https://www.bls.gov/nls/nlsy97.htm), and the Behavioral Risk Factor Surveillance System (https://www.cdc.gov/brfss/index.html). Because the Veteran Affairs (VA) data was not directly available, we created a synthetic comparison pool of individuals by mimicking the VA’s VR&E eligibility criteria for veterans with SCDs across these databases. We looked at potential outcomes from these data sources that covered employment and earning, poverty status, highest level of education, frequency of poor mental/physical health days, and frequency of alcohol/marijuana consumption. While this project didn’t move forward, the public data was able to fill the gap created by not having access to the primary VA data. Ultimately, the operational coordination issues stopped the project at the feasibility stage, it was a great use case on how public, non-sensitive data can still inform better decision-making.""When Veritas helped organizations implement data governance, they realized many positive outcomes, such as better understanding of data risk and value. Submitted by: Veritas.","Use Case 2: In healthcare, a data governance strategy is absolutely critical and must extend beyond the data contained in an electronic medical record (EMR), as over 80% of the world’s healthcare data is unstructured. This includes all electronic communication and all of the organization’s files, whether they are stored on premise or in the cloud. Veritas’ Integrated Classification Engine supports numerous public sector and private sector healthcare organizations by scanning and tagging this data to provide the visibility, security, and protection of both structured and unstructured patient data, giving clinicians a better view of a patient’s health over time.In addition to protecting health information and ensuring availability, providers must know what information they have and how to use that data effectively and securely. Veritas’ approach to data governance in these organizations has led to many positive outcomes, including:- A complete understanding of data risks, assets, and vulnerabilities;- Compliance with HIPAA and other relevant regulations; - Enhanced value of content to ease end-user access, discovery, search, and supervision; and - New sources of business intelligence and analytics insights.The visibility and control of data are prerequisites for healthcare organizations to comply with HIPAA.Establishing a repeatable HIPAA compliance framework improves patient outcomes while increasing reimbursement for these institutions. Veritas helps reduce costs by offering valuable metadata that enables these organization to take appropriate action on how best to store and recover patient data.Information Map gives backup and storage administrators visibility into what they are actually backing up. This helps identify all of the health information that needs to be protected, as well as data that should not be protected on the organization’s infrastructure. Data Insight is used to provide intelligence about all of the unstructured data within a healthcare organization, helping identify information in unsecure locations, such as user workstations or shares, and ensuring data is always protected.The ultimate goal for a healthcare provider is to reach a steady state where content is efficiently stored for as long as it is needed, classified and categorized in near real-time, and ultimately disposed of when possible to reduce risk and storage costs. While data governance is required to meet healthcare regulations and to ensure litigation readiness, proper governance also leads to cost savings, a more effective workforce, and greater efficiencies throughout these organization.""When PCORnet pooled electronic health records, it enabled clinical data research and aided in development of a common data model for integrating records. Submitted by: Northwestern University .","PCORnet [2, 9] is an interesting use case on the challenges and potential solutions of leveraging massive data assets from many, disparate stakeholders to improve the outcomes for all. This initiative pools electronic health records from all over the country for clinical data research. They have a well-defined set of governance policies, which are used to guide the sharing of patient data.They obtained buy-in from the medical community for this sharing by assembling a panel of expert stakeholders from health care institutions throughout the US who collaborate to find the best policies for sharing their sensitive data for research and analytics. PCORnet also has a common data model and data quality checks for the records they integrate. This will be crucial for situations where you bring together data from many disparate sources, such as that of state governments or cooperating agencies.""If the Federal Data Strategy focuses its education- and health-related work on education and workforce outcomes, states would be better able to collaborate on the data- and policy-related aspects of this work. Submitted by: State Chief Data Officers Network & State of Connecticut.","State CDOs also see an opportunity to collaborate on the federal use cases. “Economic Development”, “Public Safety”, “National Security”, “Health”, and “Education” are all valuable use cases and states have an interest in and are involved in all of these areas. The State CDOs recommend refining the use cases in education and health to focus on the following:1. Education and Workforce Outcomes: Most states operate or are in the process of establishing longitudinal data systems that seek to inform policy makers on the workforce outcomes of publicly funded education programs, including higher education. These systems help states understand how public education aligns with the needs of employers in their states as well as to evaluate the effectiveness of publicly funded education.""When states formulate strategies to monitor and reduce opioid-related harm, the federal government can support and learn from these efforts. Submitted by: State Chief Data Officers Network & State of Connecticut.","Combating the Opioid and Addiction Crises: Virtually every state is in the process of formulating strategy to monitor and reduce overdose related deaths and improve addiction treatment and intervention strategies. There are numerous sources of data that exist at local, state, and federal levels to support research and evidence based policies in this area.""If governments link data from other domains (housing, Medicaid, employment…) to efforts aimed at reducing recidivism, those efforts will be more effective. Submitted by: State Chief Data Officers Network & State of Connecticut.","Recidivism: Most levels of government have made reducing recidivism a priority. In order to not only understand the causes of recidivism, but to identify the effective strategies to reduce it, data from a variety of programs not typically associated with the justice system are required. Integrating data related to housing, substance abuse, Medicaid, employment, education, transportation, and many others can aid in this area.""If the federal government leverages administrative data, it will be able to scale and replicate state governments' success at identifying fraud, waste, and abuse. Submitted by: State Chief Data Officers Network & State of Connecticut.","Fraud, Waste, and Abuse: There’s a significant opportunity to leverage administrative data to identify instances of fraud, waste, and abuse. Efforts are currently underway in several states to ensure federal and state funds are used appropriately and we believe these initiatives can be scaled in collaboration with the federal government.""When agencies used modern technologies, like mobile and cloud products, they were able to unlock more timely and valuable data for decision-making.  Submitted by: NTIS, Department of Commerce.","• Working on over ten projects-o Health and safety (FDA)--used a mobile platform for real-time data aggregation to find and track the flu and place maps, actually predicting where it would go.o Fraud detection (HHS, OIG)--improved access to data for decision making; using open source and cloud, predictive analytics; facilitated value based reimbursement o Recruiting and Hiring—Improved transparency in hiring; creating single authoritative source on the Federal employee""When CMS created an analytic tool environment and related vetting program, it improved the ability of data owners to get data into the hands of researchers in a timely fashion. Submitted by: Centers for Medicare and Medicaid Services .","o Tool Approach - Virtual Research Data Center, it used to take a long time to figure out where the data was, how to put the data together, and finally deliver the data. This would take so much time that the data was outdated by the time it reached the researcher.  Five years ago CMS created an analytic tool environment.  Users are vetted under the same rules as previously and allowed into the data environment.  Users can upload their own data and integrate it with CMS data.  Researchers can use tools in the virtual center to do analysis (e.g. access SAS licenses) and download results of analysis without acquiring the raw datao Qualified Entity Program - CMS gives access to data from the Medicare fee for service.  Participants have to bring comparable data (from Part C Plans), which they combine.  Users can conduct research and can sell access to other researchers through the program. These new customers are vetted like the researcherso Innovators Research Request Program – innovators are allowed to support tools and products they can sell.  Two levels of review by data governance board: 1) vetting the researcher and data process; and 2) vet to ensure that the development is being used to benefit Medicare""When DHS launched Management Cube, it empowered analytics through a self-service capability and allowed for integrated views of investments and other important areas. Submitted by: Department of Homeland Security.","• Launched DHS Management Cube in 2012 – the main functions of DHS were stood up very quickly and created many silos sowing immediate challenges for data sharing across the different components  • Time intensive and manual data calls were the drivers of information - needed to improve the process • Integration is easy to talk about but hard to do particularly when needing to integrate data and business - business process integration is a key driver • A common platform was created in order to “unleash the analysts” which included data sets enabled for data analytics with a reporting capability that allowed for self-service – the issue arose that the users did not have the capacity outside of their day–to-day work to learn the new platform• It is difficult for an organization the size of DHS to be trained in a new data analytics system• Enterprise architecture and data architecture needs to be managed together in order to drive the changes in business process that support good data management• Cube fits into several domains and since it was designed with the end user in mind it is easier to support mission functions• Integration teams help to drive change, there are currently four main initiatives that the integration teams focus on - unified views of investments, Human capital to budget execution, occupancy rate, and management health metrics• If anyone has any additional questions about the DHS Cube they should reach out""When DHS used Hadoop to prove out the idea that analytics could support strategic decisions, it demonstrated that a modern computer lab could help other projects in a similar way. Submitted by: Department of Homeland Security, Office of Science and Technology.","• Used ICE dataset which included multiple data sets across disparate systems in order to test in the lab to prove that analytics were going to work to make strategic decisions around this data • Used Hadoop—intended as a proof of concept.• This resulted in the establishment of a modern computer lab in order to solve these types of problems focused on the end user• The lab brings together people and machines and drives experimentation through testing and reforming• There are lots of experiments and designs, as well as, data projects to see what can be done around cleaning up and enabling better data analytics• Need to transition technology into operations• Lab uses rapid prototyping• Conducts technical evaluations based on what is out there, narrowing down the market survey to find the things that are needed; tech is owned by the business• A lot of partners in the private sector and academia""When OIRA made a prototype Wiki for linking a variety of data sets, it allowed users to help enable better search capabilities through tags and tag connections. Submitted by: Bureau of Labor Statistics, Department of Labor .","• OIRA made an experimental Wiki which linked a couple of the enterprise data sets, information collection requests (ICRs), and could be added to enterprise datao This had never done before – it allowed users to tag datasets, connect the tags, and enabled search capabilities for outside agencies• Census changed aspects of its data collection, as technologies evolved, which can be hard to track over time • Changes were done but the process could be improved - many universities and industry are already doing this work which is not glamorous because many do not want to take the risk of developing something imperfect• The Patent Category system changed over time - there are integration systems that need to be mapped and cross walks created in order to fix issues such as eight different definitions of “Boston”• Better to have more than one person working on it - Data Scientists could help• Wiki Data-  if updates are regulated could be translated to many languages and areas automatically o CDC and Smithsonian both contribute to the site, but on a volunteer basis • It is important to keep in touch and work with outside organizations""When NOAA provides data to the private sector, it helps enable a wide variety of valuable predictive analytical products. Submitted by: SAS Institute Inc..","One example of commercialization comes from National Oceanic and Atmospheric Administration (NOAA). NOAA is in a unique position to be an industry market maker, given the abundance of data it currently collects, as well as new technology that allows for the rapid generation and processing of data into useful information. Advancements in predictive analytics, for example, are allowing cities and local governments to create personalized or micro-weather forecasts to better predict weather-related business impacts. These approaches combine data from a variety of sources, including the Census, to simulate or model the impact of weather on a variety of different economic entities (e.g., air travel, tourism, public transportation). Such knowledge is being used by city planners to create better budgets, anticipate resource needs, and react to emergencies more efficiently. One can envision future use of such data in many other sectors. Consider medicine: NOAA data may one day be used to anticipate outbreaks of asthma and other respiratory disorders, including the flu, and better plan for them (e.g., alert local pharmacies and health care clinics, redirect pharmaceutical shipments). The applications are vast and growing as the types of available data continue to grow and access to those data improves. ""If the US Department of Transportation implement strong data governance as autonomous vehicles explode the available transportation data, it will be able to inform a wide variety of operations and financial analyses. Submitted by: SAS Institute Inc..","Transportation. The US Department of Transportation (USDOT) collects and manages over 90 data sets. The anticipated explosion of data generated from autonomous vehicles, sensors, etc. will need to be combined and cross-pollinated with the USDOT’s historical data sets. Combining this data will be a significant undertaking and requires strong data governance. A valuable data set comes from the National Highway Transportation Safety Administration (NHTSA)’s National Center for Statistics and Analysis (NCSA): https://www.nhtsa.gov/research-data/national-center-statistics-and-analysisncsa.  This data could be used to ensure safety and save lives.  Some examples of potential outcomes include: • Data related to road design, driver demographics, weather, congestion and more to reveal the key factors in major car crashes. With these insights, USDOT and state DOTs can proactively take countermeasures to reduce congestion and improve public safety.  NCSA data would be a good starting point.   • Through better financial and cash flow analysis, state DOTs can gain insight into the timing of revenue streams and project expenditures. Better understanding of cash flow and fund management enabled one state to plan additional projects valued at hundreds of millions of dollars. • Winter storms can cripple the most efficient road systems. Using data and analysis, state DOTs can improve winter operations through real-time analysis of weather and road conditions, optimizing the deployment of key resources to return roads to full operation in a timely manner. ""If the CDC and state health departments combined data on a common platform, this project would help drive insights relating to addiction treatment gaps. Submitted by: SAS Institute Inc..","Health. The common data language and common data platform between CDC and all state departments of health could be combined with social determinant data such as socio-economic, education, employment, Census and healthcare access data.  This combined data could be used to identify populations at risk and address numerous health problems through allocation of services. For example, to address the opioid epidemic, state health departments could combine claims data with other data sources such as geospatial data, department of corrections, department of transportation, etc. to create visualizations such as heat maps to better understand gaps in addiction treatment.  These gaps could be filled by recruiting more providers and pushing community outreach.""It is important to use data in developing, tracking, and reporting program performance metrics. Submitted by: XML Community of Practice (xmlCoP).","By far the most important use case for using data to assist policy-makers is to develop, track, and report agency and program performance metrics to support evidence-based decision-making.  Agencies are already required by law (section 10 of GPRAMA) to do so.  The necessary first step is for them to comply with the law.""If the government sets out to link the difference business identifiers in use by various agencies, it will help decrease business malfeasance in the US economy. Submitted by: Dun & Bradstreet, Inc.","U S E C A S E SPREVENTING BUSINESS MALFEASANCE IN THE US ECONOMYFormation over time of some 98 Million United States business entities i.e., corporation, limited liability corporation, trust, etc. are distributed across US Federal, State, Local and tribal-nations and was not constitutionally assigned to the US Federal Government. This distributed approach, and which differs materially from other markets, i.e., not centrally formed, curated; can enable inefficiencies including malfeasance and fraud.This distributed approach has also led to the various parts of the US government to each create their own and unconnected “master data”. A goal should be set by the Federal government to be able to consistently discover, curate and connect all US business entities and across all agencies. Such goals would help to prevent business malfeasance and protect victims of business crimes and protect the US economy including consumers from higher prices to cover losses.The Financial Action Task Force (FATF), of which the United states is a founding and active member puts out a set recommendations for each country relative to targeted actions to increase business entity transparency. While these recommendations include steps to target and prevent or detect things like money laundering, terrorist financing that would serve to also mitigate other general business fraud including various types e. g. business identity theft, bust-out fraud, payment fraud, insurance fraud, bid-rigging, healthcare fraud, as well as government impacting fraud i.e., tax evasion, public corruption etc. Facilitate transparency into a formed business, its operations, and lack of beneficial ownership to avoid fraud, waste and abuse Identify perpetrators in business to business malfeasance and fraud include Protect suppliers who write-off losses to general bad-debt; Protect from government losses due to lost business revenue tax; Prevent business identity theft to small and even large businesses who have their business identity stolen and used""If the government maintained a unique entity identifier, it would support improvements in public health. Submitted by: Dun & Bradstreet, Inc.","PROTECTING THE FOOD AND DRUG SUPPLYGovernment’s mission includes responsibility for protecting the public health by ensuring the safety, efficacy, and security of human and veterinary drugs, biological products, and medical devices; and by ensuring the safety of our nation's food supply, cosmetics, and products that emit radiation.Identifying, investigating and monitoring the regulated entities that manufacture, market, and distribute these products to protect the public health is critical. Having a unique entity identifier assigned to these entities allows global sharing of information on unique entities across multiple domestic and foreign regulatory agencies. Establishment of an electronic regulatory environment including master data management for food, drug and cosmetic processors and suppliers that are regulated by and must be registered with the government. Integrating information and data from disparate systems and business processes to ensure accuracy of data and a single source of truth Establishing business entity identities, assessing regulatory and product risk, determining a business entity’s involvement in regulated activities, and tracking product supply chains. Protecting public health by investigating and verifying global food and drug facilities Ensuring business compliance with food and drug safety regulations Flagging high risk food manufacturers, drug companies and other regulated entities""When facilitating the credit market for small businesses, enabling analytics can strengthen the nation's economy. Submitted by: Dun & Bradstreet, Inc.","SMALL BUSINESS GROWTHSmall business growth is largely dependent on access to capital – access to capital that government affords by guaranteeing over loans to small businesses. To ensure small business success, government aids, counsels, assists, and protects the interests of small business concerns. To ensure small business access to capital in order start, build, and grow businesses, government works with banks and other lending partners to guarantee loans for small businesses that cannot obtain reasonable credit in the conventional lending market. These programs are vital to preserving free competitive enterprise and strengthening the nation’s economy, creating and retaining jobs for workers, and supporting those who need capital the most – including minorities, women, veterans, and rural business owners. Enabling loan and lender monitoring systems with advanced analytic models, supporting business owners, improving small business viability, and strengthening the national economy. Conduct effective lender oversight and credit risk management of government lending partners that offer government backed credit Providing a robust set of current and historical data and forward-looking predictive analyses of lender portfolios and individual loans for over 3,800 lenders responsible for over $100 billion in loans Powering advanced analytics to integrate standard credit scores and develop customized analytic models to create a holistic view of portfolio risk in a single system Enable government to target deep-dive reviews on those lenders which represent the highest risk.Deliver micro-level profiles on lender and loan-level demographic and segment factors, revealing trends that allow for hyper-customization of continually updated analytic models that adapt to the rapidly changing business environment.""When the System for Award Management at the General Services Administration acts as the single point of entry, it provides a streamlined, integrated approach to businesses wishing to contract with the Government Submitted by: Dun & Bradstreet, Inc.","CONSOLIDATED AND STREAMLINED MODERN AWARD MANAGEMENT:Having a single point of entry into a centralized system is critically important for streamlining and modernizing the government award systems for both governmental agencies as well as for companies wanting to partner and do business with the government. In the United States, the System for Award management (SAM) at the General Services Administration (GSA) is the primary supplier and grantee database for the US Federal Government and the single point of entry for all businesses that want to contract with or receive grants from the US Federal Government. Government suppliers are able to log in to one centralized system to manage their entity information in a single record through a streamlined, rules based, business process. Once the information is entered into the single interface system, the entity is validated and the validated business information is used across multiple other systems integrated into the SAM environment. This award management system has evolved over the years to meet emerging needs from its original purpose of entity management to award management to accountability and transparency. This complex solution includes components across multiple dimensions including: Foundational, firmographic data to enable business verification, including the D-U-N-S Number Corporate linkage data to enable spend transparency Global business verification services at the point of entry to ensure the company is who they say they are Data governance and data quality to ensure critical decisions are based on the most accurate information available Entity matching and integration of entities into government systems Customization for government processes as needs develop Dedicated team of associates, handling inquiries from government and awardees Program management to ensure the government and taxpayer gets the maximum value The SAM environment comprises several additional government systems to allow a complete picture to procurement officials of the entity doing business with the government including Online Representation and Certifications (ORCA) where contractors can complete social responsibility self-certifications and the Excluded Party List System listing entities who have been excluded from doing business with the government or the Past Performance Information Retrieval System which contains performance assessments on the entity doing business with the government. Reduce risk in supply chains Cost savings in strategic sourcing and reducing fraud, waste, and abuse Ensure compliance with government regulations (including contracting, diversity, reduction of modernday slavery, etc.) Increase transparency of government spending Streamline due diligence in procurement processes Identify suppliers, corporate linkage and supplier networks Perform Responsibility determinations Ongoing monitoring of business viability to perform""If governments utilize innovative technological solutions to analyze risk, they will be able to respond to crises and accelerate recovery. Submitted by: Dun & Bradstreet, Inc.","PROTECTING THE HOMELANDThreats to the homeland are varied and ever-evolving. This requires governments to be proactive and innovative to prevent bad actors that pose the threat, while at the same ensuring the free flow of commerce. Identification of high risk importers or cargo shippers that move goods into the borders and across the domestic homeland is imperative. Ensuring global supply chain and air transport security with legitimate business entities requires increased transparency and holistic understanding of the legitimacy of a business and all of that businesses trading partners. Investigative resources are often limited, especially when compared with the volume of imports on a daily basis. Therefore, governments need to utilize innovative technological solutions with which to investigate and prioritize risk.Beyond the daily influx of goods and services, government must also use innovative technologies in times of crisis, including natural disasters or national emergencies. Agencies must understand the potential risk and economic impact to domestic businesses as well as to national security using geo-spatial analysis of critical infrastructure. Emergency preparedness facilitates a better response to crises and quicker recovery times after events. Uncover hidden relationships among entities that attempt to threaten national security Understanding financial stability of entities shipping goods into and out of the country Protect air transportation Ensuring integrity of global trade partners and importers Understanding business impact in national emergencies and natural disasters Ensuring regulatory compliance with immigration Evaluating country risk Transparency of disaster responses with trusted partners""If government creates a formal process to validate employers with verified data, it will enable USCIS to meet their current demands thereby improving mission services and better achieve their mission.  Submitted by: Dun & Bradstreet, Inc.","ENSURING LAWFUL IMMIGRATIONU.S. Citizenship & Immigration Services (USCIS) processes over 34 million E-Verify cases per year. To meetthis demand, Immigration Service Officers (ISOs) must equip themselves with comprehensive, detailed, andverified information about the entities and individuals involved in immigration processes. The agency establisheda system to facilitate the verification of employee eligibility to work in the US but the agency lacked a formalprocess to validate employers with verified data. Integrating quality data facilitates the necessary verifications.-Uses commercially available data to validate basic information entities petitioning to employ certainforeign nationals- Provide Immigration Service Officers with information on the entities existence and viability, including:business activities; financial standing; number of employees; relationships with any other entities,including foreign affiliates; ownership; date of establishment; and, current address.- Allow real-time identification of employers during the registration process and enables activity trackingand detailed business operation profiles for each employer.""If the government improves data interoperability by mandating machine readable standards across state, local, and federal funds in regards to awards, then it will enable the private market to develop innovative and efficient tools, products, etc.  Submitted by: Surety Resource Connection, Inc..","1. Enterprise Data Governance. What data governance and stewardship practices should the Federal Government be employing and whyMandate the use of machine readable data standards recognized by the federal government in all public works contracts, federal, state and local where federal funds are involved.The DATA Act addresses this objective and should be rigorously appliedWith consistent and reliable data definitions and structure used throughout various agencies, and implemented on the administration of all contracts, the private market will develop innovative and efficient tools, products and services to exploit the data interoperability.""If the government enforces machine readable data standards, it will enable interoperability to better serve agency missions and the public. Submitted by: Surety Resource Connection, Inc..","2. Use, Access, and Augmentation. What data interoperability techniques or coordination tactics would better serve agency missions and the public?The use of machine readable data standards recognized by the federal government, like XBRL""If the government empowers the private sector to develop analytics and reports with federal data, it will enable policy-makers to make better decision-making. Submitted by: Surety Resource Connection, Inc..","3. Decision-making and Accountability. How can the Federal Government better assist policy-makers with data?By enabling the private sector to develop analytics and best practices to generate data and reporting that the government can replicate or incorporate depending the cost factors.""If the government implements machine readable standards across the federal and public sphere, it will enable the private sector to innovate.  Submitted by: Surety Resource Connection, Inc..","4. Commercialization, Innovation, and Public Use. What data solutions could address a pervasive problem in government service delivery or the public sphere?The use of machine readable data standards recognized by the federal government, like XBRL, to enable the private sector to innovate ways to exploit the data for data driven decisions.""If the government standardizes electronic surety bonds with machine readable data standards, it will enable the government to increase efficiency by automating process, thereby reducing manual costs and improving analysis.  Submitted by: Surety Resource Connection, Inc..","Use Case #1: Federal Contracts should require standardized electronic surety bonds utilizing XBRL for the data standard, and prohibit paper bonds.Federal Agency: All Federal Agencies, NISTFederal contracts already have standardized forms, example Form 25 and Form 24.See comments to USDOT to reduce burdens on building transportation infrastructure projects.The surety industry has been working with XBRL and incorporating the data standard as an industry best practice to increase efficiency and reduce the significant high cost of manually preparing paper bonds along with eliminating the potential for errors in document preparation that result in economic losses for stakeholders.Errors in manually prepared paper bonds cause the government to reject a low bidder’s proposal and necessitating they award to the second bidder at a higher cost to the government.The data elements utilized in the digital electronic bond are consistent with the data elements of the underlying contract enabling more efficient administration.""If the government standardizes electronic surety bonds with machine readable data standards, it will enable the government to utilize predictive analytics.  Submitted by: Surety Resource Connection, Inc..","Use Case #2: All contracts that are associated with the national energy infrastructure, the Smart Grid, should require a uniform single national standard surety electronic surety bond form tailored for the risk being bonded utilizing XBRL for the data standard, and prohibit paper bonds.Federal Agency: DOE, FERC, NISTSample: Surety Bonds tailored for the Smart GridSample: Federal Surety Bond Form 24 & 25Same business case as Use Case #1There are over 3,500 different utilities nationally, multiple entities that provide energy under various structures, and the Smart Grid is expanding constantly with new providers of energy under a changing distribution model. The obligations of stakeholders are common throughout the national system, and ultimately regulated by FERC.This does not mean everyone has the same contract, just the same standardized surety bond form for the risk being bonded. The surety bond backs the terms of the contract, so each contract can be individually drafted and have the specific terms and conditions each stakeholder wants. By having the contract contain the variable terms and conditions specific to a project or project owner, and not the surety bond, the surety underwriting can focus on the contract, providing all stakeholders with clear, predictable and reliable claims handling by the surety industry because the surety obligation and clams handling is consistent, predictable and reliable.Implementing data standards enables data driven decisions with data analytics and predictive analytics.Having multiple interpretations of those common obligations, and multiple legal contracts each with their own nuances creates a major challenge for financial markets to evaluate and underwrite financial products and services costing thousands of additional man-hours and significant legal expense.Having consistently defined obligations by FERC, utilizing XBRL data elements, would enable the financial markets to significantly streamline their providing products and services, increase their responsiveness plus provide more reliable and predicate claims response.""If the government updates the T-list with digital surety bond data it will enable efficient automation of the claims notifications process for federal, state, and local agencies.  Submitted by: Surety Resource Connection, Inc..","Use Case #3: Treasury List of Acceptable Surety Companies to Include Electronic Claim AddressFederal Agency: Treasury, All Federal Agencies.Sample: Treasury List of Acceptable Surety Companies (T-List)The T-List provides consumers with a way to locate the home office of every qualified surety, verify the license for each surety and underwriting limitations the Department of Treasury has established for each company and is a well-recognized resource for accepting surety bonds.With surety bond data transitioning to digital the T-List should be expanded so that federal, state and local agencies, along with private consumers can establish the capability to notify a surety of a potential claim by having the surety claim department email address included as part of the information available.This will allow efficient automation of the claims notification process for federal, state and local agencies and the private consumer plus allow the surety the opportunity to receive notification of a problem that could be mitigated with early notification.""If the government develops data standards for energy grids to promote consistent and reliable data, it will enable the industry to achieve supply line efficiencies. Submitted by: Surety Resource Connection, Inc..","Use Case #4: Smart Grid System Performance measurement and monitoring to utilize XBRL for reportingFederal Agency: DOE, NIST, FERCSample: Data Exchange PilotSample: DOE XBRL Orange ButtonHaving consistent and reliable data throughout the Smart Grid will enable the entire supply chain to streamline the procurement and construction process, and support financial markets underwriting and administration of products and services.The energy grids reliability and resilience is dependent on quick, predictable and reliable response to stakeholder defaults, and data standards enabled data interoperability, along with consistent monitoring standards will provide the foundation for significant risk management.""If DOT posts contract monthly progress payments in a machine readable format, it will enable DOT's responsiveness when projects are experiencing stress and potential default.  Submitted by: Surety Resource Connection, Inc..","Use Case #5: State DOT’s to post contract monthly progress payments in XBRLFederal Agency: USDOT, NISTExample: Caltrans and VDOTSample: Application to monitor contract progress: www.ProjectStatusConnection.comSample: AB1223 – Construction contract payments: Internet Web site posting.Providing the surety and financial markets with real time monitoring data on the projects that are associated with credit extension, particularly to surety markets, will improve access to surety credit for contractors, provide improved risk management for sureties, and provide improved responsiveness to DOT’s when projects are experiencing stress and potential default.The data elements in the monthly reporting would tie to the contract, and the digital electronic surety bond for more efficient administration.The surety would be able to detect a deteriorating situation on a contract it had provided bonding on with predictive analytics.Early warning would allow the surety to mitigate the potential default before it escalated into a major problem, for the benefit of the surety, the DOT and the public.""If local agencies adopt California's best practices agencies around the posting of contract and financial assistance monthly progress payments in a machine readable format, it will enable data analysis and monitoring across the United States.  Submitted by: Surety Resource Connection, Inc..","Use Case #6: Local agencies to post contract monthly progress payments in XBRLFederal Agency: NISTExample: California AB1223 – Law requiring all state agencies to post monthly progress payment on contractsSame business case as Use Case #2California agencies are required by law to post monthly progress payments, and federal assistance for best practices will not only help California agencies, but be a model for all other states to enact similar policies and procedures that are consistent nationwide.""If the government standardizes pre-qualifications data elements for businesses it will enable auto-population and process streamlining across the Federal government, thereby resulting in a reduction of burden to industry and improved agency mission services.  Submitted by: Surety Resource Connection, Inc..","Use Case #7: Federal standardized Pre-qualification, Company Profile and basic Financial StatementFederal Agency: SBA, All Federal AgenciesSample: Chico Common AppSample: SBASample: California Public Utility Commission Supplier Diversity ProgramSample: Student STEM Program – Mapping all the SBA and CPUC forms to XBRLFederal agencies routinely require entities to submit information on their companies as a procurement requirement, and in most cases the information requested is similar across agencies. Having standardized data elements that can populate/import the common data elements will streamline the procurement process for federal agencies, make it easier for companies to do business with the federal agency, and make it easier for the financial markets to provide products and services.The complete XBRL US GAAP taxonomy contains all the data fields for public companies to report their financial statement to the SEC, but that volume of data elements is too much for most common entities, and subset of data elements defined as a Basic Financial Statement can contain enough data fields to be comprehensive enough for most purposes.Having a Basic Financial Statement with a condensed list of data elements would enable a wide range of software systems to incorporate the import/export functionality and contribute to a streamlined procurement process for federal agencies that require financial statements for private entities, make it easier for entities to so business with the federal agency and improve access to surety and bank credit.""When DOT implemented data driven funding decisions, including performance management, it enabled regular monitoring and allowed DOT to demonstrate performance against their stated objectives. Submitted by: Bureau of the Census (USBC) .","Case Study #4 – Transportation: Utah DOT ties State Funding Directly to OutcomesThe Utah Department of Transportation Highways Division (UDOT) was looking for a way to enable data driven funding decisions and allow for the measurement of the outcomes of those decisions.Starting with their Strategic Direction, UDOT uses the Socrata platform to transform static PDF files into an interactive site that demonstrates performance against stated objectives. Additionally, UDOT uses the Socrata platform to identify and standardize datasets from around UDOT. This allowed all the data, regardless of source, to be accessed via the Socrata platform's API interfaces. All of this is done in a secure FedRamp Moderate environment.Now, the department can regularly and automatically update, track, and communicate their performance.Please see Attachment6_ Utah DOT for additional detail on this Case Study."If the government centralizes grants management data it will enable data analysis. Submitted by: Bureau of the Census (USBC) .,"Case Study #5 – Grants ManagementToday, grants management data is highly decentralized. It can take weeks and months to accumulate data from all the necessary departments. The process is manual, labor intensive, relies on emails and spreadsheets, and often resides with a single clerk tasked with gathering the information.What is needed is a single, central repository for the data that can be used by many.Data Visualization: Data needs to be presented in a structured way, making it easier to interpret, analyze, extrapolate, and report out. Real-time views of financial and non-financial information should be accessible through an easy-to-use dashboard.Process Reminders and Standards: Automated timeline reminders and reporting requirement workflows could provide compliance check-ins across the grant lifecycle. With standardized data processes, everyone from program managers to financial professionals can trust that a system is in place to make sure reporting is complete and on-time.Socrata's platform allows all levels of government to access and use data throughout the life cycle of a federal grant.The Socrata platform allows all levels of government to standardize reporting data, evaluate it, track progress and report outcomes. In addition, the Socrata platform's API's allow internal and external sharing of the data.See a detailed description of his case study in Attachement7_Grants Management.""If state and local credential programs adopt data standards for program, credential, and competency information it will enable the identification of investments in education,  reduce costs to the federal agencies, and further enable the labor market to meet industry demands.  Submitted by: T3 Innovation Network .","Increasing the Availability of Consumer Information on Return on Investment for Programs and Credentials. Building on inter-agency efforts like the College Scorecard, as well as public-private collaborations like LEHD's partnership with the University of Texas system, there is a significant opportunity to scale up the voluntary provision of programmatic and credential information from states and individual education and training providers to connect individual participant records with long-term labor market outcomes and make metrics around return on investment in education more broadly available. Several foundations and funders are actively working in creating data standards for program, credential, and competency information and make it easier for education and training institutions to be able to share this data as part of data collaboratives and those efforts could dramatically reduce the costs to federal agencies for receiving and connecting individual and programmatic data, while broadening the set of programs and providers connecting their data. Stakeholders could include NASWA, IMS Global, AACRAO, Dxterra, the Lumina Foundation, Gates Foundation, and others.B1""When the Brazilian government partnered with the private sector and labor/trade unions, and produced statistics on a five-year rolling basis on the skill and labor needs by geographic areas, it enabled the Brazilian government to meet emerging labor needs.  Submitted by: The MITRE Corporation.","3. Statistics on a rolling five-year projection of the manpower needed in specific geographies and skill areas was developed by the Brazilian government26 in collaboration with private companies, trade associations, and labor unions in order to identify the best training provider to co-develop a curriculum with selected companies to meet emerging manpower needs."